data:
  make_inputs:
    _target_: "trax.data.make_inputs"
    _partial_: true
    train_stream:
      - "${data.train.TFDS}"
      - "${data.ConvertToUnicode}"
      - "${data.Tokenize}"
      - "${data.FilterEmptyExamples}"
      - "${data.TruncateToLength}"
      - "${data.AppendValue}"
      - "${data.PadToLength}"
      - "${data.AddLossWeights}"
      - "${data.Batch}"
    eval_stream:
      - "${data.eval.TFDS}"
      - "${data.ConvertToUnicode}"
      - "${data.Tokenize}"
      - "${data.FilterEmptyExamples}"
      - "${data.TruncateToLength}"
      - "${data.AppendValue}"
      - "${data.PadToLength}"
      - "${data.AddLossWeights}"
      - "${data.Batch}"
  train:
    TFDS:
      _target_: "trax.data.TFDS"
      dataset_name: "scientific_papers/arxiv:1.1.1"
      keys: "('article', 'abstract')"
      train: true
  ConvertToUnicode:
    _target_: "trax.data.ConvertToUnicode"
    keys:
      - 0
      - 1
  Tokenize:
    _target_: "trax.data.Tokenize"
    vocab_file: "gs://t5-data/vocabs/cc_all.32000/sentencepiece.model"
    keys:
      - 0
      - 1
    vocab_type: "sentencepiece"
  TruncateToLength:
    _target_: "trax.data.TruncateToLength"
    len_map:
      0: "(15359, )"
      1: "(1023, )"
  AppendValue:
    _target_: "trax.data.AppendValue"
    val:
      0:
        - 0
      1:
        - 1
  PadToLength:
    _target_: "trax.data.PadToLength"
    len_map:
      0: 15360
      1: 1024
    pad_value:
      0: 0
      1: 0
  Batch:
    _target_: "trax.data.Batch"
    batch_size: 32
  AddLossWeights:
    _target_: "trax.data.AddLossWeights"
    id_to_mask: 0
  eval:
    TFDS:
      _target_: "trax.data.TFDS"
      dataset_name: "scientific_papers/arxiv:1.1.1"
      keys: "('article', 'abstract')"
      train: false
  rekey:
    get_t5_preprocessor_by_name:
      _target_: "trax.data.get_t5_preprocessor_by_name"
      name: "rekey"
      fn_kwargs:
        key_map:
          inputs: "article"
          targets: "abstract"
  batcher:
    _target_: "trax.data.batcher"
    data_streams: "${data.data_streams}"
    max_eval_length: 16384
    buckets: "([16384], [32, 1])"
    strict_pad_on_len: false
    id_to_mask: 0
  data_streams:
    _target_: "trax.data.data_streams"
    _partial_: true
    data_dir: null
    dataset_name: "scientific_papers/arxiv:1.1.1"
    bare_preprocess_fn: "${data.generic_text_dataset_preprocess_fn}"
    input_name: "inputs"
    target_name: "targets"
  filter_dataset_on_len:
    _target_: "trax.data.filter_dataset_on_len"
    _partial_: true
    len_map:
      inputs: "(1, 16384)"
      targets: "(1, 1024)"
    filter_on_eval: true
  truncate_dataset_on_len:
    _target_: "trax.data.truncate_dataset_on_len"
    _partial_: true
    len_map:
      inputs: 16384
      targets: 1024
    truncate_on_eval: true
  pad_dataset_to_length:
    _target_: "trax.data.pad_dataset_to_length"
    _partial_: true
    len_map:
      inputs: 16384
      targets: 1024
  generic_text_dataset_preprocess_fn:
    _target_: "trax.data.generic_text_dataset_preprocess_fn"
    _partial_: true
    text_preprocess_fns:
      - "${data.rekey.get_t5_preprocessor_by_name}"
    token_preprocess_fns:
      - "${data.add_eos_to_output_features}"
      - "${data.truncate_dataset_on_len}"
      - "${data.filter_dataset_on_len}"
      - "${data.pad_dataset_to_length}"
  FilterEmptyExamples:
    _target_: "trax.data.FilterEmptyExamples"
  add_eos_to_output_features:
    _target_: "trax.data.add_eos_to_output_features"
    _partial_: true
