defaults:
  - /data/base
  - _self_

data:
  make_streams:
    _target_: "trax.data.make_streams"
    _partial_: true
    train_stream:
      - "${data.training.TFDS}"
      - "${data.CountAndSkip}"
      - "${data.SentencePieceTokenize}"
      - "${data.MLM}"
      - "${data.FilterEmptyExamples}"
      - "${data.AppendValue}"
      - "${data.TruncateToLength}"
      - "${data.PadToLength}"
      - "${data.AddLossWeights}"
      - "${data.Shuffle}"
      - "${data.Batch}"
    eval_stream:
      - "${data.validation.TFDS}"
      - "${data.SentencePieceTokenize}"
      - "${data.MLM}"
      - "${data.FilterEmptyExamples}"
      - "${data.AppendValue}"
      - "${data.TruncateToLength}"
      - "${data.PadToLength}"
      - "${data.AddLossWeights}"
      - "${data.validation.Batch}"
  TFDS:
    _target_: "trax.data.TFDS"
    dataset_name: "c4/en:2.3.0"
    keys: "('text',)"
    shuffle_train: false
  training:
    TFDS:
      _target_: "trax.data.TFDS"
      train: true
  validation:
    TFDS:
      _target_: "trax.data.TFDS"
      train: false
    Batch:
      _target_: "trax.data.Batch"
      batch_size: 64
  MLM:
    _target_: "trax.data.MLM"
    vocab_size: 32000
    max_length: 512
    noise_density: 0.15
    mean_noise_span_length: 3.0
  AppendValue:
    _target_: "trax.data.AppendValue"
    val:
      1:
        - 1
  CountAndSkip:
    _target_: "trax.data.CountAndSkip"
    name: "c4"
  TruncateToLength:
    _target_: "trax.data.TruncateToLength"
    len_map:
      0:
        - 512
      1:
        - 512
  PadToLength:
    _target_: "trax.data.PadToLength"
    len_map:
      0: 512
      1: 512
    pad_value:
      0: 0
      1: 0
  AddLossWeights:
    _target_: "trax.data.AddLossWeights"
    id_to_mask: 0
  Batch:
    _target_: "trax.data.Batch"
    batch_size: 32
  SentencePieceTokenize:
    _target_: "trax.data.SentencePieceTokenize"
    spm_path: null
