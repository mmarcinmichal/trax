defaults:
  - /model/base
  - _self_

model:
  MultiplicativeConvCausalAttention:
    _target_: "trax.layers.MultiplicativeConvCausalAttention"
    sparsity: 16
    length_kernel_size: 3
  ConfigurableTransformer:
    _target_: "trax.models.ConfigurableTransformer"
    _partial_: true
    d_model: 1024
    d_ff: 4096
    dropout: 0.1
    ff_dropout: 0.1
    ff_chunk_size: 0
    ff_sparsity: 0
    max_len: "max_length"
    n_heads: 16
    n_encoder_layers: 24
    n_decoder_layers: 24
    input_vocab_size: 32000
    encoder_attention_type: "${model.Attention}"
    encoder_decoder_attention_type: "${model.CausalAttention}"
    loss_sparsity: 0
  Transformer:
    _target_: "trax.models.Transformer"
    d_ff: 4096
    d_model: 1024
    dropout: 0.1
    dropout_shared_axes: null
    input_vocab_size: 32000
    max_len: 2048
    n_decoder_layers: 24
    n_encoder_layers: 24
    n_heads: 16
    output_vocab_size: null
  Adam:
    _target_: "trax.models.Adam"
    weight_decay_rate: 0.0
