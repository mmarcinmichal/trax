defaults:
  - /model/base
  - _self_

model:
  LSHSelfAttention:
    _target_: "trax.layers.LSHSelfAttention"
    _partial_: true
    attention_dropout: 0.2
    chunk_len: 128
    n_buckets: 128
    n_chunks_after: 0
    n_chunks_before: 1
    n_hashes: 2
    n_parallel_heads: 1
    predict_drop_len: 16384
    predict_mem_len: 16384
  ref2_encoder:
    LSHSelfAttention:
      _target_: "trax.layers.LSHSelfAttention"
      _partial_: true
      n_chunks_after: 1
  ConfigurableTerraformer:
    _target_: "trax.models.ConfigurableTerraformer"
    _partial_: true
    d_model: 1024
    d_ff: 4096
    dropout: 0.2
    ff_activation: "${model.Relu}"
    ff_dropout: 0.1
    ff_chunk_size: 0
    max_len: 16384
    n_heads: 8
    n_encoder_layers: 8
    n_decoder_layers: 8
    input_vocab_size: 32000
    d_attention_key: 128
    d_attention_value: 128
    encoder_attention_type: "${model.ref2_encoder.LSHSelfAttention}"
    encoder_decoder_attention_type: "${model.LSHSelfAttention}"
    pos_type: "fixed-base"
    pos_d_axial_embs: "(512, 512)"
  ReformerLM:
    _target_: "trax.models.ReformerLM"
    attention_type: "${model.LSHSelfAttention}"
    d_attention_key: 128
    d_attention_value: 128
    d_model: 1024
    d_ff: 4096
    dropout: 0.2
    ff_activation: "${model.Relu}"
    max_len: 16384
    mode: "train"
    n_heads: 8
    n_layers: 8
    vocab_size: 32000
    pos_type: "fixed-base"
    pos_d_axial_embs: "(512, 512)"
  attn_type:
    _target_: "trax.models.attn_type"
