model:
  MultiplicativeModularCausalAttention:
    _target_: "trax.layers.MultiplicativeModularCausalAttention"
    sparsity: 16
  MultiplicativeConvCausalAttention:
    _target_: "trax.layers.MultiplicativeConvCausalAttention"
    sparsity: 16
  ConfigurableTransformerLM:
    _target_: "trax.models.ConfigurableTransformerLM"
    _partial_: true
    d_model: 1024
    d_ff: 4096
    dropout: 0.1
    ff_dropout: 0.1
    ff_chunk_size: 0
    ff_sparsity: 0
    max_len: 2048
    n_heads: 16
    n_layers: 24
    vocab_size: 32000
    attention_type: "${model.CausalAttention}"
    ff_use_sru: 0
    loss_sparsity: 0
  ReformerLM:
    _target_: "trax.models.ReformerLM"
    d_model: 1024
    d_ff: 4096
    dropout: 0.1
    ff_activation: "${model.Relu}"
    ff_chunk_size: 0
    ff_sparsity: 0
    max_len: 2048
    mode: "train"
    n_heads: 16
    n_layers: 24
    vocab_size: 32000
    d_attention_key: 64
    d_attention_value: 64
    attention_type: "${model.CausalAttention}"
    loss_sparsity: 0
  attn_type:
    _target_: "trax.models.attn_type"
  CausalAttention:
    _target_: "trax.layers.CausalAttention"
    _partial_: true
  Relu:
    _target_: "trax.layers.Relu"
    _partial_: true
