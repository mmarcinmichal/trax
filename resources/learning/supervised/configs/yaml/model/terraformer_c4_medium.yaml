model:
  MultiplicativeModularCausalAttention:
    _target_: "trax.layers.MultiplicativeModularCausalAttention"
    sparsity: 16
  MultiplicativeConvCausalAttention:
    _target_: "trax.layers.MultiplicativeConvCausalAttention"
    _partial_: true
    sparsity: 16
    length_kernel_size: 3
  SelfAttention:
    _target_: "trax.layers.SelfAttention"
    attention_dropout: 0.1
    chunk_len: null
  LSHSelfAttention:
    _target_: "trax.layers.LSHSelfAttention"
    attention_dropout: 0.1
    chunk_len: 128
    n_buckets: null
    n_chunks_after: 0
    n_chunks_before: 1
    n_hashes: 2
    n_parallel_heads: 1
    predict_drop_len: "max_length"
    predict_mem_len: "max_length"
  encoder:
    LSHSelfAttention:
      _target_: "trax.layers.LSHSelfAttention"
      n_chunks_after: 1
    PureLSHSelfAttention:
      _target_: "trax.layers.PureLSHSelfAttention"
      _partial_: true
      n_chunks_after: 1
    PureLSHSelfAttentionWrapper:
      _target_: "trax.layers.PureLSHSelfAttentionWrapper"
      pure_lsh_implementation: "${model.encoder.PureLSHSelfAttention}"
      weights_format: "model"
  PureLSHSelfAttention:
    _target_: "trax.layers.PureLSHSelfAttention"
    _partial_: true
    attention_dropout: 0.2
    chunk_len: 128
    n_buckets: null
    n_chunks_after: 0
    n_chunks_before: 1
    n_hashes: 4
    n_parallel_heads: 1
    predict_drop_len: "max_length"
    predict_mem_len: "max_length"
  PureLSHSelfAttentionWrapper:
    _target_: "trax.layers.PureLSHSelfAttentionWrapper"
    pure_lsh_implementation: "${model.PureLSHSelfAttention}"
    weights_format: "sparse"
  ConfigurableTerraformer:
    _target_: "trax.models.ConfigurableTerraformer"
    _partial_: true
    d_model: 1024
    d_ff: 6144
    dropout: 0.1
    ff_activation: "${model.Relu}"
    ff_dropout: 0.1
    ff_chunk_size: 512
    ff_sparsity: 64
    ff_use_sru: "(1, 32)"
    max_len: 32768
    n_heads: 16
    n_encoder_layers: 24
    n_decoder_layers: 24
    input_vocab_size: "vocab_size"
    d_attention_key: 64
    d_attention_value: 64
    encoder_attention_type: "${model.Attention}"
    encoder_decoder_attention_type: "${model.MultiplicativeConvCausalAttention}"
    use_two_swaps_per_encoder_block: true
    reversible_encoder: true
    loss_sparsity: 4
    n_layers_forget: 0
    pos_type: null
    attention_chunk_size: 0
    use_bfloat16: false
  Transformer2:
    _target_: "trax.models.Transformer2"
    d_model: 1024
    d_ff: 6144
    dropout: 0.1
    dropout_shared_axes:
      - -2
    ff_activation: "${model.Relu}"
    ff_dropout: 0.1
    ff_chunk_size: 512
    ff_sparsity: 64
    ff_use_sru: "(1, 32)"
    max_len: 32768
    mode: "train"
    n_heads: 16
    n_encoder_layers: 24
    n_decoder_layers: 24
    input_vocab_size: "vocab_size"
    encoder_attention_type: "${model.Attention}"
    decoder_attention_type: "${model.MultiplicativeConvCausalAttention}"
    pos_type: null
    attention_chunk_size: 0
  enc_attn_type:
    _target_: "trax.models.enc_attn_type"
  dec_attn_type:
    _target_: "trax.models.dec_attn_type"
  Relu:
    _target_: "trax.layers.Relu"
    _partial_: true
  Attention:
    _target_: "trax.layers.Attention"
    _partial_: true
