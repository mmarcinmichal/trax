{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "# Copyright 2020 Google LLC.\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author- [@yashkhasbage25](https://github.com/yashkhasbage25 \"Yash Khasbage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZWS_qfMw1B3"
   },
   "source": [
    "# Semantic Segmentation \n",
    "Semantic Segmentation is a computer vision task that divides an image into segments, identifying what parts of image belong to what object. \n",
    "\n",
    "In this tutorial, we will train a Convolutional neural network to segment images. \n",
    "\n",
    "Briefly, we will discuss\n",
    "1. downloading an image segmentation dataset from kaggle\n",
    "2. processing the dataset according to our need\n",
    "3. Create a dataloader\n",
    "4. Creating a Custom loss function\n",
    "5. Creating TrainTask and EvalTask \n",
    "6. Create a Neural Network and train it\n",
    "\n",
    "(You need to have a kaggle account for downloading the dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0AjBi0zHE4pv"
   },
   "source": [
    "Assuming that you already have a kaggle account, we will first begin by creating a kaggle API token. \n",
    "If you don't have API token, follow these steps to create a new one:\n",
    "1. Go to the Account section of kaggle website, after you login. \n",
    "2. Click \"Expire API Token\" and then \"Create New API Token\". A file \"kaggle.json\" will be downloaded. \n",
    "3. Using \"Choose files\" button, upload the kaggle.json file. The API token present in this file will help us download the dataset directly from kaggle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "id": "dzXwMVFPf2qR",
    "outputId": "0d776c36-8dbd-4242-e933-0b73abe243b0"
   },
   "outputs": [],
   "source": [
    "! pip install -q kaggle\n",
    "from google.colab import files\n",
    "files.upload()  # upload kaggle.json\n",
    "! mkdir ~/.kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sskHBrFsM4Yl"
   },
   "source": [
    "We need to place kaggle.json at ~/.kaggle and also change its file permissions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TT61H-y8gg4E",
    "outputId": "333b528f-e768-496d-9593-09e4036703c0"
   },
   "outputs": [],
   "source": [
    "! cp kaggle.json ~/.kaggle/\n",
    "! chmod 600 ~/.kaggle/kaggle.json\n",
    "! kaggle datasets list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S867tktZNHqD"
   },
   "source": [
    "Now with this command, we actually download the dataset. This may take some time, depending on internet speed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZQ_96E2ngwvJ",
    "outputId": "0aba3d27-0698-4abd-8057-c9615518e7f2"
   },
   "outputs": [],
   "source": [
    "! kaggle datasets download -d dansbecker/cityscapes-image-pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKF2tpAAPHBN"
   },
   "source": [
    "The download has to be uncompressed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DQtpMD67hbAO"
   },
   "outputs": [],
   "source": [
    "! unzip -q cityscapes-image-pairs.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKH-76ZJPMeR"
   },
   "source": [
    "Intall trax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mh05_t3Phy2h",
    "outputId": "21c0cd27-8c13-49d5-b30f-65533d9a8084"
   },
   "outputs": [],
   "source": [
    "! pip install -q -U trax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6HGMKVu1kfYh"
   },
   "outputs": [],
   "source": [
    "# several imports from trax\n",
    "\n",
    "import trax\n",
    "import numpy as np\n",
    "import trax.layers as tl\n",
    "from trax.fastmath import numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o-3g4wi1leJy"
   },
   "outputs": [],
   "source": [
    "# several imports out of trax\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "from PIL import Image\n",
    "from itertools import cycle\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZTt5oh_QjdcI"
   },
   "outputs": [],
   "source": [
    "# let's fix batch size\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KmqJJqs8PnEN"
   },
   "source": [
    "Some details of the dataset in its original form: \n",
    "The original images are of the shape 256x512x3. The left half and the right half of images belong to input and label respectively. In a typical segmentation label, the label should be a 2D matrix consisting of the class label of objects, such that each pixel is alloted a class. In the label images given, we are not directly provided with the class labels. However, each class label is represented with a specific color. We need to map colors to class labels, to convert them into usable format. \n",
    "\n",
    "We know that there are total 13 classes in the dataset. Hence, we will be given 13 different colors in labels. For processing the label images, according to the procedure mentioned above, we will use K-Means utility of sklearn.\n",
    "\n",
    "We do the processing in the following manner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tIBSJ3gpkmf9"
   },
   "outputs": [],
   "source": [
    "def color_kmean(root):\n",
    "  \"\"\" creates a k-means objects that recognizes all 13 colors of dataset. \"\"\"\n",
    "  \n",
    "  # take 10 first images\n",
    "  files = os.listdir(root)[:10] \n",
    "  colors = list()\n",
    "  for f in files:\n",
    "    img = load_image(osp.join(root, f))\n",
    "    # total width\n",
    "    w = img.shape[2]\n",
    "    # get the right half of image, which is the label image\n",
    "    img = img[:, w:, :]\n",
    "    # collect all the colors present in label image\n",
    "    colors.append(img.reshape(-1, 3))\n",
    "\n",
    "  colors = np.array(colors)\n",
    "  colors = colors.reshape(-1, 3)\n",
    "\n",
    "  # finally, fit all the colors into the KMeans\n",
    "  kmeans = KMeans(13)\n",
    "  kmeans.fit(colors)\n",
    "\n",
    "  return kmeans\n",
    "\n",
    "def load_image(path):\n",
    "  \"\"\" loading an image. \"\"\"\n",
    "  \n",
    "  assert osp.exists(path), path + \" not found\"\n",
    "  image = Image.open(path)\n",
    "  image = np.asarray(image)\n",
    "  return image\n",
    "\n",
    "def color2class(segs, km):\n",
    "  \"\"\" \n",
    "  given an label image, convert it to class matrix, \n",
    "  which is a 2D matrix of class labels (scalars).\n",
    "  \"\"\"\n",
    "  \n",
    "  h, w, c = segs.shape\n",
    "  segs = segs.reshape((-1, 3))\n",
    "  segs = km.predict(segs)\n",
    "  segs = segs.reshape((h, w, 1))\n",
    "  return segs\n",
    "\n",
    "def load_dataset(root, km):\n",
    "  \"\"\" load dataset. \"\"\"\n",
    "  index = 0\n",
    "  imgs_path = [osp.join(root, f) for f in os.listdir(root)]\n",
    "\n",
    "  # load images one by one, finally, and image and \n",
    "  # its label matrix is returned\n",
    "  while True:\n",
    "    img = load_image(imgs_path[index])\n",
    "    w = img.shape[1] // 2\n",
    "    img, seg = img[:, :w, :], img[:, w:, :]\n",
    "\n",
    "    seg = color2class(seg, km)\n",
    "\n",
    "    seg = seg.reshape(-1)\n",
    "    assert img.shape == (256, 256, 3), img.shape\n",
    "    assert seg.shape == (256 * 256,), seg.shape\n",
    "    yield img, seg\n",
    "\n",
    "    index = (index + 1) % len(imgs_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "udqueyxmA6Pc"
   },
   "source": [
    "Uncomment to try other backend. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DJq1biuLxeFa",
    "outputId": "f95918ee-413a-4ecb-9982-a34c3d3e6177"
   },
   "outputs": [],
   "source": [
    "# trax.fastmath.set_backend('tensorflow-numpy')\n",
    "print(trax.fastmath.backend_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ce_KBGtTBB50"
   },
   "source": [
    "Set path to dataset, and get kmeans color setter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HLysKwN0Xy5t"
   },
   "outputs": [],
   "source": [
    "root = 'cityscapes_data'\n",
    "\n",
    "trainset_path = osp.join(root, 'train')\n",
    "valset_path = osp.join(root, 'val')\n",
    "\n",
    "km = color_kmean(trainset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lex2Tm72BrFf"
   },
   "source": [
    "Create dataset loaders and data transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ngHMyZBbjfft"
   },
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(trainset_path, km)\n",
    "val_dataset = load_dataset(valset_path, km)\n",
    "\n",
    "train_transforms = trax.data.Serial(\n",
    "    trax.data.Shuffle(),\n",
    "    trax.data.Batch(batch_size),\n",
    "    lambda g: map(lambda p: (p[0].astype(np.float32), p[1]), g),\n",
    ")\n",
    "val_transforms = trax.data.Serial(\n",
    "    trax.data.Batch(batch_size),\n",
    "    lambda g: map(lambda p: (p[0].astype(np.float32), p[1]), g),\n",
    ")\n",
    "\n",
    "train_dataset = train_transforms(train_dataset)\n",
    "val_dataset = val_transforms(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HURVJcElB9et"
   },
   "source": [
    "Create a custom loss. In semantic segmentation we need to apply cross entropy for every pixel of image. Hence, we decrease the number of dimensions of the matrices so that we can use CrossEntropy2d, while maintaining the order of elements of matrices. \n",
    "\n",
    "Here, we convert the 3D Neural Network to 2D array and 2D label matrix to 1D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZEdJXM9g8rif",
    "outputId": "6b78ca76-db43-44c6-b618-435cbd8c8f3e"
   },
   "outputs": [],
   "source": [
    "def CrossEntropy3d(criterion_2d):\n",
    "  \"\"\" returns 3D cross entropy loss function \"\"\"\n",
    "  def _loss_fn(output, target):\n",
    "    output = output.reshape(-1, 13)\n",
    "    target = target.reshape(-1,)\n",
    "    loss = criterion_2d((output, target))\n",
    "    return loss\n",
    "  return _loss_fn\n",
    "\n",
    "# check dataset\n",
    "x, y = next(train_dataset) \n",
    "print(x.shape, y.shape)\n",
    "print(x.dtype, y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VWmhQZDElSo6"
   },
   "outputs": [],
   "source": [
    "# set learning rate\n",
    "lr = 1e-2\n",
    "\n",
    "# create new trax Fn for new loss fn, and provide it a name\n",
    "criterion = trax.layers.base.Fn(\"CrossEntropy3d\", \n",
    "                                CrossEntropy3d(tl.CategoryCrossEntropy())\n",
    "                                )\n",
    "\n",
    "# create TrainTask\n",
    "train_task = trax.supervised.training.TrainTask(\n",
    "    labeled_data=train_dataset,\n",
    "    loss_layer=criterion,\n",
    "    optimizer=trax.optimizers.Momentum(lr),\n",
    "    n_steps_per_checkpoint=50\n",
    ")\n",
    "\n",
    "# create EvalTask\n",
    "eval_task = trax.supervised.training.EvalTask(\n",
    "    labeled_data=val_dataset,\n",
    "    metrics=[criterion]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mieHBPnpExJo"
   },
   "source": [
    "Now create a simple Serial model. You can create a complex one according to your need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LgWQmYCVoBXU"
   },
   "outputs": [],
   "source": [
    "model = tl.Serial(\n",
    "    tl.Conv(13, (3, 3), (1, 1), padding='SAME', kernel_initializer=tl.KaimingNormalInitializer()),\n",
    "    tl.Relu(),\n",
    "    tl.LayerNorm(),\n",
    "    tl.Conv(32, (3, 3), (1, 1), padding='SAME', kernel_initializer=tl.KaimingNormalInitializer()),\n",
    "    tl.Relu(),\n",
    "    tl.LayerNorm(),\n",
    "    tl.Conv(32, (3, 3), (1, 1), padding='SAME', kernel_initializer=tl.KaimingNormalInitializer()),\n",
    "    tl.Relu(),\n",
    "    tl.LayerNorm(),\n",
    "    tl.Conv(64, (3, 3), (1, 1), padding='SAME', kernel_initializer=tl.KaimingNormalInitializer()),\n",
    "    tl.Relu(),\n",
    "    tl.LayerNorm(),\n",
    "    tl.Conv(128, (3, 3), (1, 1), padding='SAME', kernel_initializer=tl.KaimingNormalInitializer()),\n",
    "    tl.Relu(),\n",
    "    tl.LayerNorm(),\n",
    "    tl.Conv(64, (3, 3), (1, 1), padding='SAME', kernel_initializer=tl.KaimingNormalInitializer()),\n",
    "    tl.Relu(),\n",
    "    tl.LayerNorm(),\n",
    "    tl.Conv(32, (3, 3), (1, 1), padding='SAME', kernel_initializer=tl.KaimingNormalInitializer()),\n",
    "    tl.Relu(),\n",
    "    tl.LayerNorm(),\n",
    "    tl.Conv(32, (3, 3), (1, 1), padding='SAME', kernel_initializer=tl.KaimingNormalInitializer()),\n",
    "    tl.Relu(),\n",
    "    tl.LayerNorm(),\n",
    "    tl.Conv(13, (3, 3), (1, 1), padding='SAME', kernel_initializer=tl.KaimingNormalInitializer())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Z5SsOVNE6KJ"
   },
   "source": [
    "Crete a training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TE2Rfdafv5xl",
    "outputId": "3cc3fc96-f812-470b-d058-b07b7d67f339"
   },
   "outputs": [],
   "source": [
    "training_loop = trax.supervised.training.Loop(\n",
    "    model, \n",
    "    train_task, \n",
    "    eval_tasks=[eval_task],\n",
    "    output_dir=None\n",
    ")\n",
    "\n",
    "training_loop.run(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_eQXlgAJQd8"
   },
   "source": [
    "Lets see some example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CcR_gzqsJUom",
    "outputId": "ea1e1457-b4d1-4499-f7da-c791163eb740"
   },
   "outputs": [],
   "source": [
    "x, y = next(val_dataset)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3)\n",
    "\n",
    "x = x[0]\n",
    "y = y[0]\n",
    "\n",
    "y = np.reshape(y, (256, 256))\n",
    "axs[0].imshow(x.astype(np.int32))\n",
    "axs[1].imshow(y)\n",
    "fig.show()\n",
    "\n",
    "x = np.expand_dims(x, 0)\n",
    "y_hat = model(x)\n",
    "y_hat = y_hat[0]\n",
    "\n",
    "y_hat = np.argmax(y_hat, 2)\n",
    "y_hat = np.reshape(y_hat, (-1,))\n",
    "y_hat = km.cluster_centers_[y_hat]\n",
    "y_hat = np.reshape(y_hat, (256, 256, 3))\n",
    "y_hat = np.round_(y_hat).astype(np.int32)\n",
    "\n",
    "axs[2].imshow(y_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q-TYBBWHk1v6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "semantic_segmentation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
