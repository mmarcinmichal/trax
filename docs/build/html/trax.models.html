

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>trax.models &mdash; Trax  documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=9edc463e" />

  
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=5929fcd5"></script>
      <script src="_static/doctools.js?v=fd6eb6e6"></script>
      <script src="_static/sphinx_highlight.js?v=6ffebe34"></script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="trax.data" href="trax.data.html" />
    <link rel="prev" title="trax.layers" href="trax.layers.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            Trax
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Introductory Notebooks</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="notebooks/trax_intro.html">Trax Quick Intro</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/layers_intro.html">Trax Layers Intro</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Packages/modules</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="trax.html">trax.*</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="trax.fastmath.html">fastmath.*</a></li>
<li class="toctree-l2"><a class="reference internal" href="trax.layers.html">layers.*</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">models.*</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#module-trax.models.atari_cnn">atari_cnn</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#trax.models.atari_cnn.AtariCnn"><code class="docutils literal notranslate"><span class="pre">AtariCnn()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.models.atari_cnn.AtariCnnBody"><code class="docutils literal notranslate"><span class="pre">AtariCnnBody()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.models.atari_cnn.FrameStackMLP"><code class="docutils literal notranslate"><span class="pre">FrameStackMLP()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#module-trax.models.mlp">mlp</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#trax.models.mlp.MLP"><code class="docutils literal notranslate"><span class="pre">MLP()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#module-trax.models.neural_gpu">neural_gpu</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#trax.models.neural_gpu.SaturationCost"><code class="docutils literal notranslate"><span class="pre">SaturationCost()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.models.neural_gpu.DiagonalGate"><code class="docutils literal notranslate"><span class="pre">DiagonalGate()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.models.neural_gpu.ConvDiagonalGRU"><code class="docutils literal notranslate"><span class="pre">ConvDiagonalGRU()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.models.neural_gpu.NeuralGPU"><code class="docutils literal notranslate"><span class="pre">NeuralGPU()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#module-trax.models.resnet">resnet</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#trax.models.resnet.ConvBlock"><code class="docutils literal notranslate"><span class="pre">ConvBlock()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.models.resnet.IdentityBlock"><code class="docutils literal notranslate"><span class="pre">IdentityBlock()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.models.resnet.Resnet50"><code class="docutils literal notranslate"><span class="pre">Resnet50()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.models.resnet.WideResnetBlock"><code class="docutils literal notranslate"><span class="pre">WideResnetBlock()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.models.resnet.WideResnetGroup"><code class="docutils literal notranslate"><span class="pre">WideResnetGroup()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.models.resnet.WideResnet"><code class="docutils literal notranslate"><span class="pre">WideResnet()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#module-trax.models.rl">rl</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#trax.models.rl.Policy"><code class="docutils literal notranslate"><span class="pre">Policy()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.models.rl.Value"><code class="docutils literal notranslate"><span class="pre">Value()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.models.rl.PolicyAndValue"><code class="docutils literal notranslate"><span class="pre">PolicyAndValue()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.models.rl.Quality"><code class="docutils literal notranslate"><span class="pre">Quality()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#module-trax.models.rnn">rnn</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#trax.models.rnn.RNNLM"><code class="docutils literal notranslate"><span class="pre">RNNLM()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.models.rnn.GRULM"><code class="docutils literal notranslate"><span class="pre">GRULM()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.models.rnn.LSTMSeq2SeqAttn"><code class="docutils literal notranslate"><span class="pre">LSTMSeq2SeqAttn()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#module-trax.models.transformer">transformer</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#trax.models.transformer.TransformerEncoder"><code class="docutils literal notranslate"><span class="pre">TransformerEncoder()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.models.transformer.TransformerDecoder"><code class="docutils literal notranslate"><span class="pre">TransformerDecoder()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.models.transformer.TransformerLM"><code class="docutils literal notranslate"><span class="pre">TransformerLM()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.models.transformer.Transformer"><code class="docutils literal notranslate"><span class="pre">Transformer()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#module-trax.models.reformer.reformer">reformer.reformer</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#trax.models.reformer.reformer.DecoderBlock"><code class="docutils literal notranslate"><span class="pre">DecoderBlock()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.models.reformer.reformer.ReformerLM"><code class="docutils literal notranslate"><span class="pre">ReformerLM()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.models.reformer.reformer.ReformerShortenLM"><code class="docutils literal notranslate"><span class="pre">ReformerShortenLM()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.models.reformer.reformer.EncoderBlock"><code class="docutils literal notranslate"><span class="pre">EncoderBlock()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.models.reformer.reformer.EncoderDecoderBlock"><code class="docutils literal notranslate"><span class="pre">EncoderDecoderBlock()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.models.reformer.reformer.Reformer"><code class="docutils literal notranslate"><span class="pre">Reformer()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#module-trax.models.research.bert">research.bert</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#trax.models.research.bert.AddBias"><code class="docutils literal notranslate"><span class="pre">AddBias</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.models.research.bert.BERTClassifierHead"><code class="docutils literal notranslate"><span class="pre">BERTClassifierHead()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.models.research.bert.BERTRegressionHead"><code class="docutils literal notranslate"><span class="pre">BERTRegressionHead()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.models.research.bert.BERTMLMHead"><code class="docutils literal notranslate"><span class="pre">BERTMLMHead()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.models.research.bert.BERTPretrainingLoss"><code class="docutils literal notranslate"><span class="pre">BERTPretrainingLoss()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.models.research.bert.BERTPretrainingHead"><code class="docutils literal notranslate"><span class="pre">BERTPretrainingHead()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.models.research.bert.BERT"><code class="docutils literal notranslate"><span class="pre">BERT()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.models.research.bert.PretrainedBERT"><code class="docutils literal notranslate"><span class="pre">PretrainedBERT</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#research-skipping-transformer">research.skipping_transformer</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="trax.data.html">data.*</a></li>
<li class="toctree-l2"><a class="reference internal" href="trax.optimizers.html">optimizers.*</a></li>
<li class="toctree-l2"><a class="reference internal" href="trax.supervised.html">learning.*</a></li>
<li class="toctree-l2"><a class="reference internal" href="trax.rl.html">rl.*</a></li>
<li class="toctree-l2"><a class="reference internal" href="trax.html#shapes">shapes</a></li>
<li class="toctree-l2"><a class="reference internal" href="trax.html#trainer">trainer</a></li>
<li class="toctree-l2"><a class="reference internal" href="trax.html#rl-trainer">rl_trainer</a></li>
<li class="toctree-l2"><a class="reference internal" href="trax.html#trax2keras">trax2keras</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Trax</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="trax.html">trax</a></li>
      <li class="breadcrumb-item active">trax.models</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/trax.models.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="trax-models">
<h1>trax.models<a class="headerlink" href="#trax-models" title="Link to this heading"></a></h1>
<section id="module-trax.models.atari_cnn">
<span id="atari-cnn"></span><h2>atari_cnn<a class="headerlink" href="#module-trax.models.atari_cnn" title="Link to this heading"></a></h2>
<p>Simple net for playing Atari games using PPO.</p>
<dl class="py function">
<dt class="sig sig-object py" id="trax.models.atari_cnn.AtariCnn">
<span class="sig-prename descclassname"><span class="pre">trax.models.atari_cnn.</span></span><span class="sig-name descname"><span class="pre">AtariCnn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_frames</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_sizes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(32,</span> <span class="pre">32)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">128</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.models.atari_cnn.AtariCnn" title="Link to this definition"></a></dt>
<dd><p>An Atari CNN.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.models.atari_cnn.AtariCnnBody">
<span class="sig-prename descclassname"><span class="pre">trax.models.atari_cnn.</span></span><span class="sig-name descname"><span class="pre">AtariCnnBody</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_frames</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_sizes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(32,</span> <span class="pre">64,</span> <span class="pre">64)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">512</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'VALID'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.models.atari_cnn.AtariCnnBody" title="Link to this definition"></a></dt>
<dd><p>An Atari CNN.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.models.atari_cnn.FrameStackMLP">
<span class="sig-prename descclassname"><span class="pre">trax.models.atari_cnn.</span></span><span class="sig-name descname"><span class="pre">FrameStackMLP</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_frames</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_sizes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(64,)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.models.atari_cnn.FrameStackMLP" title="Link to this definition"></a></dt>
<dd><p>MLP operating on a fixed number of last frames.</p>
</dd></dl>

</section>
<section id="module-trax.models.mlp">
<span id="mlp"></span><h2>mlp<a class="headerlink" href="#module-trax.models.mlp" title="Link to this heading"></a></h2>
<p>mlp – functions that assemble “multilayer perceptron” networks.</p>
<dl class="py function">
<dt class="sig sig-object py" id="trax.models.mlp.MLP">
<span class="sig-prename descclassname"><span class="pre">trax.models.mlp.</span></span><span class="sig-name descname"><span class="pre">MLP</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">layer_widths</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(128,</span> <span class="pre">64)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">gin.external_configurable</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">flatten</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.models.mlp.MLP" title="Link to this definition"></a></dt>
<dd><p>A “multilayer perceptron” (MLP) network.</p>
<p>This is a classic fully connected feedforward network, with one or more
layers and a (nonlinear) activation function between each layer. For
historical reasons, such networks are often called multilayer perceptrons;
but they are more accurately described as multilayer networks, where
each layer + activation function is a perceptron-like unit (see, e.g.,
[<a class="reference external" href="https://en.wikipedia.org/wiki/Multilayer_perceptron#Terminology">https://en.wikipedia.org/wiki/Multilayer_perceptron#Terminology</a>]).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>layer_widths</strong> – Tuple of ints telling the number of layers and the width of
each layer. For example, setting <cite>layer_widths=(128, 64, 32)</cite> would
yield 3 layers with successive widths of 128, 64, and 32.</p></li>
<li><p><strong>activation_fn</strong> – Type of activation function between pairs of fully connected
layers; must be an activation-type subclass of <cite>Layer</cite>.</p></li>
<li><p><strong>out_activation</strong> – If True, include a copy of the activation function as the
last layer in the network.</p></li>
<li><p><strong>flatten</strong> – If True, insert a layer at the head of the network to flatten the
input tensor into a matrix of shape (batch_size. -1).</p></li>
<li><p><strong>mode</strong> – Ignored.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>An assembled MLP network with the specified layers. This network can either
be initialized and trained as a full model, or can be used as a building
block in a larger network.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-trax.models.neural_gpu">
<span id="neural-gpu"></span><h2>neural_gpu<a class="headerlink" href="#module-trax.models.neural_gpu" title="Link to this heading"></a></h2>
<p>Implementation of the improved Neural GPU (NGPU).</p>
<dl class="py function">
<dt class="sig sig-object py" id="trax.models.neural_gpu.SaturationCost">
<span class="sig-prename descclassname"><span class="pre">trax.models.neural_gpu.</span></span><span class="sig-name descname"><span class="pre">SaturationCost</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">limit</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.9</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.models.neural_gpu.SaturationCost" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.models.neural_gpu.DiagonalGate">
<span class="sig-prename descclassname"><span class="pre">trax.models.neural_gpu.</span></span><span class="sig-name descname"><span class="pre">DiagonalGate</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#trax.models.neural_gpu.DiagonalGate" title="Link to this definition"></a></dt>
<dd><p>Split channels in 3 parts. Shifts 1st and 3rd sections to left/right.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.models.neural_gpu.ConvDiagonalGRU">
<span class="sig-prename descclassname"><span class="pre">trax.models.neural_gpu.</span></span><span class="sig-name descname"><span class="pre">ConvDiagonalGRU</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">units</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(3,</span> <span class="pre">3)</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.models.neural_gpu.ConvDiagonalGRU" title="Link to this definition"></a></dt>
<dd><p>Build convolutional GRU with diagonal gating as in ImprovedNGPU.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.models.neural_gpu.NeuralGPU">
<span class="sig-prename descclassname"><span class="pre">trax.models.neural_gpu.</span></span><span class="sig-name descname"><span class="pre">NeuralGPU</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d_feature</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">96</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">steps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vocab_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.models.neural_gpu.NeuralGPU" title="Link to this definition"></a></dt>
<dd><p>Implementation of Neural GPU: <a class="reference external" href="https://arxiv.org/abs/1702.08727">https://arxiv.org/abs/1702.08727</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>d_feature</strong> – Number of memory channels (dimensionality of feature embedding).</p></li>
<li><p><strong>steps</strong> – Number of times depthwise recurrence steps.</p></li>
<li><p><strong>vocab_size</strong> – Vocabulary size.</p></li>
<li><p><strong>mode</strong> – Whether we are training or evaluating or doing inference.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A NeuralGPU Stax model.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-trax.models.resnet">
<span id="resnet"></span><h2>resnet<a class="headerlink" href="#module-trax.models.resnet" title="Link to this heading"></a></h2>
<p>ResNet.</p>
<dl class="py function">
<dt class="sig sig-object py" id="trax.models.resnet.ConvBlock">
<span class="sig-prename descclassname"><span class="pre">trax.models.resnet.</span></span><span class="sig-name descname"><span class="pre">ConvBlock</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filters</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strides</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">non_linearity</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.models.resnet.ConvBlock" title="Link to this definition"></a></dt>
<dd><p>ResNet convolutional striding block.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.models.resnet.IdentityBlock">
<span class="sig-prename descclassname"><span class="pre">trax.models.resnet.</span></span><span class="sig-name descname"><span class="pre">IdentityBlock</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filters</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">non_linearity</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.models.resnet.IdentityBlock" title="Link to this definition"></a></dt>
<dd><p>ResNet identical size block.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.models.resnet.Resnet50">
<span class="sig-prename descclassname"><span class="pre">trax.models.resnet.</span></span><span class="sig-name descname"><span class="pre">Resnet50</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d_hidden</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_output_classes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">gin.external_configurable</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">non_linearity</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">gin.external_configurable</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.models.resnet.Resnet50" title="Link to this definition"></a></dt>
<dd><p>ResNet.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>d_hidden</strong> – Dimensionality of the first hidden layer (multiplied later).</p></li>
<li><p><strong>n_output_classes</strong> – Number of distinct output classes.</p></li>
<li><p><strong>mode</strong> – Whether we are training or evaluating or doing inference.</p></li>
<li><p><strong>norm</strong> – <cite>Layer</cite> used for normalization, Ex: BatchNorm or
FilterResponseNorm.</p></li>
<li><p><strong>non_linearity</strong> – <cite>Layer</cite> used as a non-linearity, Ex: If norm is
BatchNorm then this is a Relu, otherwise for FilterResponseNorm this
should be ThresholdedLinearUnit.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The list of layers comprising a ResNet model with the given parameters.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.models.resnet.WideResnetBlock">
<span class="sig-prename descclassname"><span class="pre">trax.models.resnet.</span></span><span class="sig-name descname"><span class="pre">WideResnetBlock</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strides</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(1,</span> <span class="pre">1)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bn_momentum</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.models.resnet.WideResnetBlock" title="Link to this definition"></a></dt>
<dd><p>WideResnet convolutional block.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.models.resnet.WideResnetGroup">
<span class="sig-prename descclassname"><span class="pre">trax.models.resnet.</span></span><span class="sig-name descname"><span class="pre">WideResnetGroup</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strides</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(1,</span> <span class="pre">1)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bn_momentum</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.models.resnet.WideResnetGroup" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.models.resnet.WideResnet">
<span class="sig-prename descclassname"><span class="pre">trax.models.resnet.</span></span><span class="sig-name descname"><span class="pre">WideResnet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_blocks</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">widen_factor</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_output_classes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bn_momentum</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.models.resnet.WideResnet" title="Link to this definition"></a></dt>
<dd><p>WideResnet from <a class="reference external" href="https://arxiv.org/pdf/1605.07146.pdf">https://arxiv.org/pdf/1605.07146.pdf</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_blocks</strong> – int, number of blocks in a group. total layers = 6n + 4.</p></li>
<li><p><strong>widen_factor</strong> – int, widening factor of each group. k=1 is vanilla resnet.</p></li>
<li><p><strong>n_output_classes</strong> – int, number of distinct output classes.</p></li>
<li><p><strong>bn_momentum</strong> – float, momentum in BatchNorm.</p></li>
<li><p><strong>mode</strong> – Whether we are training or evaluating or doing inference.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The list of layers comprising a WideResnet model with the given parameters.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-trax.models.rl">
<span id="rl"></span><h2>rl<a class="headerlink" href="#module-trax.models.rl" title="Link to this heading"></a></h2>
<p>Policy networks.</p>
<dl class="py function">
<dt class="sig sig-object py" id="trax.models.rl.Policy">
<span class="sig-prename descclassname"><span class="pre">trax.models.rl.</span></span><span class="sig-name descname"><span class="pre">Policy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">policy_distribution</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">body</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_init_range</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_axes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.models.rl.Policy" title="Link to this definition"></a></dt>
<dd><p>Attaches a policy head to a model body.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.models.rl.Value">
<span class="sig-prename descclassname"><span class="pre">trax.models.rl.</span></span><span class="sig-name descname"><span class="pre">Value</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">body</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inject_actions</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inject_actions_n_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inject_actions_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_axes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">is_discrete</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vocab_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">multiplicative_action_injection</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_init_range</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.models.rl.Value" title="Link to this definition"></a></dt>
<dd><p>Attaches a value head to a model body.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.models.rl.PolicyAndValue">
<span class="sig-prename descclassname"><span class="pre">trax.models.rl.</span></span><span class="sig-name descname"><span class="pre">PolicyAndValue</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">policy_distribution</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">body</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">policy_top</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">&lt;function</span> <span class="pre">Policy&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value_top</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">&lt;function</span> <span class="pre">Value&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">joint</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.models.rl.PolicyAndValue" title="Link to this definition"></a></dt>
<dd><p>Attaches policy and value heads to a model body.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.models.rl.Quality">
<span class="sig-prename descclassname"><span class="pre">trax.models.rl.</span></span><span class="sig-name descname"><span class="pre">Quality</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">body</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_axes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_actions</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_init_range</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.models.rl.Quality" title="Link to this definition"></a></dt>
<dd><p>The network takes as input an observation and outputs values of actions.</p>
</dd></dl>

</section>
<section id="module-trax.models.rnn">
<span id="rnn"></span><h2>rnn<a class="headerlink" href="#module-trax.models.rnn" title="Link to this heading"></a></h2>
<p>RNNs (recursive neural networks).</p>
<dl class="py function">
<dt class="sig sig-object py" id="trax.models.rnn.RNNLM">
<span class="sig-prename descclassname"><span class="pre">trax.models.rnn.</span></span><span class="sig-name descname"><span class="pre">RNNLM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">vocab_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">512</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rnn_cell</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">gin.external_configurable</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rnn_cell_d_state_multiplier</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.models.rnn.RNNLM" title="Link to this definition"></a></dt>
<dd><p>Returns an RNN language model.</p>
<p>This model performs autoregressive language modeling:</p>
<blockquote>
<div><ul class="simple">
<li><p>input: rank 2 tensor representing a batch of text strings via token IDs
plus padding markers; shape is (batch_size, sequence_length). The tensor
elements are integers in <cite>range(vocab_size)</cite>, and <cite>0</cite> values mark padding
positions.</p></li>
<li><p>output: rank 3 tensor representing a batch of log-probability
distributions for each sequence position over possible token IDs;
shape is (batch_size, sequence_length, <cite>vocab_size</cite>).</p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>vocab_size</strong> – Input vocabulary size – each element of the input tensor
should be an integer in <cite>range(vocab_size)</cite>. These integers typically
represent token IDs from a vocabulary-based tokenizer.</p></li>
<li><p><strong>d_model</strong> – Embedding depth throughout the model.</p></li>
<li><p><strong>n_layers</strong> – Number of RNN layers.</p></li>
<li><p><strong>rnn_cell</strong> – Type of RNN cell; must be a subclass of <cite>Layer</cite>.</p></li>
<li><p><strong>rnn_cell_d_state_multiplier</strong> – Multiplier for feature depth of RNN cell
state.</p></li>
<li><p><strong>dropout</strong> – Stochastic rate (probability) for dropping an activation value
when applying dropout.</p></li>
<li><p><strong>mode</strong> – If <cite>‘predict’</cite>, use fast inference; if <cite>‘train’</cite> apply dropout.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>An RNN language model as a layer that maps from a tensor of tokens
to activations over a vocab set.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.models.rnn.GRULM">
<span class="sig-prename descclassname"><span class="pre">trax.models.rnn.</span></span><span class="sig-name descname"><span class="pre">GRULM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">vocab_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">256</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">512</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.models.rnn.GRULM" title="Link to this definition"></a></dt>
<dd><p>Returns a GRU (gated recurrent unit) language model.</p>
<p>This model performs autoregressive language modeling:</p>
<blockquote>
<div><ul class="simple">
<li><p>input: rank 2 tensor representing a batch of text strings via token IDs
plus padding markers; shape is (batch_size, sequence_length). The tensor
elements are integers in <cite>range(vocab_size)</cite>, and <cite>0</cite> values mark padding
positions.</p></li>
<li><p>output: rank 3 tensor representing a batch of log-probability
distributions for each sequence position over possible token IDs;
shape is (batch_size, sequence_length, <cite>vocab_size</cite>).</p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>vocab_size</strong> – Input vocabulary size – each element of the input tensor
should be an integer in <cite>range(vocab_size)</cite>. These integers typically
represent token IDs from a vocabulary-based tokenizer.</p></li>
<li><p><strong>d_model</strong> – Embedding depth throughout the model.</p></li>
<li><p><strong>n_layers</strong> – Number of GRU layers.</p></li>
<li><p><strong>mode</strong> – If <cite>‘predict’</cite>, use fast inference (and omit the right shift).</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A GRU language model as a layer that maps from a tensor of tokens
to activations over a vocab set.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.models.rnn.LSTMSeq2SeqAttn">
<span class="sig-prename descclassname"><span class="pre">trax.models.rnn.</span></span><span class="sig-name descname"><span class="pre">LSTMSeq2SeqAttn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_vocab_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">256</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_vocab_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">256</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">512</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_encoder_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_decoder_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_attention_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.models.rnn.LSTMSeq2SeqAttn" title="Link to this definition"></a></dt>
<dd><p>Returns an LSTM sequence-to-sequence model with attention.</p>
<p>This model is an encoder-decoder that performs tokenized string-to-string
(“source”-to-“target”) transduction:</p>
<blockquote>
<div><ul>
<li><p>inputs (2):</p>
<blockquote>
<div><ul class="simple">
<li><p>source: rank 2 tensor representing a batch of text strings via token
IDs plus padding markers; shape is (batch_size, sequence_length). The
tensor elements are integers in <cite>range(input_vocab_size)</cite>, and <cite>0</cite>
values mark padding positions.</p></li>
<li><p>target: rank 2 tensor representing a batch of text strings via token
IDs plus padding markers; shape is (batch_size, sequence_length). The
tensor elements are integers in <cite>range(output_vocab_size)</cite>, and <cite>0</cite>
values mark padding positions.</p></li>
</ul>
</div></blockquote>
</li>
<li><p>output: rank 3 tensor representing a batch of log-probability
distributions for each sequence position over possible token IDs;
shape is (batch_size, sequence_length, <cite>vocab_size</cite>).</p></li>
</ul>
</div></blockquote>
<p>An example use would be to translate (tokenized) sentences from English to
German.</p>
<p>The model works as follows:</p>
<ul class="simple">
<li><p>Input encoder runs on the input tokens and creates activations that
are used as both keys and values in attention.</p></li>
<li><p>Pre-attention decoder runs on the targets and creates
activations that are used as queries in attention.</p></li>
<li><p>Attention runs on the queries, keys and values masking out input padding.</p></li>
<li><p>Decoder runs on the result, followed by a cross-entropy loss.</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_vocab_size</strong> – Input vocabulary size – each element of the input tensor
should be an integer in <cite>range(vocab_size)</cite>. These integers typically
represent token IDs from a vocabulary-based tokenizer.</p></li>
<li><p><strong>target_vocab_size</strong> – Target vocabulary size.</p></li>
<li><p><strong>d_model</strong> – Final dimension of tensors at most points in the model, including
the initial embedding output.</p></li>
<li><p><strong>n_encoder_layers</strong> – Number of LSTM layers in the encoder.</p></li>
<li><p><strong>n_decoder_layers</strong> – Number of LSTM layers in the decoder after attention.</p></li>
<li><p><strong>n_attention_heads</strong> – Number of attention heads.</p></li>
<li><p><strong>attention_dropout</strong> – Stochastic rate (probability) for dropping an activation
value when applying dropout within an attention block.</p></li>
<li><p><strong>mode</strong> – If <cite>‘predict’</cite>, use fast inference. If <cite>‘train’</cite>, each attention block
will include dropout; else, it will pass all values through unaltered.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>An LSTM sequence-to-sequence model as a layer that maps from a
source-target tokenized text pair to activations over a vocab set.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-trax.models.transformer">
<span id="transformer"></span><h2>transformer<a class="headerlink" href="#module-trax.models.transformer" title="Link to this heading"></a></h2>
<p>Transformer models: encoder, decoder, language model, and encoder-decoder.</p>
<p>The “Transformer” name and network architecture were introduced in the paper
[Attention Is All You Need](<a class="reference external" href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a>).</p>
<dl class="py function">
<dt class="sig sig-object py" id="trax.models.transformer.TransformerEncoder">
<span class="sig-prename descclassname"><span class="pre">trax.models.transformer.</span></span><span class="sig-name descname"><span class="pre">TransformerEncoder</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">vocab_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_classes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">512</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_ff</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2048</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">6</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2048</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout_shared_axes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ff_activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">gin.external_configurable</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.models.transformer.TransformerEncoder" title="Link to this definition"></a></dt>
<dd><p>Returns a Transformer encoder suitable for N-way classification.</p>
<p>This model maps tokenized text to N-way (<code class="docutils literal notranslate"><span class="pre">n_classes</span></code>) activations:</p>
<blockquote>
<div><ul class="simple">
<li><p>input: Array representing a batch of text strings via token IDs plus
padding markers; shape is (batch_size, sequence_length), where
sequence_length &lt;= <code class="docutils literal notranslate"><span class="pre">max_len</span></code>. Array elements are integers in
<code class="docutils literal notranslate"><span class="pre">range(vocab_size)</span></code>, and 0 values mark padding positions.</p></li>
<li><p>output: Array representing a batch of raw (non-normalized) activations
over <code class="docutils literal notranslate"><span class="pre">n_classes</span></code> categories; shape is (batch_size, <code class="docutils literal notranslate"><span class="pre">n_classes</span></code>).</p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>vocab_size</strong> – Input vocabulary size – each element of the input array
should be an integer in <code class="docutils literal notranslate"><span class="pre">range(vocab_size)</span></code>. These integers typically
represent token IDs from a vocabulary-based tokenizer.</p></li>
<li><p><strong>n_classes</strong> – Last/innermost dimension of output arrays, suitable for N-way
classification.</p></li>
<li><p><strong>d_model</strong> – Last/innermost dimension of activation arrays at most points in
the model, including the initial embedding output.</p></li>
<li><p><strong>d_ff</strong> – Last/innermost dimension of special (typically wider)
<code class="xref py py-class docutils literal notranslate"><span class="pre">Dense</span></code> layer in the feedforward part of each encoder block.</p></li>
<li><p><strong>n_layers</strong> – Number of encoder blocks. Each block includes attention, dropout,
residual, layer-norm, feedforward (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dense</span></code>), and activation
layers.</p></li>
<li><p><strong>n_heads</strong> – Number of attention heads.</p></li>
<li><p><strong>max_len</strong> – Maximum symbol length for positional encoding.</p></li>
<li><p><strong>dropout</strong> – Stochastic rate (probability) for dropping an activation value
when applying dropout within encoder blocks. The same rate is also
used for attention dropout in encoder blocks.</p></li>
<li><p><strong>dropout_shared_axes</strong> – Tensor axes on which to share a dropout mask.
Sharing along batch and sequence axes (<code class="docutils literal notranslate"><span class="pre">dropout_shared_axes=(0,1)</span></code>)
is a useful way to save memory and apply consistent masks to activation
vectors at different sequence positions.</p></li>
<li><p><strong>mode</strong> – If <code class="docutils literal notranslate"><span class="pre">'train'</span></code>, each encoder block will include dropout; else, it
will pass all values through unaltered.</p></li>
<li><p><strong>ff_activation</strong> – Type of activation function at the end of each encoder
block; must be an activation-type subclass of <code class="xref py py-class docutils literal notranslate"><span class="pre">Layer</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A Transformer model that maps strings (conveyed by token IDs) to
raw (non-normalized) activations over a range of output classes.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.models.transformer.TransformerDecoder">
<span class="sig-prename descclassname"><span class="pre">trax.models.transformer.</span></span><span class="sig-name descname"><span class="pre">TransformerDecoder</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">vocab_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">512</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_ff</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2048</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">6</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2048</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout_shared_axes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ff_activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">gin.external_configurable</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.models.transformer.TransformerDecoder" title="Link to this definition"></a></dt>
<dd><p>Returns a Transformer decoder.</p>
<p>This model maps sequential inputs to sequential outputs:</p>
<blockquote>
<div><ul class="simple">
<li><p>input if <code class="docutils literal notranslate"><span class="pre">vocab_size</span></code> is specified: array representing a batch
of text strings via token IDs plus padding markers; shape is
(batch_size, sequence_length). The tensor elements are integers in
<code class="docutils literal notranslate"><span class="pre">range(vocab_size)</span></code>, and 0 values mark padding positions.</p></li>
<li><p>input if <code class="docutils literal notranslate"><span class="pre">vocab_size</span></code> is <code class="docutils literal notranslate"><span class="pre">None</span></code>: 3-D array representing a batch of
sequences of activation vectors; shape is (batch_size, sequence_length,
<code class="docutils literal notranslate"><span class="pre">d_model</span></code>).</p></li>
<li><p>output: 3-D array with shape (batch_size, sequence_length, <code class="docutils literal notranslate"><span class="pre">d_model</span></code>).</p></li>
</ul>
</div></blockquote>
<p>The model uses causal attention and does <em>not</em> shift the input to the right.
Thus, the output for position <cite>t</cite> is based on inputs up to and including
position <cite>t</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>vocab_size</strong> – If specified, gives the input vocabulary size – each element
of the input tensor should be an integer in <code class="docutils literal notranslate"><span class="pre">range(vocab_size)</span></code>.
If <code class="docutils literal notranslate"><span class="pre">None</span></code>, indicates that the model expects as input sequences of
floating point vectors, each with <code class="docutils literal notranslate"><span class="pre">d_model</span></code> components.</p></li>
<li><p><strong>d_model</strong> – Last/innermost dimension of activation arrays at most points in
the model, including the initial embedding output.</p></li>
<li><p><strong>d_ff</strong> – Last/innermost dimension of special (typically wider)
<code class="xref py py-class docutils literal notranslate"><span class="pre">Dense</span></code> layer in the feedforward part of each encoder block.</p></li>
<li><p><strong>n_layers</strong> – Number of decoder blocks. Each block includes attention, dropout,
residual, layer-norm, feedforward (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dense</span></code>), and activation
layers.</p></li>
<li><p><strong>n_heads</strong> – Number of attention heads.</p></li>
<li><p><strong>max_len</strong> – Maximum symbol length for positional encoding.</p></li>
<li><p><strong>dropout</strong> – Stochastic rate (probability) for dropping an activation value
when applying dropout within decoder blocks. The same rate is also
used for attention dropout in decoder blocks.</p></li>
<li><p><strong>dropout_shared_axes</strong> – Tensor axes on which to share a dropout mask.
Sharing along batch and sequence axes (<code class="docutils literal notranslate"><span class="pre">dropout_shared_axes=(0,1)</span></code>)
is a useful way to save memory and apply consistent masks to activation
vectors at different sequence positions.</p></li>
<li><p><strong>mode</strong> – If <code class="docutils literal notranslate"><span class="pre">'train'</span></code>, each encoder block will include dropout; else, it
will pass all values through unaltered.</p></li>
<li><p><strong>ff_activation</strong> – Type of activation function at the end of each encoder
block; must be an activation-type subclass of <code class="xref py py-class docutils literal notranslate"><span class="pre">Layer</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><p>a Transformer model that maps strings
(conveyed by token IDs) to sequences of activation vectors.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">vocab_size</span></code> is <code class="docutils literal notranslate"><span class="pre">None</span></code>: a Transformer model that maps sequences of
activation vectors to sequences of activation vectors.</p>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>If <code class="docutils literal notranslate"><span class="pre">vocab_size</span></code> is defined</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.models.transformer.TransformerLM">
<span class="sig-prename descclassname"><span class="pre">trax.models.transformer.</span></span><span class="sig-name descname"><span class="pre">TransformerLM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">vocab_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">512</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_ff</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2048</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">6</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2048</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout_shared_axes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ff_activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">gin.external_configurable</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.models.transformer.TransformerLM" title="Link to this definition"></a></dt>
<dd><p>Returns a Transformer language model.</p>
<p>This model performs autoregressive language modeling:</p>
<blockquote>
<div><ul class="simple">
<li><p>input: Array representing a batch of text strings via token IDs
plus padding markers; shape is (batch_size, sequence_length). Array
elements are integers in <code class="docutils literal notranslate"><span class="pre">range(vocab_size)</span></code>, and 0 values mark padding
positions.</p></li>
<li><p>output: 3-D array of raw activations with last/innermost dimension of
<code class="docutils literal notranslate"><span class="pre">vocab_size</span></code>, suitable for decoding into a batch of token strings;
shape is (batch_size, sequence_length, <code class="docutils literal notranslate"><span class="pre">vocab_size</span></code>).</p></li>
</ul>
</div></blockquote>
<p>This model uses only the decoder part of the overall Transformer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>vocab_size</strong> – Input vocabulary size – each element of the input array
should be an integer in <code class="docutils literal notranslate"><span class="pre">range(vocab_size)</span></code>. These integers typically
represent token IDs from a vocabulary-based tokenizer.</p></li>
<li><p><strong>d_model</strong> – Last/innermost dimension of activation arrays at most points in
the model, including the initial embedding output.</p></li>
<li><p><strong>d_ff</strong> – Last/innermost dimension of special (typically wider)
<code class="xref py py-class docutils literal notranslate"><span class="pre">Dense</span></code> layer in the feedforward part of each encoder block.</p></li>
<li><p><strong>n_layers</strong> – Number of decoder blocks. Each block includes attention, dropout,
residual, layer-norm, feedforward (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dense</span></code>), and activation
layers.</p></li>
<li><p><strong>n_heads</strong> – Number of attention heads.</p></li>
<li><p><strong>max_len</strong> – Maximum symbol length for positional encoding.</p></li>
<li><p><strong>dropout</strong> – Stochastic rate (probability) for dropping an activation value
when applying dropout within decoder blocks. The same rate is also
used for attention dropout in decoder blocks.</p></li>
<li><p><strong>dropout_shared_axes</strong> – Tensor axes on which to share a dropout mask.
Sharing along batch and sequence axes (<code class="docutils literal notranslate"><span class="pre">dropout_shared_axes=(0,1)</span></code>)
is a useful way to save memory and apply consistent masks to activation
vectors at different sequence positions.</p></li>
<li><p><strong>mode</strong> – If <code class="docutils literal notranslate"><span class="pre">'predict'</span></code>, use fast inference. If <code class="docutils literal notranslate"><span class="pre">'train'</span></code>, each decoder
block will include dropout; else, it will pass all values through
unaltered.</p></li>
<li><p><strong>ff_activation</strong> – Type of activation function at the end of each encoder
block; must be an activation-type subclass of <code class="xref py py-class docutils literal notranslate"><span class="pre">Layer</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A Transformer language model that maps strings (represented as token ID
sequences) to sequences of raw (non-normalized) activation vectors; each
vector in the sequence can be mapped (e.g., by <cite>argmax</cite>) to a token ID.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.models.transformer.Transformer">
<span class="sig-prename descclassname"><span class="pre">trax.models.transformer.</span></span><span class="sig-name descname"><span class="pre">Transformer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_vocab_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_vocab_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">512</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_ff</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2048</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_encoder_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">6</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_decoder_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">6</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2048</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout_shared_axes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ff_activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">gin.external_configurable</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.models.transformer.Transformer" title="Link to this definition"></a></dt>
<dd><p>Returns a full Transformer model.</p>
<p>This model is an encoder-decoder that performs tokenized string-to-string
(“source”-to-“target”) transduction:</p>
<blockquote>
<div><ul>
<li><p>inputs (2):</p>
<blockquote>
<div><ul class="simple">
<li><p>source: Array representing a batch of text strings via token
IDs plus padding markers; shape is (batch_size, sequence_length),
where sequence_length &lt;= <code class="docutils literal notranslate"><span class="pre">max_len</span></code>. Array elements are integers in
<code class="docutils literal notranslate"><span class="pre">range(input_vocab_size)</span></code>, and 0 values mark padding positions.</p></li>
<li><p>target: Array representing a batch of text strings via token
IDs plus padding markers; shape is (batch_size, sequence_length),
where sequence_length &lt;= <code class="docutils literal notranslate"><span class="pre">max_len</span></code>. Array elements are integers in
<code class="docutils literal notranslate"><span class="pre">range(output_vocab_size)</span></code>, and 0 values mark padding positions.</p></li>
</ul>
</div></blockquote>
</li>
<li><p>output: 3-D array of raw activations with last/innermost dimension of
<code class="docutils literal notranslate"><span class="pre">output_vocab_size</span></code>, suitable for decoding into a batch of token
strings; shape is (batch_size, sequence_length, <code class="docutils literal notranslate"><span class="pre">vocab_size</span></code>).</p></li>
</ul>
</div></blockquote>
<p>An example use would be to translate (tokenized) sentences from English to
German.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_vocab_size</strong> – Input vocabulary size – each element of the input tensor
should be an integer in <code class="docutils literal notranslate"><span class="pre">range(vocab_size)</span></code>. These integers typically
represent token IDs from a vocabulary-based tokenizer.</p></li>
<li><p><strong>output_vocab_size</strong> – If specified, gives the vocabulary size for the targets;
if <code class="docutils literal notranslate"><span class="pre">None</span></code>, then input and target integers (token IDs) are assumed to
come from the same vocabulary.</p></li>
<li><p><strong>d_model</strong> – Last/innermost dimension of activation arrays at most points in
the model, including the initial embedding output.</p></li>
<li><p><strong>d_ff</strong> – Last/innermost dimension of special (typically wider)
<code class="xref py py-class docutils literal notranslate"><span class="pre">Dense</span></code> layer in the feedforward part of each encoder block.</p></li>
<li><p><strong>n_encoder_layers</strong> – Number of encoder blocks.</p></li>
<li><p><strong>n_decoder_layers</strong> – Number of decoder blocks.</p></li>
<li><p><strong>n_heads</strong> – Number of attention heads.</p></li>
<li><p><strong>max_len</strong> – Maximum symbol length for positional encoding.</p></li>
<li><p><strong>dropout</strong> – Stochastic rate (probability) for dropping an activation value
when applying dropout within encoder/decoder blocks. The same rate is
also used for attention dropout in encoder/decoder blocks.</p></li>
<li><p><strong>dropout_shared_axes</strong> – Tensor axes on which to share a dropout mask.
Sharing along batch and sequence axes (<code class="docutils literal notranslate"><span class="pre">dropout_shared_axes=(0,1)</span></code>)
is a useful way to save memory and apply consistent masks to activation
vectors at different sequence positions.</p></li>
<li><p><strong>mode</strong> – If <code class="docutils literal notranslate"><span class="pre">'predict'</span></code>, use fast inference. If <code class="docutils literal notranslate"><span class="pre">'train'</span></code>, each
encoder/decoder block will include dropout; else, it will pass all
values through unaltered.</p></li>
<li><p><strong>ff_activation</strong> – Type of activation function at the end of each
encoder/decoder block; must be an activation-type subclass of
<code class="xref py py-class docutils literal notranslate"><span class="pre">Layer</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A Transformer model as a layer that maps from a source-target tokenized
text pair to activations over a vocab set.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-trax.models.reformer.reformer">
<span id="reformer-reformer"></span><h2>reformer.reformer<a class="headerlink" href="#module-trax.models.reformer.reformer" title="Link to this heading"></a></h2>
<p>Reformer Models.</p>
<dl class="py function">
<dt class="sig sig-object py" id="trax.models.reformer.reformer.DecoderBlock">
<span class="sig-prename descclassname"><span class="pre">trax.models.reformer.reformer.</span></span><span class="sig-name descname"><span class="pre">DecoderBlock</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d_model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_ff</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_attention_key</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_attention_value</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_type</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ff_activation</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ff_dropout</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ff_use_sru</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ff_chunk_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ff_sparsity</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_chunk_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_attention_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_feedforward_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">center_layernorm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_bfloat16</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.models.reformer.reformer.DecoderBlock" title="Link to this definition"></a></dt>
<dd><p>Reversible transformer decoder layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>d_model</strong> – int:  depth of embedding</p></li>
<li><p><strong>d_ff</strong> – int: depth of feed-forward layer</p></li>
<li><p><strong>d_attention_key</strong> – int: depth of key vector for each attention head</p></li>
<li><p><strong>d_attention_value</strong> – int: depth of value vector for each attention head</p></li>
<li><p><strong>n_heads</strong> – int: number of attention heads</p></li>
<li><p><strong>attention_type</strong> – subclass of tl.BaseCausalAttention: attention class to use</p></li>
<li><p><strong>dropout</strong> – float: dropout rate (how much to drop out)</p></li>
<li><p><strong>ff_activation</strong> – the non-linearity in feed-forward layer</p></li>
<li><p><strong>ff_dropout</strong> – the dropout rate in feed-forward layer</p></li>
<li><p><strong>ff_use_sru</strong> – int; if &gt; 0, we use this many SRU layers instead of feed-forward</p></li>
<li><p><strong>ff_chunk_size</strong> – int; if &gt; 0, chunk feed-forward into this-sized chunks</p></li>
<li><p><strong>ff_sparsity</strong> – int, if &gt; 0 use sparse feed-forward block with this sparsity</p></li>
<li><p><strong>attention_chunk_size</strong> – int, if &gt; 0 run attention chunked at this size</p></li>
<li><p><strong>n_attention_layers</strong> – how many residual causal attention layers should we
have before the feed-forward block (default: 1, the standard block)</p></li>
<li><p><strong>n_feedforward_layers</strong> – how many FFNN layers should we have (default 1).</p></li>
<li><p><strong>center_layernorm</strong> – whether to use centering in LayerNorm (default) or if
to skip it, which is known as RMS normalization.</p></li>
<li><p><strong>use_bfloat16</strong> – whether to use bfloat16 for weights (default: False).</p></li>
<li><p><strong>mode</strong> – str: ‘train’ or ‘eval’</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>the layer.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.models.reformer.reformer.ReformerLM">
<span class="sig-prename descclassname"><span class="pre">trax.models.reformer.reformer.</span></span><span class="sig-name descname"><span class="pre">ReformerLM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">vocab_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">512</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_ff</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2048</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_attention_key</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_attention_value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">6</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2048</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">gin.external_configurable</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_axial_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_d_axial_embs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_start_from_zero_prob</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_max_offset_to_add</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ff_activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">gin.external_configurable</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ff_use_sru</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ff_chunk_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ff_sparsity</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_sparsity_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mult'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_sparsity</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_d_lowrank</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_sparsity_prob</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_chunk_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.models.reformer.reformer.ReformerLM" title="Link to this definition"></a></dt>
<dd><p>Reversible transformer language model (only uses a decoder, no encoder).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>vocab_size</strong> – int: vocab size</p></li>
<li><p><strong>d_model</strong> – int:  depth of <em>each half</em> of the two-part features</p></li>
<li><p><strong>d_ff</strong> – int: depth of feed-forward layer</p></li>
<li><p><strong>d_attention_key</strong> – int: depth of key vector for each attention head</p></li>
<li><p><strong>d_attention_value</strong> – int: depth of value vector for each attention head</p></li>
<li><p><strong>n_layers</strong> – int: number of decoder layers</p></li>
<li><p><strong>n_heads</strong> – int: number of attention heads</p></li>
<li><p><strong>dropout</strong> – float: dropout rate (how much to drop out)</p></li>
<li><p><strong>max_len</strong> – int: maximum symbol length for positional encoding</p></li>
<li><p><strong>attention_type</strong> – class: attention class to use, such as SelfAttention.</p></li>
<li><p><strong>pos_type</strong> – string, the type of positional embeddings to use.</p></li>
<li><p><strong>pos_axial_shape</strong> – tuple of ints: input shape to use for the axial position
encoding. If unset, axial position encoding is disabled.</p></li>
<li><p><strong>pos_d_axial_embs</strong> – tuple of ints: depth of position embedding for each axis.
Tuple length must match pos_axial_shape, and values must sum to d_model.</p></li>
<li><p><strong>pos_start_from_zero_prob</strong> – how often to start from 0 during training,
(if 1.0, we always start from position 0, if less, we randomize).</p></li>
<li><p><strong>pos_max_offset_to_add</strong> – maximum offset to add to positions during training
when randomizing; this offset plus input length must still be less than
max_len for all training examples.</p></li>
<li><p><strong>ff_activation</strong> – the non-linearity in feed-forward layer</p></li>
<li><p><strong>ff_use_sru</strong> – int; if &gt; 0, we use this many SRU layers instead of feed-forward</p></li>
<li><p><strong>ff_chunk_size</strong> – int; if &gt; 0, chunk feed-forward into this-sized chunks</p></li>
<li><p><strong>ff_sparsity</strong> – int, if &gt; 0 use sparse feed-forward block with this sparsity</p></li>
<li><p><strong>loss_sparsity_type</strong> – str, type of sparsity to used in loss layer. See
SparseDenseWithOptions for options. None if no sparsity should be used.</p></li>
<li><p><strong>loss_sparsity</strong> – int, the sparsity for loss layer (if used)</p></li>
<li><p><strong>loss_d_lowrank</strong> – int, the dimensions for intermediate layer (if used)</p></li>
<li><p><strong>loss_sparsity_prob</strong> – float, the probability for sparse version of loss to be
used. If None, only sparse version is used.</p></li>
<li><p><strong>attention_chunk_size</strong> – int, if &gt; 0 run attention chunked at this size</p></li>
<li><p><strong>mode</strong> – str: ‘train’, ‘eval’, or ‘predict’</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>the layer.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.models.reformer.reformer.ReformerShortenLM">
<span class="sig-prename descclassname"><span class="pre">trax.models.reformer.reformer.</span></span><span class="sig-name descname"><span class="pre">ReformerShortenLM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">vocab_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shorten_factor</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_embedding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">256</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">512</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_ff</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2048</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_attention_key</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_attention_value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">6</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2048</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">gin.external_configurable</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_axial_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_d_axial_embs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ff_activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">gin.external_configurable</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ff_use_sru</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ff_chunk_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ff_sparsity</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_chunk_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.models.reformer.reformer.ReformerShortenLM" title="Link to this definition"></a></dt>
<dd><p>Reversible transformer language model with shortening.</p>
<p>When shorten_factor is F and processing an input of shape [batch, length],
we embed the (shifted-right) input and then group each F elements (on length)
into a single vector – so that in the end we process a tensor of shape</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="n">length</span> <span class="o">//</span> <span class="n">F</span><span class="p">,</span> <span class="n">d_model</span><span class="p">]</span>
</pre></div>
</div>
<p>almost until the end – at the end it’s un-shortend and a SRU is applied.
This reduces the length processed inside the main model body, effectively
making the model faster but possibly slightly less accurate.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>vocab_size</strong> – int: vocab size</p></li>
<li><p><strong>shorten_factor</strong> – by how much to shorten, see above</p></li>
<li><p><strong>d_embedding</strong> – the depth of the embedding layer and final logits</p></li>
<li><p><strong>d_model</strong> – int:  depth of <em>each half</em> of the two-part features</p></li>
<li><p><strong>d_ff</strong> – int: depth of feed-forward layer</p></li>
<li><p><strong>d_attention_key</strong> – int: depth of key vector for each attention head</p></li>
<li><p><strong>d_attention_value</strong> – int: depth of value vector for each attention head</p></li>
<li><p><strong>n_layers</strong> – int: number of decoder layers</p></li>
<li><p><strong>n_heads</strong> – int: number of attention heads</p></li>
<li><p><strong>dropout</strong> – float: dropout rate (how much to drop out)</p></li>
<li><p><strong>max_len</strong> – int: maximum symbol length for positional encoding</p></li>
<li><p><strong>attention_type</strong> – class: attention class to use, such as SelfAttention.</p></li>
<li><p><strong>pos_type</strong> – string, the type of positional embeddings to use.</p></li>
<li><p><strong>pos_axial_shape</strong> – tuple of ints: input shape to use for the axial position
encoding. If unset, axial position encoding is disabled.</p></li>
<li><p><strong>pos_d_axial_embs</strong> – tuple of ints: depth of position embedding for each axis.
Tuple length must match pos_axial_shape, values must sum to d_embedding.</p></li>
<li><p><strong>ff_activation</strong> – the non-linearity in feed-forward layer</p></li>
<li><p><strong>ff_use_sru</strong> – int; if &gt; 0, we use this many SRU layers instead of feed-forward</p></li>
<li><p><strong>ff_chunk_size</strong> – int; if &gt; 0, chunk feed-forward into this-sized chunks</p></li>
<li><p><strong>ff_sparsity</strong> – int, if &gt; 0 use sparse feed-forward block with this sparsity</p></li>
<li><p><strong>attention_chunk_size</strong> – int, if &gt; 0 run attention chunked at this size</p></li>
<li><p><strong>mode</strong> – str: ‘train’ or ‘eval’</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>the layer.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.models.reformer.reformer.EncoderBlock">
<span class="sig-prename descclassname"><span class="pre">trax.models.reformer.reformer.</span></span><span class="sig-name descname"><span class="pre">EncoderBlock</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d_model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_ff</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_type</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ff_activation</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ff_dropout</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ff_use_sru</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ff_chunk_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ff_sparsity</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_chunk_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">center_layernorm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_bfloat16</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_two_swaps_per_block</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.models.reformer.reformer.EncoderBlock" title="Link to this definition"></a></dt>
<dd><p>Returns a list of layers that implements a Reformer encoder block.</p>
<p>The input to the layer is a pair, (activations, mask), where the mask was
created from the original source tokens to prevent attending to the padding
part of the input.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>d_model</strong> – int:  depth of embedding</p></li>
<li><p><strong>d_ff</strong> – int: depth of feed-forward layer</p></li>
<li><p><strong>n_heads</strong> – int: number of attention heads</p></li>
<li><p><strong>attention_type</strong> – subclass of tl.BaseCausalAttention: attention class to use</p></li>
<li><p><strong>dropout</strong> – float: dropout rate (how much to drop out)</p></li>
<li><p><strong>ff_activation</strong> – the non-linearity in feed-forward layer</p></li>
<li><p><strong>ff_dropout</strong> – the dropout rate in feed-forward layer</p></li>
<li><p><strong>ff_use_sru</strong> – int; if &gt; 0, we use this many SRU layers instead of feed-forward</p></li>
<li><p><strong>ff_chunk_size</strong> – int; if &gt; 0, chunk feed-forward into this-sized chunks</p></li>
<li><p><strong>ff_sparsity</strong> – int, if &gt; 0 use sparse feed-forward block with this sparsity</p></li>
<li><p><strong>attention_chunk_size</strong> – int, if &gt; 0 run attention chunked at this size</p></li>
<li><p><strong>center_layernorm</strong> – whether to use centering in LayerNorm (default) or if
to skip it, which is known as RMS normalization.</p></li>
<li><p><strong>use_bfloat16</strong> – whether to use bfloat16 for weights (default: False)</p></li>
<li><p><strong>use_two_swaps_per_block</strong> – bool, if True use two reversible swaps in Encoder
block, otherwise use only one swap.</p></li>
<li><p><strong>mode</strong> – str: ‘train’ or ‘eval’</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A list of layers that maps (activations, mask) to (activations, mask).</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.models.reformer.reformer.EncoderDecoderBlock">
<span class="sig-prename descclassname"><span class="pre">trax.models.reformer.reformer.</span></span><span class="sig-name descname"><span class="pre">EncoderDecoderBlock</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d_model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_ff</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ff_activation</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ff_dropout</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ff_use_sru</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ff_chunk_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ff_sparsity</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.models.reformer.reformer.EncoderDecoderBlock" title="Link to this definition"></a></dt>
<dd><p>Reversible transformer decoder layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>d_model</strong> – int:  depth of embedding</p></li>
<li><p><strong>d_ff</strong> – int: depth of feed-forward layer</p></li>
<li><p><strong>n_heads</strong> – int: number of attention heads</p></li>
<li><p><strong>dropout</strong> – float: dropout rate (how much to drop out)</p></li>
<li><p><strong>ff_activation</strong> – the non-linearity in feed-forward layer</p></li>
<li><p><strong>ff_dropout</strong> – float: (optional) separate dropout rate for feed-forward layer</p></li>
<li><p><strong>mode</strong> – str: ‘train’ or ‘eval’</p></li>
<li><p><strong>ff_use_sru</strong> – int; if &gt; 0, we use this many SRU layers instead of feed-forward</p></li>
<li><p><strong>ff_chunk_size</strong> – int; if &gt; 0, chunk feed-forward into this-sized chunks</p></li>
<li><p><strong>ff_sparsity</strong> – int, if &gt; 0 use sparse feed-forward block with this sparsity</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>the layer.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.models.reformer.reformer.Reformer">
<span class="sig-prename descclassname"><span class="pre">trax.models.reformer.reformer.</span></span><span class="sig-name descname"><span class="pre">Reformer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_vocab_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_vocab_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">512</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_ff</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2048</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_encoder_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">6</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_decoder_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">6</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2048</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ff_activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">gin.external_configurable</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ff_dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_axial_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_d_axial_embs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ff_use_sru</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ff_chunk_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ff_sparsity</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.models.reformer.reformer.Reformer" title="Link to this definition"></a></dt>
<dd><p>Reversible transformer encoder-decoder model.</p>
<p>This model expects an input pair: target, source.</p>
<p>At the moment, this model supports dot-product attention only. For the
attention types in the Reformer paper, see ReformerLM.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_vocab_size</strong> – int: vocab size of the source.</p></li>
<li><p><strong>output_vocab_size</strong> – int (optional): vocab size of the target. If None, the
source and target are assumed to have the same vocab.</p></li>
<li><p><strong>d_model</strong> – int:  depth of embedding</p></li>
<li><p><strong>d_ff</strong> – int: depth of feed-forward layer</p></li>
<li><p><strong>n_encoder_layers</strong> – int: number of encoder layers</p></li>
<li><p><strong>n_decoder_layers</strong> – int: number of decoder layers</p></li>
<li><p><strong>n_heads</strong> – int: number of attention heads</p></li>
<li><p><strong>dropout</strong> – float: dropout rate (how much to drop out)</p></li>
<li><p><strong>max_len</strong> – int: maximum symbol length for positional encoding</p></li>
<li><p><strong>ff_activation</strong> – the non-linearity in feed-forward layer</p></li>
<li><p><strong>ff_dropout</strong> – float: (optional) separate dropout rate at feed-forward
nonlinearity. This is called relu_dropout in T2T.</p></li>
<li><p><strong>mode</strong> – str: ‘train’ or ‘eval’</p></li>
<li><p><strong>pos_type</strong> – string, the type of positional embeddings to use.</p></li>
<li><p><strong>pos_axial_shape</strong> – tuple of ints: input shape to use for the axial position
encoding. If unset, axial position encoding is disabled.</p></li>
<li><p><strong>pos_d_axial_embs</strong> – tuple of ints: depth of position embedding for each axis.
Tuple length must match pos_axial_shape, and values must sum to d_model.</p></li>
<li><p><strong>ff_use_sru</strong> – int; if &gt; 0, we use this many SRU layers instead of feed-forward</p></li>
<li><p><strong>ff_chunk_size</strong> – int; if &gt; 0, chunk feed-forward into this-sized chunks</p></li>
<li><p><strong>ff_sparsity</strong> – int, if &gt; 0 use sparse feed-forward block with this sparsity</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A Reformer model as a layer that maps from a target, source pair to
activations over a vocab set.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-trax.models.research.bert">
<span id="research-bert"></span><h2>research.bert<a class="headerlink" href="#module-trax.models.research.bert" title="Link to this heading"></a></h2>
<p>BERT.</p>
<dl class="py class">
<dt class="sig sig-object py" id="trax.models.research.bert.AddBias">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.models.research.bert.</span></span><span class="sig-name descname"><span class="pre">AddBias</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_in</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_out</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sublayers_to_print</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.models.research.bert.AddBias" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="trax.layers.html#trax.layers.base.Layer" title="trax.layers.base.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Layer</span></code></a></p>
<dl class="py method">
<dt class="sig sig-object py" id="trax.models.research.bert.AddBias.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.models.research.bert.AddBias.forward" title="Link to this definition"></a></dt>
<dd><p>Computes this layer’s output as part of a forward pass through the model.</p>
<p>A layer subclass overrides this method to define how the layer computes
outputs from inputs. If the layer depends on weights, state, or randomness
as part of the computation, the needed information can be accessed as
properties of the layer object: <cite>self.weights</cite>, <cite>self.state</cite>, and
<cite>self.rng</cite>. (See numerous examples in <cite>trax.layers.core</cite>.)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>inputs</strong> – Zero or more input tensors, packaged as described in the <cite>Layer</cite>
class docstring.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Zero or more output tensors, packaged as described in the <cite>Layer</cite> class
docstring.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.models.research.bert.AddBias.init_weights_and_state">
<span class="sig-name descname"><span class="pre">init_weights_and_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_signature</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.models.research.bert.AddBias.init_weights_and_state" title="Link to this definition"></a></dt>
<dd><p>Initializes weights and state, to handle input with the given signature.</p>
<p>A layer subclass must override this method if the layer uses weights or
state. To initialize weights, set <cite>self.weights</cite> to desired (typically
random) values. To initialize state (uncommon), set <cite>self.state</cite> to desired
starting values.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input_signature</strong> – A <cite>ShapeDtype</cite> instance (if this layer takes one input)
or a list/tuple of <cite>ShapeDtype</cite> instances.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.models.research.bert.BERTClassifierHead">
<span class="sig-prename descclassname"><span class="pre">trax.models.research.bert.</span></span><span class="sig-name descname"><span class="pre">BERTClassifierHead</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_classes</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.models.research.bert.BERTClassifierHead" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.models.research.bert.BERTRegressionHead">
<span class="sig-prename descclassname"><span class="pre">trax.models.research.bert.</span></span><span class="sig-name descname"><span class="pre">BERTRegressionHead</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#trax.models.research.bert.BERTRegressionHead" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.models.research.bert.BERTMLMHead">
<span class="sig-prename descclassname"><span class="pre">trax.models.research.bert.</span></span><span class="sig-name descname"><span class="pre">BERTMLMHead</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">vocab_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">30522</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.models.research.bert.BERTMLMHead" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.models.research.bert.BERTPretrainingLoss">
<span class="sig-prename descclassname"><span class="pre">trax.models.research.bert.</span></span><span class="sig-name descname"><span class="pre">BERTPretrainingLoss</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#trax.models.research.bert.BERTPretrainingLoss" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.models.research.bert.BERTPretrainingHead">
<span class="sig-prename descclassname"><span class="pre">trax.models.research.bert.</span></span><span class="sig-name descname"><span class="pre">BERTPretrainingHead</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_classes</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.models.research.bert.BERTPretrainingHead" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.models.research.bert.BERT">
<span class="sig-prename descclassname"><span class="pre">trax.models.research.bert.</span></span><span class="sig-name descname"><span class="pre">BERT</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d_model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">768</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vocab_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">30522</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">512</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">type_vocab_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">12</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_ff</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3072</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">12</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_checkpoint</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'eval'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.models.research.bert.BERT" title="Link to this definition"></a></dt>
<dd><p>BERT (default hparams are for bert-training-uncased).</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="trax.models.research.bert.PretrainedBERT">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.models.research.bert.</span></span><span class="sig-name descname"><span class="pre">PretrainedBERT</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">sublayers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_checkpoint</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.models.research.bert.PretrainedBERT" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="trax.layers.html#trax.layers.combinators.Serial" title="trax.layers.combinators.Serial"><code class="xref py py-class docutils literal notranslate"><span class="pre">Serial</span></code></a></p>
<p>Wrapper that always initializes weights from a pre-trained checkpoint.</p>
<dl class="py method">
<dt class="sig sig-object py" id="trax.models.research.bert.PretrainedBERT.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">sublayers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_checkpoint</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.models.research.bert.PretrainedBERT.__init__" title="Link to this definition"></a></dt>
<dd><p>Creates a partially initialized, unconnected layer instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_in</strong> – Number of inputs expected by this layer.</p></li>
<li><p><strong>n_out</strong> – Number of outputs promised by this layer.</p></li>
<li><p><strong>name</strong> – Class-like name for this layer; for use when printing this layer.</p></li>
<li><p><strong>sublayers_to_print</strong> – Sublayers to display when printing out this layer;
if None (the default), display all sublayers.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.models.research.bert.PretrainedBERT.download_model">
<span class="property"><span class="k"><span class="pre">classmethod</span></span><span class="w"> </span></span><span class="sig-name descname"><span class="pre">download_model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model_name</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.models.research.bert.PretrainedBERT.download_model" title="Link to this definition"></a></dt>
<dd><p>Returns model dir path with model filename.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.models.research.bert.PretrainedBERT.init_weights_and_state">
<span class="sig-name descname"><span class="pre">init_weights_and_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_signature</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.models.research.bert.PretrainedBERT.init_weights_and_state" title="Link to this definition"></a></dt>
<dd><p>Initializes weights and state for inputs with the given signature.</p>
</dd></dl>

</dd></dl>

</section>
<section id="research-skipping-transformer">
<h2>research.skipping_transformer<a class="headerlink" href="#research-skipping-transformer" title="Link to this heading"></a></h2>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="trax.layers.html" class="btn btn-neutral float-left" title="trax.layers" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="trax.data.html" class="btn btn-neutral float-right" title="trax.data" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, Google LLC..</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>