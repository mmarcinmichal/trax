

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Trax Layers Intro &mdash; Trax  documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=9edc463e" />
      <link rel="stylesheet" type="text/css" href="../_static/nbsphinx-code-cells.css?v=2aa19091" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=5929fcd5"></script>
      <script src="../_static/doctools.js?v=fd6eb6e6"></script>
      <script src="../_static/sphinx_highlight.js?v=6ffebe34"></script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
      <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="trax" href="../trax.html" />
    <link rel="prev" title="Trax Quick Intro" href="trax_intro.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Trax
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Introductory Notebooks</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="trax_intro.html">Trax Quick Intro</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Trax Layers Intro</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#1.-Layers">1. Layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Layers-compute-functions.">Layers compute functions.</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Layers-are-configurable.">Layers are configurable.</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Layers-are-trainable.">Layers are trainable.</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Layers-combine-into-layers.">Layers combine into layers.</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#2.-Inputs-and-Outputs">2. Inputs and Outputs</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Data-Stack">Data Stack</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#3.-Defining-New-Layer-Classes">3. Defining New Layer Classes</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#With-the-Fn-layer-creating-function.">With the <code class="docutils literal notranslate"><span class="pre">Fn</span></code> layer-creating function.</a></li>
<li class="toctree-l3"><a class="reference internal" href="#By-defining-a-Layer-subclass">By defining a <code class="docutils literal notranslate"><span class="pre">Layer</span></code> subclass</a></li>
<li class="toctree-l3"><a class="reference internal" href="#By-defining-a-Combinator-subclass">By defining a <code class="docutils literal notranslate"><span class="pre">Combinator</span></code> subclass</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#4.-Testing-and-Debugging-Layer-Classes">4. Testing and Debugging Layer Classes</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Packages/modules</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../trax.html">trax.*</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Trax</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Trax Layers Intro</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/notebooks/layers_intro.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="Trax-Layers-Intro">
<h1>Trax Layers Intro<a class="headerlink" href="#Trax-Layers-Intro" title="Link to this heading">ÔÉÅ</a></h1>
<p>This notebook introduces the core concepts of the Trax library through a series of code samples and explanations. The topics covered in following sections are:</p>
<ol class="arabic simple">
<li><p><strong>Layers</strong>: the basic building blocks and how to combine them</p></li>
<li><p><strong>Inputs and Outputs</strong>: how data streams flow through layers</p></li>
<li><p><strong>Defining New Layer Classes</strong> (if combining existing layers isn‚Äôt enough)</p></li>
<li><p><strong>Testing and Debugging Layer Classes</strong></p></li>
</ol>
<p><strong>General Setup</strong></p>
<p>Execute the following few cells (once) before running any of the code samples in this notebook.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span># Copyright 2018 Google LLC.

# Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

# https://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import numpy as np
<br/><br/><br/></pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span># Import Trax

! pip install -q -U trax
! pip install -q tensorflow

from trax import fastmath
from trax import layers as tl
from trax import shapes
from trax.fastmath import numpy as jnp  # For use in defining new layer types.
from trax.shapes import ShapeDtype
from trax.shapes import signature
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
/bin/sh: pip: command not found
/bin/sh: pip: command not found
</pre></div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span># Settings and utilities for handling inputs, outputs, and object properties.

np.set_printoptions(precision=3)  # Reduce visual noise from extra digits.

def show_layer_properties(layer_obj, layer_name):
  template = (&#39;{}.n_in:  {}\n&#39;
              &#39;{}.n_out: {}\n&#39;
              &#39;{}.sublayers: {}\n&#39;
              &#39;{}.weights:    {}\n&#39;)
  print(template.format(layer_name, layer_obj.n_in,
                        layer_name, layer_obj.n_out,
                        layer_name, layer_obj.sublayers,
                        layer_name, layer_obj.weights))
</pre></div>
</div>
</div>
<section id="1.-Layers">
<h2>1. Layers<a class="headerlink" href="#1.-Layers" title="Link to this heading">ÔÉÅ</a></h2>
<p>The Layer class represents Trax‚Äôs basic building blocks:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>class Layer:
  &quot;&quot;&quot;Base class for composable layers in a deep learning network.

  Layers are the basic building blocks for deep learning models. A Trax layer
  computes a function from zero or more inputs to zero or more outputs,
  optionally using trainable weights (common) and non-parameter state (not
  common).  ...

  ...
</pre></div>
</div>
<section id="Layers-compute-functions.">
<h3>Layers compute functions.<a class="headerlink" href="#Layers-compute-functions." title="Link to this heading">ÔÉÅ</a></h3>
<p>A layer computes a function from zero or more inputs to zero or more outputs. The inputs and outputs are NumPy arrays or JAX objects behaving as NumPy arrays.</p>
<p>The simplest layers, those with no weights or sublayers, can be used without initialization. You can think of them as (pure) mathematical functions that can be plugged into neural networks.</p>
<p>For ease of testing and interactive exploration, layer objects implement the <code class="docutils literal notranslate"><span class="pre">__call__</span></code> method, so you can call them directly on input data:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>y = my_layer(x)
</pre></div>
</div>
<p>Layers are also objects, so you can inspect their properties. For example:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>print(f&#39;Number of inputs expected by this layer: {my_layer.n_in}&#39;)
</pre></div>
</div>
<p><strong>Example 1.</strong> tl.Relu <span class="math notranslate nohighlight">\([n_{in} = 1, n_{out} = 1]\)</span></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>relu = tl.Relu()

x = np.array([[-2, -1, 0, 1, 2],
              [-20, -10, 0, 10, 20]])
y = relu(x)

# Show input, output, and two layer properties.
print(f&#39;x:\n{x}\n\n&#39;
      f&#39;relu(x):\n{y}\n\n&#39;
      f&#39;Number of inputs expected by this layer: {relu.n_in}\n&#39;
      f&#39;Number of outputs promised by this layer: {relu.n_out}&#39;)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
x:
[[ -2  -1   0   1   2]
 [-20 -10   0  10  20]]

relu(x):
[[ 0  0  0  1  2]
 [ 0  0  0 10 20]]

Number of inputs expected by this layer: 1
Number of outputs promised by this layer: 1
</pre></div></div>
</div>
<p><strong>Example 2.</strong> tl.Concatenate <span class="math notranslate nohighlight">\([n_{in} = 2, n_{out} = 1]\)</span></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>concat = tl.Concatenate()

x0 = np.array([[1, 2, 3],
               [4, 5, 6]])
x1 = np.array([[10, 20, 30],
               [40, 50, 60]])
y = concat([x0, x1])

print(f&#39;x0:\n{x0}\n\n&#39;
      f&#39;x1:\n{x1}\n\n&#39;
      f&#39;concat([x1, x2]):\n{y}\n\n&#39;
      f&#39;Number of inputs expected by this layer: {concat.n_in}\n&#39;
      f&#39;Number of outputs promised by this layer: {concat.n_out}&#39;)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
x0:
[[1 2 3]
 [4 5 6]]

x1:
[[10 20 30]
 [40 50 60]]

concat([x1, x2]):
[[ 1  2  3 10 20 30]
 [ 4  5  6 40 50 60]]

Number of inputs expected by this layer: 2
Number of outputs promised by this layer: 1
</pre></div></div>
</div>
</section>
<section id="Layers-are-configurable.">
<h3>Layers are configurable.<a class="headerlink" href="#Layers-are-configurable." title="Link to this heading">ÔÉÅ</a></h3>
<p>Many layer types have creation-time parameters for flexibility. The <code class="docutils literal notranslate"><span class="pre">Concatenate</span></code> layer type, for instance, has two optional parameters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">axis</span></code>: index of axis along which to concatenate the tensors; default value of -1 means to use the last axis.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">n_items</span></code>: number of tensors to join into one by concatenation; default value is 2.</p></li>
</ul>
<p>The following example shows <code class="docutils literal notranslate"><span class="pre">Concatenate</span></code> configured for <strong>3</strong> input tensors, and concatenation along the initial <span class="math notranslate nohighlight">\((0^{th})\)</span> axis.</p>
<p><strong>Example 3.</strong> tl.Concatenate(n_items=3, axis=0)</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>concat3 = tl.Concatenate(n_items=3, axis=0)

x0 = np.array([[1, 2, 3],
               [4, 5, 6]])
x1 = np.array([[10, 20, 30],
               [40, 50, 60]])
x2 = np.array([[100, 200, 300],
               [400, 500, 600]])

y = concat3([x0, x1, x2])

print(f&#39;x0:\n{x0}\n\n&#39;
      f&#39;x1:\n{x1}\n\n&#39;
      f&#39;x2:\n{x2}\n\n&#39;
      f&#39;concat3([x0, x1, x2]):\n{y}&#39;)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
x0:
[[1 2 3]
 [4 5 6]]

x1:
[[10 20 30]
 [40 50 60]]

x2:
[[100 200 300]
 [400 500 600]]

concat3([x0, x1, x2]):
[[  1   2   3]
 [  4   5   6]
 [ 10  20  30]
 [ 40  50  60]
 [100 200 300]
 [400 500 600]]
</pre></div></div>
</div>
</section>
<section id="Layers-are-trainable.">
<h3>Layers are trainable.<a class="headerlink" href="#Layers-are-trainable." title="Link to this heading">ÔÉÅ</a></h3>
<p>Many layer types include weights that affect the computation of outputs from inputs, and they use back-progagated gradients to update those weights.</p>
<p>üößüöß <em>A very small subset of layer types, such as ``BatchNorm``, also include modifiable weights (called ``state``) that are updated based on forward-pass inputs/computation rather than back-propagated gradients.</em></p>
<p><strong>Initialization</strong></p>
<p>Trainable layers must be initialized before use. Trax can take care of this as part of the overall training process. In other settings (e.g., in tests or interactively in a Colab notebook), you need to initialize the <em>outermost/topmost</em> layer explicitly. For this, use <code class="docutils literal notranslate"><span class="pre">init</span></code>:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>def init(self, input_signature, rng=None, use_cache=False):
  &quot;&quot;&quot;Initializes weights/state of this layer and its sublayers recursively.

  Initialization creates layer weights and state, for layers that use them.
  It derives the necessary array shapes and data types from the layer&#39;s input
  signature, which is itself just shape and data type information.

  For layers without weights or state, this method safely does nothing.

  This method is designed to create weights/state only once for each layer
  instance, even if the same layer instance occurs in multiple places in the
  network. This enables weight sharing to be implemented as layer sharing.

  Args:
    input_signature: `ShapeDtype` instance (if this layer takes one input)
        or list/tuple of `ShapeDtype` instances.
    rng: Single-use random number generator (JAX PRNG key), or `None`;
        if `None`, use a default computed from an integer 0 seed.
    use_cache: If `True`, and if this layer instance has already been
        initialized elsewhere in the network, then return special marker
        values -- tuple `(GET_WEIGHTS_FROM_CACHE, GET_STATE_FROM_CACHE)`.
        Else return this layer&#39;s newly initialized weights and state.

  Returns:
    A `(weights, state)` tuple.
  &quot;&quot;&quot;
</pre></div>
</div>
<p>Input signatures can be built from scratch using <code class="docutils literal notranslate"><span class="pre">ShapeDType</span></code> objects, or can be derived from data via the <code class="docutils literal notranslate"><span class="pre">signature</span></code> function (in module <code class="docutils literal notranslate"><span class="pre">shapes</span></code>):</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>def signature(obj):
  &quot;&quot;&quot;Returns a `ShapeDtype` signature for the given `obj`.

  A signature is either a `ShapeDtype` instance or a tuple of `ShapeDtype`
  instances. Note that this function is permissive with respect to its inputs
  (accepts lists or tuples or dicts, and underlying objects can be any type
  as long as they have shape and dtype attributes) and returns the corresponding
  nested structure of `ShapeDtype`.

  Args:
    obj: An object that has `shape` and `dtype` attributes, or a list/tuple/dict
        of such objects.

  Returns:
    A corresponding nested structure of `ShapeDtype` instances.
  &quot;&quot;&quot;
</pre></div>
</div>
<p><strong>Example 4.</strong> tl.LayerNorm <span class="math notranslate nohighlight">\([n_{in} = 1, n_{out} = 1]\)</span></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>layer_norm = tl.LayerNorm()

x = np.array([[-2, -1, 0, 1, 2],
              [1, 2, 3, 4, 5],
              [10, 20, 30, 40, 50]]).astype(np.float32)
layer_norm.init(shapes.signature(x))

y = layer_norm(x)

print(f&#39;x:\n{x}\n\n&#39;
      f&#39;layer_norm(x):\n{y}\n&#39;)
print(f&#39;layer_norm.weights:\n{layer_norm.weights}&#39;)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
x:
[[-2. -1.  0.  1.  2.]
 [ 1.  2.  3.  4.  5.]
 [10. 20. 30. 40. 50.]]

layer_norm(x):
[[-1.414 -0.707  0.     0.707  1.414]
 [-1.414 -0.707  0.     0.707  1.414]
 [-1.414 -0.707  0.     0.707  1.414]]

layer_norm.weights:
(DeviceArray([1., 1., 1., 1., 1.], dtype=float32), DeviceArray([0., 0., 0., 0., 0.], dtype=float32))
</pre></div></div>
</div>
</section>
<section id="Layers-combine-into-layers.">
<h3>Layers combine into layers.<a class="headerlink" href="#Layers-combine-into-layers." title="Link to this heading">ÔÉÅ</a></h3>
<p>The Trax library authors encourage users to build networks and network components as combinations of existing layers, by means of a small set of <em>combinator</em> layers. A combinator makes a list of layers behave as a single layer ‚Äì by combining the sublayer computations yet looking from the outside like any other layer. The combined layer, like other layers, can:</p>
<ul class="simple">
<li><p>compute outputs from inputs,</p></li>
<li><p>update parameters from gradients, and</p></li>
<li><p>combine with yet more layers.</p></li>
</ul>
<p><strong>Combine with ``Serial``</strong></p>
<p>The most common way to combine layers is with the <code class="docutils literal notranslate"><span class="pre">Serial</span></code> combinator:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>class Serial(base.Layer):
  &quot;&quot;&quot;Combinator that applies layers serially (by function composition).

  This combinator is commonly used to construct deep networks, e.g., like this::

      mlp = tl.Serial(
        tl.Dense(128),
        tl.Relu(),
        tl.Dense(10),
      )

  A Serial combinator uses stack semantics to manage data for its sublayers.
  Each sublayer sees only the inputs it needs and returns only the outputs it
  has generated. The sublayers interact via the data stack. For instance, a
  sublayer k, following sublayer j, gets called with the data stack in the
  state left after layer j has applied. The Serial combinator then:

    - takes n_in items off the top of the stack (n_in = k.n_in) and calls
      layer k, passing those items as arguments; and

    - takes layer k&#39;s n_out return values (n_out = k.n_out) and pushes
      them onto the data stack.

  A Serial instance with no sublayers acts as a special-case (but useful)
  1-input 1-output no-op.
  &quot;&quot;&quot;
</pre></div>
</div>
<p>If one layer has the same number of outputs as the next layer has inputs (which is the usual case), the successive layers behave like function composition:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>#  h(.) = g(f(.))
layer_h = Serial(
    layer_f,
    layer_g,
)
</pre></div>
</div>
<p>Note how, inside <code class="docutils literal notranslate"><span class="pre">Serial</span></code>, function composition is expressed naturally as a succession of operations, so that no nested parentheses are needed.</p>
<p><strong>Example 5.</strong> y = layer_norm(relu(x)) <span class="math notranslate nohighlight">\([n_{in} = 1, n_{out} = 1]\)</span></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>layer_block = tl.Serial(
    tl.Relu(),
    tl.LayerNorm(),
)

x = np.array([[-2, -1, 0, 1, 2],
              [-20, -10, 0, 10, 20]]).astype(np.float32)
layer_block.init(shapes.signature(x))
y = layer_block(x)

print(f&#39;x:\n{x}\n\n&#39;
      f&#39;layer_block(x):\n{y}&#39;)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
x:
[[ -2.  -1.   0.   1.   2.]
 [-20. -10.   0.  10.  20.]]

layer_block(x):
[[-0.75 -0.75 -0.75  0.5   1.75]
 [-0.75 -0.75 -0.75  0.5   1.75]]
</pre></div></div>
</div>
<p>And we can inspect the block as a whole, as if it were just another layer:</p>
<p><strong>Example 5‚Äô.</strong> Inspecting a <code class="docutils literal notranslate"><span class="pre">Serial</span></code> layer.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>print(f&#39;layer_block: {layer_block}\n\n&#39;
      f&#39;layer_block.weights: {layer_block.weights}&#39;)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
layer_block: Serial[
  Relu
  LayerNorm
]

layer_block.weights: ((), (DeviceArray([1., 1., 1., 1., 1.], dtype=float32), DeviceArray([0., 0., 0., 0., 0.], dtype=float32)))
</pre></div></div>
</div>
<p><strong>Combine with ``Branch``</strong></p>
<p>The <code class="docutils literal notranslate"><span class="pre">Branch</span></code> combinator arranges layers into parallel computational channels:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>def Branch(*layers, name=&#39;Branch&#39;):
  &quot;&quot;&quot;Combinator that applies a list of layers in parallel to copies of inputs.

  Each layer in the input list is applied to as many inputs from the stack
  as it needs, and their outputs are successively combined on stack.

  For example, suppose one has three layers:

    - F: 1 input, 1 output
    - G: 3 inputs, 1 output
    - H: 2 inputs, 2 outputs (h1, h2)

  Then Branch(F, G, H) will take 3 inputs and give 4 outputs:

    - inputs: a, b, c
    - outputs: F(a), G(a, b, c), h1, h2    where h1, h2 = H(a, b)

  As an important special case, a None argument to Branch acts as if it takes
  one argument, which it leaves unchanged. (It acts as a one-arg no-op.)

  Args:
    *layers: List of layers.
    name: Descriptive name for this layer.

  Returns:
    A branch layer built from the given sublayers.
  &quot;&quot;&quot;
</pre></div>
</div>
<p>Residual blocks, for example, are implemented using <code class="docutils literal notranslate"><span class="pre">Branch</span></code>:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>def Residual(*layers, shortcut=None):
  &quot;&quot;&quot;Wraps a series of layers with a residual connection.

  Args:
    *layers: One or more layers, to be applied in series.
    shortcut: If None (the usual case), the Residual layer computes the
        element-wise sum of the stack-top input with the output of the layer
        series. If specified, the `shortcut` layer applies to a copy of the
        inputs and (elementwise) adds its output to the output from the main
        layer series.

  Returns:
      A layer representing a residual connection paired with a layer series.
  &quot;&quot;&quot;
  layers = _ensure_flat(layers)
  layer = layers[0] if len(layers) == 1 else Serial(layers)
  return Serial(
      Branch(shortcut, layer),
      Add(),
  )
</pre></div>
</div>
<p>Here‚Äôs a simple code example to highlight the mechanics.</p>
<p><strong>Example 6.</strong> <code class="docutils literal notranslate"><span class="pre">Branch</span></code></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>relu = tl.Relu()
times_100 = tl.Fn(&quot;Times100&quot;, lambda x: x * 100.0)
branch_relu_t100 = tl.Branch(relu, times_100)

x = np.array([[-2, -1, 0, 1, 2],
              [-20, -10, 0, 10, 20]])
branch_relu_t100.init(shapes.signature(x))

y0, y1 = branch_relu_t100(x)

print(f&#39;x:\n{x}\n\n&#39;
      f&#39;y0:\n{y0}\n\n&#39;
      f&#39;y1:\n{y1}&#39;)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
x:
[[ -2  -1   0   1   2]
 [-20 -10   0  10  20]]

y0:
[[ 0  0  0  1  2]
 [ 0  0  0 10 20]]

y1:
[[ -200.  -100.     0.   100.   200.]
 [-2000. -1000.     0.  1000.  2000.]]
</pre></div></div>
</div>
</section>
</section>
<section id="2.-Inputs-and-Outputs">
<h2>2. Inputs and Outputs<a class="headerlink" href="#2.-Inputs-and-Outputs" title="Link to this heading">ÔÉÅ</a></h2>
<p>Trax allows layers to have multiple input streams and output streams. When designing a network, you have the flexibility to use layers that:</p>
<ul class="simple">
<li><p>process a single data stream (<span class="math notranslate nohighlight">\(n_{in} = n_{out} = 1\)</span>),</p></li>
<li><p>process multiple parallel data streams ($n_{in} = n_{out} = 2, 3, ‚Ä¶ $),</p></li>
<li><p>split or inject data streams (<span class="math notranslate nohighlight">\(n_{in} &lt; n_{out}\)</span>), or</p></li>
<li><p>merge or remove data streams (<span class="math notranslate nohighlight">\(n_{in} &gt; n_{out}\)</span>).</p></li>
</ul>
<p>We saw in section 1 the example of <code class="docutils literal notranslate"><span class="pre">Residual</span></code>, which involves both a split and a merge:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>...
return Serial(
    Branch(shortcut, layer),
    Add(),
)
</pre></div>
</div>
<p>In other words, layer by layer:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Branch(shortcut,</span> <span class="pre">layers)</span></code>: makes two copies of the single incoming data stream, passes one copy via the shortcut (typically a no-op), and processes the other copy via the given layers (applied in series). [<span class="math notranslate nohighlight">\(n_{in} = 1\)</span>, <span class="math notranslate nohighlight">\(n_{out} = 2\)</span>]</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Add()</span></code>: combines the two streams back into one by adding two tensors elementwise. [<span class="math notranslate nohighlight">\(n_{in} = 2\)</span>, <span class="math notranslate nohighlight">\(n_{out} = 1\)</span>]</p></li>
</ul>
<section id="Data-Stack">
<h3>Data Stack<a class="headerlink" href="#Data-Stack" title="Link to this heading">ÔÉÅ</a></h3>
<p>Trax supports flexible data flows through a network via a data stack, which is managed by the <code class="docutils literal notranslate"><span class="pre">Serial</span></code> combinator:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>class Serial(base.Layer):
  &quot;&quot;&quot;Combinator that applies layers serially (by function composition).

  ...

  A Serial combinator uses stack semantics to manage data for its sublayers.
  Each sublayer sees only the inputs it needs and returns only the outputs it
  has generated. The sublayers interact via the data stack. For instance, a
  sublayer k, following sublayer j, gets called with the data stack in the
  state left after layer j has applied. The Serial combinator then:

    - takes n_in items off the top of the stack (n_in = k.n_in) and calls
      layer k, passing those items as arguments; and

    - takes layer k&#39;s n_out return values (n_out = k.n_out) and pushes
      them onto the data stack.

  ...

  &quot;&quot;&quot;
</pre></div>
</div>
<p><strong>Simple Case 1 ‚Äì Each layer takes one input and has one output.</strong></p>
<p>This is in effect a single data stream pipeline, and the successive layers behave like function composition:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>#  s(.) = h(g(f(.)))
layer_s = Serial(
    layer_f,
    layer_g,
    layer_h,
)
</pre></div>
</div>
<p>Note how, inside <code class="docutils literal notranslate"><span class="pre">Serial</span></code>, function composition is expressed naturally as a succession of operations, so that no nested parentheses are needed and the order of operations matches the textual order of layers.</p>
<p><strong>Simple Case 2 ‚Äì Each layer consumes all outputs of the preceding layer.</strong></p>
<p>This is still a single pipeline, but data streams internal to it can split and merge. The <code class="docutils literal notranslate"><span class="pre">Residual</span></code> example above illustrates this kind.</p>
<p><strong>General Case ‚Äì Successive layers interact via the data stack.</strong></p>
<p>As described in the <code class="docutils literal notranslate"><span class="pre">Serial</span></code> class docstring, each layer gets its inputs from the data stack after the preceding layer has put its outputs onto the stack. This covers the simple cases above, but also allows for more flexible data interactions between non-adjacent layers. The following example is schematic:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>x, y_target = get_batch_of_labeled_data()

model_plus_eval = Serial(
    my_fancy_deep_model(),  # Takes one arg (x) and has one output (y_hat)
    my_eval(),  # Takes two args (y_hat, y_target) and has one output (score)
)

eval_score = model_plus_eval((x, y_target))
</pre></div>
</div>
<p>Here is the corresponding progression of stack states:</p>
<ol class="arabic simple" start="0">
<li><p>At start: <em>‚Äìempty‚Äì</em></p></li>
<li><p>After <code class="docutils literal notranslate"><span class="pre">get_batch_of_labeled_data()</span></code>: <em>x</em>, <em>y_target</em></p></li>
<li><p>After <code class="docutils literal notranslate"><span class="pre">my_fancy_deep_model()</span></code>: <em>y_hat</em>, <em>y_target</em></p></li>
<li><p>After <code class="docutils literal notranslate"><span class="pre">my_eval()</span></code>: <em>score</em></p></li>
</ol>
<p>Note in particular how the application of the model (between stack states 1 and 2) only uses and affects the top element on the stack: <code class="docutils literal notranslate"><span class="pre">x</span></code> ‚Äì&gt; <code class="docutils literal notranslate"><span class="pre">y_hat</span></code>. The rest of the data stack (<code class="docutils literal notranslate"><span class="pre">y_target</span></code>) comes in use only later, for the eval function.</p>
</section>
</section>
<section id="3.-Defining-New-Layer-Classes">
<h2>3. Defining New Layer Classes<a class="headerlink" href="#3.-Defining-New-Layer-Classes" title="Link to this heading">ÔÉÅ</a></h2>
<p>If you need a layer type that is not easily defined as a combination of existing layer types, you can define your own layer classes in a couple different ways.</p>
<section id="With-the-Fn-layer-creating-function.">
<h3>With the <code class="docutils literal notranslate"><span class="pre">Fn</span></code> layer-creating function.<a class="headerlink" href="#With-the-Fn-layer-creating-function." title="Link to this heading">ÔÉÅ</a></h3>
<p>Many layer types needed in deep learning compute pure functions from inputs to outputs, using neither weights nor randomness. You can use Trax‚Äôs <code class="docutils literal notranslate"><span class="pre">Fn</span></code> function to define your own pure layer types:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>def Fn(name, f, n_out=1):  # pylint: disable=invalid-name
  &quot;&quot;&quot;Returns a layer with no weights that applies the function `f`.

  `f` can take and return any number of arguments, and takes only positional
  arguments -- no default or keyword arguments. It often uses JAX-numpy (`jnp`).
  The following, for example, would create a layer that takes two inputs and
  returns two outputs -- element-wise sums and maxima:

      `Fn(&#39;SumAndMax&#39;, lambda x0, x1: (x0 + x1, jnp.maximum(x0, x1)), n_out=2)`

  The layer&#39;s number of inputs (`n_in`) is automatically set to number of
  positional arguments in `f`, but you must explicitly set the number of
  outputs (`n_out`) whenever it&#39;s not the default value 1.

  Args:
    name: Class-like name for the resulting layer; for use in debugging.
    f: Pure function from input tensors to output tensors, where each input
        tensor is a separate positional arg, e.g., `f(x0, x1) --&gt; x0 + x1`.
        Output tensors must be packaged as specified in the `Layer` class
        docstring.
    n_out: Number of outputs promised by the layer; default value 1.

  Returns:
    Layer executing the function `f`.
  &quot;&quot;&quot;
</pre></div>
</div>
<p><strong>Example 7.</strong> Use <code class="docutils literal notranslate"><span class="pre">Fn</span></code> to define a new layer type:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span># Define new layer type.
def Gcd():
  &quot;&quot;&quot;Returns a layer to compute the greatest common divisor, elementwise.&quot;&quot;&quot;
  return tl.Fn(&#39;Gcd&#39;, lambda x0, x1: jnp.gcd(x0, x1))

# Use it.
gcd = Gcd()

x0 = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
x1 = np.array([11, 12, 13, 14, 15, 16, 17, 18, 19, 20])

y = gcd((x0, x1))

print(f&#39;x0:\n{x0}\n\n&#39;
      f&#39;x1:\n{x1}\n\n&#39;
      f&#39;gcd((x0, x1)):\n{y}&#39;)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
x0:
[ 1  2  3  4  5  6  7  8  9 10]

x1:
[11 12 13 14 15 16 17 18 19 20]

gcd((x0, x1)):
[ 1  2  1  2  5  2  1  2  1 10]
</pre></div></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">Fn</span></code> function infers <code class="docutils literal notranslate"><span class="pre">n_in</span></code> (number of inputs) as the length of <code class="docutils literal notranslate"><span class="pre">f</span></code>‚Äôs arg list. <code class="docutils literal notranslate"><span class="pre">Fn</span></code> does not infer <code class="docutils literal notranslate"><span class="pre">n_out</span></code> (number out outputs) though. If your <code class="docutils literal notranslate"><span class="pre">f</span></code> has more than one output, you need to give an explicit value using the <code class="docutils literal notranslate"><span class="pre">n_out</span></code> keyword arg.</p>
<p><strong>Example 8.</strong> <code class="docutils literal notranslate"><span class="pre">Fn</span></code> with multiple outputs:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span># Define new layer type.
def SumAndMax():
  &quot;&quot;&quot;Returns a layer to compute sums and maxima of two input tensors.&quot;&quot;&quot;
  return tl.Fn(&#39;SumAndMax&#39;,
               lambda x0, x1: (x0 + x1, jnp.maximum(x0, x1)),
               n_out=2)

# Use it.
sum_and_max = SumAndMax()

x0 = np.array([1, 2, 3, 4, 5])
x1 = np.array([10, -20, 30, -40, 50])

y0, y1 = sum_and_max([x0, x1])

print(f&#39;x0:\n{x0}\n\n&#39;
      f&#39;x1:\n{x1}\n\n&#39;
      f&#39;y0:\n{y0}\n\n&#39;
      f&#39;y1:\n{y1}&#39;)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
x0:
[1 2 3 4 5]

x1:
[ 10 -20  30 -40  50]

y0:
[ 11 -18  33 -36  55]

y1:
[10  2 30  4 50]
</pre></div></div>
</div>
<p><strong>Example 9.</strong> Use <code class="docutils literal notranslate"><span class="pre">Fn</span></code> to define a configurable layer:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span># Function defined in trax/layers/core.py:
def Flatten(n_axes_to_keep=1):
  &quot;&quot;&quot;Returns a layer that combines one or more trailing axes of a tensor.

  Flattening keeps all the values of the input tensor, but reshapes it by
  collapsing one or more trailing axes into a single axis. For example, a
  `Flatten(n_axes_to_keep=2)` layer would map a tensor with shape
  `(2, 3, 5, 7, 11)` to the same values with shape `(2, 3, 385)`.

  Args:
    n_axes_to_keep: Number of leading axes to leave unchanged when reshaping;
        collapse only the axes after these.
  &quot;&quot;&quot;
  layer_name = f&#39;Flatten_keep{n_axes_to_keep}&#39;
  def f(x):
    in_rank = len(x.shape)
    if in_rank &lt;= n_axes_to_keep:
      raise ValueError(f&#39;Input rank ({in_rank}) must exceed the number of &#39;
                       f&#39;axes to keep ({n_axes_to_keep}) after flattening.&#39;)
    return jnp.reshape(x, (x.shape[:n_axes_to_keep] + (-1,)))
  return tl.Fn(layer_name, f)

flatten_keep_1_axis = Flatten(n_axes_to_keep=1)
flatten_keep_2_axes = Flatten(n_axes_to_keep=2)

x = np.array([[[1, 2, 3],
               [10, 20, 30],
               [100, 200, 300]],
              [[4, 5, 6],
               [40, 50, 60],
               [400, 500, 600]]])

y1 = flatten_keep_1_axis(x)
y2 = flatten_keep_2_axes(x)

print(f&#39;x:\n{x}\n\n&#39;
      f&#39;flatten_keep_1_axis(x):\n{y1}\n\n&#39;
      f&#39;flatten_keep_2_axes(x):\n{y2}&#39;)
<br/><br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
x:
[[[  1   2   3]
  [ 10  20  30]
  [100 200 300]]

 [[  4   5   6]
  [ 40  50  60]
  [400 500 600]]]

flatten_keep_1_axis(x):
[[  1   2   3  10  20  30 100 200 300]
 [  4   5   6  40  50  60 400 500 600]]

flatten_keep_2_axes(x):
[[[  1   2   3]
  [ 10  20  30]
  [100 200 300]]

 [[  4   5   6]
  [ 40  50  60]
  [400 500 600]]]
</pre></div></div>
</div>
</section>
<section id="By-defining-a-Layer-subclass">
<h3>By defining a <code class="docutils literal notranslate"><span class="pre">Layer</span></code> subclass<a class="headerlink" href="#By-defining-a-Layer-subclass" title="Link to this heading">ÔÉÅ</a></h3>
<p>If you need a layer type that uses trainable weights (or state), you can extend the base <code class="docutils literal notranslate"><span class="pre">Layer</span></code> class:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>class Layer:
  &quot;&quot;&quot;Base class for composable layers in a deep learning network.

  ...

  Authors of new layer subclasses typically override at most two methods of
  the base `Layer` class:

    `forward(inputs)`:
      Computes this layer&#39;s output as part of a forward pass through the model.

    `init_weights_and_state(self, input_signature)`:
      Initializes weights and state for inputs with the given signature.

  ...
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">forward</span></code> method uses <em>weights stored in the layer object</em> (<code class="docutils literal notranslate"><span class="pre">self.weights</span></code>) to compute outputs from inputs. For example, here is the definition of <code class="docutils literal notranslate"><span class="pre">forward</span></code> for Trax‚Äôs <code class="docutils literal notranslate"><span class="pre">Dense</span></code> layer:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>def forward(self, x):
  &quot;&quot;&quot;Executes this layer as part of a forward pass through the model.

  Args:
    x: Tensor of same shape and dtype as the input signature used to
        initialize this layer.

  Returns:
    Tensor of same shape and dtype as the input, except the final dimension
    is the layer&#39;s `n_units` value.
  &quot;&quot;&quot;
  if self._use_bias:
    if not isinstance(self.weights, (tuple, list)):
      raise ValueError(f&#39;Weights should be a (w, b) tuple or list; &#39;
                       f&#39;instead got: {self.weights}&#39;)
    w, b = self.weights
    return jnp.dot(x, w) + b  # Affine map.
  else:
    w = self.weights
    return jnp.dot(x, w)  # Linear map.
</pre></div>
</div>
<p>Layer weights must be initialized before the layer can be used; the <code class="docutils literal notranslate"><span class="pre">init_weights_and_state</span></code> method specifies how. Continuing the <code class="docutils literal notranslate"><span class="pre">Dense</span></code> example, here is the corresponding initialization code:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>def init_weights_and_state(self, input_signature):
  &quot;&quot;&quot;Randomly initializes this layer&#39;s weights.

  Weights are a `(w, b)` tuple for layers created with `use_bias=True` (the
  default case), or a `w` tensor for layers created with `use_bias=False`.

  Args:
    input_signature: `ShapeDtype` instance characterizing the input this layer
        should compute on.
  &quot;&quot;&quot;
  shape_w = (input_signature.shape[-1], self._n_units)
  shape_b = (self._n_units,)
  rng_w, rng_b = fastmath.random.split(self.rng, 2)
  w = self._kernel_initializer(shape_w, rng_w)

  if self._use_bias:
    b = self._bias_initializer(shape_b, rng_b)
    self.weights = (w, b)
  else:
    self.weights = w
</pre></div>
</div>
</section>
<section id="By-defining-a-Combinator-subclass">
<h3>By defining a <code class="docutils literal notranslate"><span class="pre">Combinator</span></code> subclass<a class="headerlink" href="#By-defining-a-Combinator-subclass" title="Link to this heading">ÔÉÅ</a></h3>
<p><em>TBD</em></p>
</section>
</section>
<section id="4.-Testing-and-Debugging-Layer-Classes">
<h2>4. Testing and Debugging Layer Classes<a class="headerlink" href="#4.-Testing-and-Debugging-Layer-Classes" title="Link to this heading">ÔÉÅ</a></h2>
<p><em>TBD</em></p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="trax_intro.html" class="btn btn-neutral float-left" title="Trax Quick Intro" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../trax.html" class="btn btn-neutral float-right" title="trax" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, Google LLC..</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>