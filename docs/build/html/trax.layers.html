

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>trax.layers &mdash; Trax  documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=9edc463e" />

  
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=5929fcd5"></script>
      <script src="_static/doctools.js?v=fd6eb6e6"></script>
      <script src="_static/sphinx_highlight.js?v=6ffebe34"></script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
      <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="trax.models" href="trax.models.html" />
    <link rel="prev" title="trax.fastmath" href="trax.fastmath.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            Trax
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Introductory Notebooks</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="notebooks/trax_intro.html">Trax Quick Intro</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/layers_intro.html">Trax Layers Intro</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Packages/modules</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="trax.html">trax.*</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="trax.fastmath.html">fastmath.*</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">layers.*</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#module-trax.layers.acceleration">acceleration</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.acceleration.Accelerate"><code class="docutils literal notranslate"><span class="pre">Accelerate</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.acceleration.mean"><code class="docutils literal notranslate"><span class="pre">mean()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.acceleration.jit_forward"><code class="docutils literal notranslate"><span class="pre">jit_forward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.acceleration.reshape_by_device"><code class="docutils literal notranslate"><span class="pre">reshape_by_device()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.acceleration.for_n_devices"><code class="docutils literal notranslate"><span class="pre">for_n_devices()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.acceleration.on_cpu"><code class="docutils literal notranslate"><span class="pre">on_cpu()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.acceleration.on_accelerator"><code class="docutils literal notranslate"><span class="pre">on_accelerator()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#module-trax.layers.activation_fns">activation_fns</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.activation_fns.Relu"><code class="docutils literal notranslate"><span class="pre">Relu()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.activation_fns.ParametricRelu"><code class="docutils literal notranslate"><span class="pre">ParametricRelu()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.activation_fns.LeakyRelu"><code class="docutils literal notranslate"><span class="pre">LeakyRelu()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.activation_fns.Elu"><code class="docutils literal notranslate"><span class="pre">Elu()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.activation_fns.Selu"><code class="docutils literal notranslate"><span class="pre">Selu()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.activation_fns.Gelu"><code class="docutils literal notranslate"><span class="pre">Gelu()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.activation_fns.FastGelu"><code class="docutils literal notranslate"><span class="pre">FastGelu()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.activation_fns.Sigmoid"><code class="docutils literal notranslate"><span class="pre">Sigmoid()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.activation_fns.Tanh"><code class="docutils literal notranslate"><span class="pre">Tanh()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.activation_fns.HardSigmoid"><code class="docutils literal notranslate"><span class="pre">HardSigmoid()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.activation_fns.HardTanh"><code class="docutils literal notranslate"><span class="pre">HardTanh()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.activation_fns.Softplus"><code class="docutils literal notranslate"><span class="pre">Softplus()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.activation_fns.Exp"><code class="docutils literal notranslate"><span class="pre">Exp()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.activation_fns.Log"><code class="docutils literal notranslate"><span class="pre">Log()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.activation_fns.Swish"><code class="docutils literal notranslate"><span class="pre">Swish()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.activation_fns.Glu"><code class="docutils literal notranslate"><span class="pre">Glu()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.activation_fns.ThresholdedLinearUnit"><code class="docutils literal notranslate"><span class="pre">ThresholdedLinearUnit</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#module-trax.layers.attention">attention</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.attention.Attention"><code class="docutils literal notranslate"><span class="pre">Attention()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.attention.AttentionQKV"><code class="docutils literal notranslate"><span class="pre">AttentionQKV()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.attention.PureAttention"><code class="docutils literal notranslate"><span class="pre">PureAttention</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.attention.DotProductAttention"><code class="docutils literal notranslate"><span class="pre">DotProductAttention</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.attention.SplitIntoHeads"><code class="docutils literal notranslate"><span class="pre">SplitIntoHeads()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.attention.MergeHeads"><code class="docutils literal notranslate"><span class="pre">MergeHeads()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.attention.ConfigurableAttention"><code class="docutils literal notranslate"><span class="pre">ConfigurableAttention()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.attention.CausalAttention"><code class="docutils literal notranslate"><span class="pre">CausalAttention()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.attention.DotProductCausalAttention"><code class="docutils literal notranslate"><span class="pre">DotProductCausalAttention</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.attention.ShiftRight"><code class="docutils literal notranslate"><span class="pre">ShiftRight()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.attention.PaddingMask"><code class="docutils literal notranslate"><span class="pre">PaddingMask()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.attention.EncoderDecoderMask"><code class="docutils literal notranslate"><span class="pre">EncoderDecoderMask()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.attention.PositionalEncoding"><code class="docutils literal notranslate"><span class="pre">PositionalEncoding</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#module-trax.layers.base">base</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.base.Layer"><code class="docutils literal notranslate"><span class="pre">Layer</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.base.PureLayer"><code class="docutils literal notranslate"><span class="pre">PureLayer</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.base.Fn"><code class="docutils literal notranslate"><span class="pre">Fn()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.base.LayerError"><code class="docutils literal notranslate"><span class="pre">LayerError</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.base.flatten_weights_and_state"><code class="docutils literal notranslate"><span class="pre">flatten_weights_and_state()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.base.unflatten_weights_and_state"><code class="docutils literal notranslate"><span class="pre">unflatten_weights_and_state()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.base.np_to_file"><code class="docutils literal notranslate"><span class="pre">np_to_file()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.base.np_from_file"><code class="docutils literal notranslate"><span class="pre">np_from_file()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.base.to_list"><code class="docutils literal notranslate"><span class="pre">to_list()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.base.shard"><code class="docutils literal notranslate"><span class="pre">shard()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.base.unshard_in_pmap"><code class="docutils literal notranslate"><span class="pre">unshard_in_pmap()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.base.unshard"><code class="docutils literal notranslate"><span class="pre">unshard()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#module-trax.layers.combinators">combinators</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.combinators.Serial"><code class="docutils literal notranslate"><span class="pre">Serial</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.combinators.Parallel"><code class="docutils literal notranslate"><span class="pre">Parallel</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.combinators.Concatenate"><code class="docutils literal notranslate"><span class="pre">Concatenate</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.combinators.Split"><code class="docutils literal notranslate"><span class="pre">Split</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.combinators.Scan"><code class="docutils literal notranslate"><span class="pre">Scan</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.combinators.Cond"><code class="docutils literal notranslate"><span class="pre">Cond</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.combinators.Chunk"><code class="docutils literal notranslate"><span class="pre">Chunk()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.combinators.Branch"><code class="docutils literal notranslate"><span class="pre">Branch()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.combinators.Residual"><code class="docutils literal notranslate"><span class="pre">Residual()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.combinators.Select"><code class="docutils literal notranslate"><span class="pre">Select()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.combinators.Drop"><code class="docutils literal notranslate"><span class="pre">Drop()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.combinators.Dup"><code class="docutils literal notranslate"><span class="pre">Dup()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.combinators.Swap"><code class="docutils literal notranslate"><span class="pre">Swap()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.combinators.SerialWithSideOutputs"><code class="docutils literal notranslate"><span class="pre">SerialWithSideOutputs()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.combinators.FlattenList"><code class="docutils literal notranslate"><span class="pre">FlattenList()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.combinators.Add"><code class="docutils literal notranslate"><span class="pre">Add()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.combinators.SubtractTop"><code class="docutils literal notranslate"><span class="pre">SubtractTop()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.combinators.Multiply"><code class="docutils literal notranslate"><span class="pre">Multiply()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.combinators.Gate"><code class="docutils literal notranslate"><span class="pre">Gate()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.combinators.Cache"><code class="docutils literal notranslate"><span class="pre">Cache</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.combinators.BatchLeadingAxes"><code class="docutils literal notranslate"><span class="pre">BatchLeadingAxes</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.combinators.Bidirectional"><code class="docutils literal notranslate"><span class="pre">Bidirectional()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.combinators.inputs_from_stack"><code class="docutils literal notranslate"><span class="pre">inputs_from_stack()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.combinators.outputs_onto_stack"><code class="docutils literal notranslate"><span class="pre">outputs_onto_stack()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#module-trax.layers.convolution">convolution</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.convolution.Conv"><code class="docutils literal notranslate"><span class="pre">Conv</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.convolution.CausalConv"><code class="docutils literal notranslate"><span class="pre">CausalConv</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.convolution.Conv1d"><code class="docutils literal notranslate"><span class="pre">Conv1d()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.convolution.CausalDepthwiseConv"><code class="docutils literal notranslate"><span class="pre">CausalDepthwiseConv</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#module-trax.layers.core">core</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.core.Dense"><code class="docutils literal notranslate"><span class="pre">Dense</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.core.Embedding"><code class="docutils literal notranslate"><span class="pre">Embedding</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.core.Dropout"><code class="docutils literal notranslate"><span class="pre">Dropout</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.core.Weights"><code class="docutils literal notranslate"><span class="pre">Weights</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.core.PrintShape"><code class="docutils literal notranslate"><span class="pre">PrintShape()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.core.SummaryImage"><code class="docutils literal notranslate"><span class="pre">SummaryImage</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.core.SummaryScalar"><code class="docutils literal notranslate"><span class="pre">SummaryScalar</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.core.RandomUniform"><code class="docutils literal notranslate"><span class="pre">RandomUniform</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.core.LocallyConnected1d"><code class="docutils literal notranslate"><span class="pre">LocallyConnected1d</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.core.Flatten"><code class="docutils literal notranslate"><span class="pre">Flatten()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.core.LogSoftmax"><code class="docutils literal notranslate"><span class="pre">LogSoftmax()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.core.LogSumExp"><code class="docutils literal notranslate"><span class="pre">LogSumExp()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.core.Softmax"><code class="docutils literal notranslate"><span class="pre">Softmax()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.core.ToFloat"><code class="docutils literal notranslate"><span class="pre">ToFloat()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.core.Mean"><code class="docutils literal notranslate"><span class="pre">Mean()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.core.Min"><code class="docutils literal notranslate"><span class="pre">Min()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.core.Max"><code class="docutils literal notranslate"><span class="pre">Max()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.core.Sum"><code class="docutils literal notranslate"><span class="pre">Sum()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.core.ThresholdToBinary"><code class="docutils literal notranslate"><span class="pre">ThresholdToBinary()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.core.ArgMax"><code class="docutils literal notranslate"><span class="pre">ArgMax()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.core.Negate"><code class="docutils literal notranslate"><span class="pre">Negate()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.core.StopGradient"><code class="docutils literal notranslate"><span class="pre">StopGradient()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.core.one_hot"><code class="docutils literal notranslate"><span class="pre">one_hot()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.core.log_softmax"><code class="docutils literal notranslate"><span class="pre">log_softmax()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.core.log_gaussian_pdf"><code class="docutils literal notranslate"><span class="pre">log_gaussian_pdf()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.core.log_gaussian_diag_pdf"><code class="docutils literal notranslate"><span class="pre">log_gaussian_diag_pdf()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.core.multigaussian_loss"><code class="docutils literal notranslate"><span class="pre">multigaussian_loss()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.core.logsoftmax_sample"><code class="docutils literal notranslate"><span class="pre">logsoftmax_sample()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#module-trax.layers.initializers">initializers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.initializers.InitializerFromFile"><code class="docutils literal notranslate"><span class="pre">InitializerFromFile()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.initializers.RandomNormalInitializer"><code class="docutils literal notranslate"><span class="pre">RandomNormalInitializer()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.initializers.RandomUniformInitializer"><code class="docutils literal notranslate"><span class="pre">RandomUniformInitializer()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.initializers.ScaledInitializer"><code class="docutils literal notranslate"><span class="pre">ScaledInitializer()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.initializers.GlorotNormalInitializer"><code class="docutils literal notranslate"><span class="pre">GlorotNormalInitializer()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.initializers.GlorotUniformInitializer"><code class="docutils literal notranslate"><span class="pre">GlorotUniformInitializer()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.initializers.LeCunNormalInitializer"><code class="docutils literal notranslate"><span class="pre">LeCunNormalInitializer()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.initializers.LeCunUniformInitializer"><code class="docutils literal notranslate"><span class="pre">LeCunUniformInitializer()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.initializers.KaimingNormalInitializer"><code class="docutils literal notranslate"><span class="pre">KaimingNormalInitializer()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.initializers.KaimingUniformInitializer"><code class="docutils literal notranslate"><span class="pre">KaimingUniformInitializer()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.initializers.OrthogonalInitializer"><code class="docutils literal notranslate"><span class="pre">OrthogonalInitializer()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.initializers.AtariConvInit"><code class="docutils literal notranslate"><span class="pre">AtariConvInit()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#module-trax.layers.metrics">metrics</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.metrics.CategoryAccuracy"><code class="docutils literal notranslate"><span class="pre">CategoryAccuracy()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.metrics.WeightedCategoryAccuracy"><code class="docutils literal notranslate"><span class="pre">WeightedCategoryAccuracy()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.metrics.CategoryCrossEntropy"><code class="docutils literal notranslate"><span class="pre">CategoryCrossEntropy()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.metrics.WeightedCategoryCrossEntropy"><code class="docutils literal notranslate"><span class="pre">WeightedCategoryCrossEntropy()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.metrics.BinaryCrossEntropy"><code class="docutils literal notranslate"><span class="pre">BinaryCrossEntropy()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.metrics.MaskedSequenceAccuracy"><code class="docutils literal notranslate"><span class="pre">MaskedSequenceAccuracy()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.metrics.Accuracy"><code class="docutils literal notranslate"><span class="pre">Accuracy()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.metrics.SequenceAccuracy"><code class="docutils literal notranslate"><span class="pre">SequenceAccuracy()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.metrics.CrossEntropyLoss"><code class="docutils literal notranslate"><span class="pre">CrossEntropyLoss()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.metrics.CrossEntropyLossWithLogSoftmax"><code class="docutils literal notranslate"><span class="pre">CrossEntropyLossWithLogSoftmax()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.metrics.BinaryCrossEntropyLoss"><code class="docutils literal notranslate"><span class="pre">BinaryCrossEntropyLoss()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.metrics.L2Loss"><code class="docutils literal notranslate"><span class="pre">L2Loss()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.metrics.SmoothL1Loss"><code class="docutils literal notranslate"><span class="pre">SmoothL1Loss()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.metrics.MacroAveragedFScore"><code class="docutils literal notranslate"><span class="pre">MacroAveragedFScore()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.metrics.WeightedFScore"><code class="docutils literal notranslate"><span class="pre">WeightedFScore()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.metrics.WeightedSum"><code class="docutils literal notranslate"><span class="pre">WeightedSum()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.metrics.CrossEntropySum"><code class="docutils literal notranslate"><span class="pre">CrossEntropySum()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.metrics.BinaryCrossEntropySum"><code class="docutils literal notranslate"><span class="pre">BinaryCrossEntropySum()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#module-trax.layers.normalization">normalization</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.normalization.BatchNorm"><code class="docutils literal notranslate"><span class="pre">BatchNorm</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.normalization.LayerNorm"><code class="docutils literal notranslate"><span class="pre">LayerNorm</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.normalization.FilterResponseNorm"><code class="docutils literal notranslate"><span class="pre">FilterResponseNorm</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#module-trax.layers.pooling">pooling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.pooling.MaxPool"><code class="docutils literal notranslate"><span class="pre">MaxPool()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.pooling.SumPool"><code class="docutils literal notranslate"><span class="pre">SumPool()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.pooling.AvgPool"><code class="docutils literal notranslate"><span class="pre">AvgPool()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#module-trax.layers.reversible">reversible</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.reversible.ReversibleLayer"><code class="docutils literal notranslate"><span class="pre">ReversibleLayer</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.reversible.ReversibleConcatenatePair"><code class="docutils literal notranslate"><span class="pre">ReversibleConcatenatePair</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.reversible.ReversibleSelect"><code class="docutils literal notranslate"><span class="pre">ReversibleSelect</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.reversible.ReversibleSwap"><code class="docutils literal notranslate"><span class="pre">ReversibleSwap()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.reversible.ReversibleReshape"><code class="docutils literal notranslate"><span class="pre">ReversibleReshape</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.reversible.ReversiblePrintShape"><code class="docutils literal notranslate"><span class="pre">ReversiblePrintShape</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.reversible.ReversibleSerial"><code class="docutils literal notranslate"><span class="pre">ReversibleSerial</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.reversible.ReversibleHalfResidual"><code class="docutils literal notranslate"><span class="pre">ReversibleHalfResidual</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#module-trax.layers.rnn">rnn</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.rnn.LSTMCell"><code class="docutils literal notranslate"><span class="pre">LSTMCell</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.rnn.MakeZeroState"><code class="docutils literal notranslate"><span class="pre">MakeZeroState()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.rnn.LSTM"><code class="docutils literal notranslate"><span class="pre">LSTM()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.rnn.GRUCell"><code class="docutils literal notranslate"><span class="pre">GRUCell</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.rnn.GRU"><code class="docutils literal notranslate"><span class="pre">GRU()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.rnn.ConvGRUCell"><code class="docutils literal notranslate"><span class="pre">ConvGRUCell()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.rnn.GeneralGRUCell"><code class="docutils literal notranslate"><span class="pre">GeneralGRUCell()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.rnn.InnerSRUCell"><code class="docutils literal notranslate"><span class="pre">InnerSRUCell()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.rnn.ScanSRUCell"><code class="docutils literal notranslate"><span class="pre">ScanSRUCell()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.rnn.SRU"><code class="docutils literal notranslate"><span class="pre">SRU()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#module-trax.layers.research.efficient_attention">research.efficient_attention</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.research.efficient_attention.length_normalized"><code class="docutils literal notranslate"><span class="pre">length_normalized()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.research.efficient_attention.hash_vecs"><code class="docutils literal notranslate"><span class="pre">hash_vecs()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.research.efficient_attention.look_adjacent"><code class="docutils literal notranslate"><span class="pre">look_adjacent()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.research.efficient_attention.mask_self_attention"><code class="docutils literal notranslate"><span class="pre">mask_self_attention()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.research.efficient_attention.attend"><code class="docutils literal notranslate"><span class="pre">attend()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.research.efficient_attention.apply_broadcasted_dropout"><code class="docutils literal notranslate"><span class="pre">apply_broadcasted_dropout()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.research.efficient_attention.permute_via_gather"><code class="docutils literal notranslate"><span class="pre">permute_via_gather()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.research.efficient_attention.permute_via_sort"><code class="docutils literal notranslate"><span class="pre">permute_via_sort()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.research.efficient_attention.EfficientAttentionBase"><code class="docutils literal notranslate"><span class="pre">EfficientAttentionBase</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.research.efficient_attention.SelfAttention"><code class="docutils literal notranslate"><span class="pre">SelfAttention</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.research.efficient_attention.LSHSelfAttention"><code class="docutils literal notranslate"><span class="pre">LSHSelfAttention</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.research.efficient_attention.PureLSHSelfAttention"><code class="docutils literal notranslate"><span class="pre">PureLSHSelfAttention</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.research.efficient_attention.MixedLSHSelfAttention"><code class="docutils literal notranslate"><span class="pre">MixedLSHSelfAttention</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.research.efficient_attention.PureLSHSelfAttentionWrapper"><code class="docutils literal notranslate"><span class="pre">PureLSHSelfAttentionWrapper</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.research.efficient_attention.EncDecAttention"><code class="docutils literal notranslate"><span class="pre">EncDecAttention</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.research.efficient_attention.LSHFF"><code class="docutils literal notranslate"><span class="pre">LSHFF</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#module-trax.layers.research.position_encodings">research.position_encodings</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.research.position_encodings.AxialPositionalEncoding"><code class="docutils literal notranslate"><span class="pre">AxialPositionalEncoding</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.research.position_encodings.SinCosPositionalEncoding"><code class="docutils literal notranslate"><span class="pre">SinCosPositionalEncoding</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.research.position_encodings.FixedBasePositionalEncoding"><code class="docutils literal notranslate"><span class="pre">FixedBasePositionalEncoding</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.research.position_encodings.threefry_2x32_prf"><code class="docutils literal notranslate"><span class="pre">threefry_2x32_prf()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.research.position_encodings.threefry_2x32_prange"><code class="docutils literal notranslate"><span class="pre">threefry_2x32_prange()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.research.position_encodings.InfinitePositionalEncoding"><code class="docutils literal notranslate"><span class="pre">InfinitePositionalEncoding</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#trax.layers.research.position_encodings.TimeBinPositionalEncoding"><code class="docutils literal notranslate"><span class="pre">TimeBinPositionalEncoding</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="trax.models.html">models.*</a></li>
<li class="toctree-l2"><a class="reference internal" href="trax.data.html">data.*</a></li>
<li class="toctree-l2"><a class="reference internal" href="trax.optimizers.html">optimizers.*</a></li>
<li class="toctree-l2"><a class="reference internal" href="trax.supervised.html">learning.*</a></li>
<li class="toctree-l2"><a class="reference internal" href="trax.rl.html">rl.*</a></li>
<li class="toctree-l2"><a class="reference internal" href="trax.html#shapes">shapes</a></li>
<li class="toctree-l2"><a class="reference internal" href="trax.html#trainer">trainer</a></li>
<li class="toctree-l2"><a class="reference internal" href="trax.html#rl-trainer">rl_trainer</a></li>
<li class="toctree-l2"><a class="reference internal" href="trax.html#trax2keras">trax2keras</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Trax</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="trax.html">trax</a></li>
      <li class="breadcrumb-item active">trax.layers</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/trax.layers.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="trax-layers">
<h1>trax.layers<a class="headerlink" href="#trax-layers" title="Link to this heading"></a></h1>
<section id="module-trax.layers.acceleration">
<span id="acceleration"></span><h2>acceleration<a class="headerlink" href="#module-trax.layers.acceleration" title="Link to this heading"></a></h2>
<p>Modifications to data and computation to use accelerators (better).</p>
<dl class="py class">
<dt class="sig sig-object py" id="trax.layers.acceleration.Accelerate">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.acceleration.</span></span><span class="sig-name descname"><span class="pre">Accelerate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">layer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_devices</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.acceleration.Accelerate" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#trax.layers.base.Layer" title="trax.layers.base.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Layer</span></code></a></p>
<p>Accelerates a layer, running in data-parallel way on multiple devices.</p>
<p>By default it uses all available accelerators, splits the input on the
first (batch) axis, and runs each part on the corresponding accelerator.
If only one accelerator is available, this layer JIT-compiles the underlying
layer and in this way makes it run faster.</p>
<p>The output is guaranteed to be the same as the output of the original layer
if the batch dimension is divisible by the number of devices. If it is not,
then 0-padding is added to make it divisible and the output may be affected
if it relies on layers like batch normalization.</p>
<p>This layer does not require calling <code class="docutils literal notranslate"><span class="pre">init</span></code> if the underlying layer has
already been initialized, so it can be used as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">layer</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">Serial</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">layer</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">fast_layer</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">Accelerate</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">fast_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Split x on batch and run data-parallel</span>
</pre></div>
</div>
<p>In case the weights of this layer need to be set using the weights of
the sublayer, use the <code class="docutils literal notranslate"><span class="pre">replicate_weights</span></code> function:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Instead of layer.weights = new_weights:</span>
<span class="n">fast_layer</span><span class="o">.</span><span class="n">replicate_weights</span><span class="p">(</span><span class="n">new_weights</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.acceleration.Accelerate.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">layer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_devices</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.acceleration.Accelerate.__init__" title="Link to this definition"></a></dt>
<dd><p>Creates a partially initialized, unconnected layer instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_in</strong> – Number of inputs expected by this layer.</p></li>
<li><p><strong>n_out</strong> – Number of outputs promised by this layer.</p></li>
<li><p><strong>name</strong> – Class-like name for this layer; for use when printing this layer.</p></li>
<li><p><strong>sublayers_to_print</strong> – Sublayers to display when printing out this layer;
if None (the default), display all sublayers.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="trax.layers.acceleration.Accelerate.sublayer">
<span class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></span><span class="sig-name descname"><span class="pre">sublayer</span></span><a class="headerlink" href="#trax.layers.acceleration.Accelerate.sublayer" title="Link to this definition"></a></dt>
<dd><p>Returns the unique sublayer managed by this layer.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.acceleration.Accelerate.pure_fn">
<span class="sig-name descname"><span class="pre">pure_fn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rng</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_cache</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.acceleration.Accelerate.pure_fn" title="Link to this definition"></a></dt>
<dd><p>Calls <code class="docutils literal notranslate"><span class="pre">self.sublayer.pure_fn</span></code> in an accelerated way.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.acceleration.Accelerate.init">
<span class="sig-name descname"><span class="pre">init</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_signature</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.acceleration.Accelerate.init" title="Link to this definition"></a></dt>
<dd><p>Calls <code class="docutils literal notranslate"><span class="pre">self.sublayer.init</span></code> and replicates its values onto devices.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.acceleration.Accelerate.replicate_weights">
<span class="sig-name descname"><span class="pre">replicate_weights</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">weights</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.acceleration.Accelerate.replicate_weights" title="Link to this definition"></a></dt>
<dd><p>Sets the weights of the sublayer and replicates them for this layer.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.acceleration.Accelerate.replicate_state">
<span class="sig-name descname"><span class="pre">replicate_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.acceleration.Accelerate.replicate_state" title="Link to this definition"></a></dt>
<dd><p>Sets the state of the sublayer and replicates it for this layer.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="trax.layers.acceleration.Accelerate.weights">
<span class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></span><span class="sig-name descname"><span class="pre">weights</span></span><a class="headerlink" href="#trax.layers.acceleration.Accelerate.weights" title="Link to this definition"></a></dt>
<dd><p>Returns this layer’s weights.</p>
<p>Depending on the layer, the weights can be in the form of:</p>
<blockquote>
<div><ul class="simple">
<li><p>an empty tuple</p></li>
<li><p>a tensor (ndarray)</p></li>
<li><p>a nested structure of tuples and tensors</p></li>
</ul>
</div></blockquote>
<p>If the layer has sublayers, the weights by convention will be
a tuple of length <cite>len(sublayers)</cite> containing the weights of sublayers.
Note that in this case self._weights only marks which ones are shared.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="trax.layers.acceleration.Accelerate.state">
<span class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></span><span class="sig-name descname"><span class="pre">state</span></span><a class="headerlink" href="#trax.layers.acceleration.Accelerate.state" title="Link to this definition"></a></dt>
<dd><p>Returns a tuple containing this layer’s state; may be empty.</p>
<p>If the layer has sublayers, the state by convention will be
a tuple of length <cite>len(sublayers)</cite> containing sublayer states.
Note that in this case self._state only marks which ones are shared.</p>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.acceleration.mean">
<span class="sig-prename descclassname"><span class="pre">trax.layers.acceleration.</span></span><span class="sig-name descname"><span class="pre">mean</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_devices</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.acceleration.mean" title="Link to this definition"></a></dt>
<dd><p>Computes the mean of a distributed value <code class="docutils literal notranslate"><span class="pre">x</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_devices</strong> – Number of devices.</p></li>
<li><p><strong>x</strong> – Distributed array.</p></li>
<li><p><strong>axis</strong> – Axis along which to compute means; can only be <code class="docutils literal notranslate"><span class="pre">0</span></code> or <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A local array.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.acceleration.jit_forward">
<span class="sig-prename descclassname"><span class="pre">trax.layers.acceleration.</span></span><span class="sig-name descname"><span class="pre">jit_forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">forward</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_devices</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">do_mean</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.acceleration.jit_forward" title="Link to this definition"></a></dt>
<dd><p>Returns a JIT-compiled forward function running on <code class="docutils literal notranslate"><span class="pre">n_devices</span></code>.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.acceleration.reshape_by_device">
<span class="sig-prename descclassname"><span class="pre">trax.layers.acceleration.</span></span><span class="sig-name descname"><span class="pre">reshape_by_device</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_devices</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pure_np</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.acceleration.reshape_by_device" title="Link to this definition"></a></dt>
<dd><p>Reshapes possibly nested <code class="docutils literal notranslate"><span class="pre">x</span></code> into a shape <code class="docutils literal notranslate"><span class="pre">(n_devices,</span> <span class="pre">...)</span></code>.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.acceleration.for_n_devices">
<span class="sig-prename descclassname"><span class="pre">trax.layers.acceleration.</span></span><span class="sig-name descname"><span class="pre">for_n_devices</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_devices</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.acceleration.for_n_devices" title="Link to this definition"></a></dt>
<dd><p>Replicates/broadcasts <code class="docutils literal notranslate"><span class="pre">x</span></code> for <code class="docutils literal notranslate"><span class="pre">n_devices</span></code>.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.acceleration.on_cpu">
<span class="sig-prename descclassname"><span class="pre">trax.layers.acceleration.</span></span><span class="sig-name descname"><span class="pre">on_cpu</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.acceleration.on_cpu" title="Link to this definition"></a></dt>
<dd><p>Puts <code class="docutils literal notranslate"><span class="pre">x</span></code> in CPU memory in JAX.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.acceleration.on_accelerator">
<span class="sig-prename descclassname"><span class="pre">trax.layers.acceleration.</span></span><span class="sig-name descname"><span class="pre">on_accelerator</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.acceleration.on_accelerator" title="Link to this definition"></a></dt>
<dd><p>Puts <code class="docutils literal notranslate"><span class="pre">x</span></code> in (single) accelerator memory in JAX.</p>
</dd></dl>

</section>
<section id="module-trax.layers.activation_fns">
<span id="activation-fns"></span><h2>activation_fns<a class="headerlink" href="#module-trax.layers.activation_fns" title="Link to this heading"></a></h2>
<p>Layers that compute activation functions.</p>
<p>An activation layer computes element-wise a nonlinear function of the preceding
layer’s output. Historically, an activation function was considered part of
each node in each layer of the neural network. Trax follows the common current
practice of separating the activation function as its own layer, which enables
easier experimentation across different activation functions.</p>
<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.activation_fns.Relu">
<span class="sig-prename descclassname"><span class="pre">trax.layers.activation_fns.</span></span><span class="sig-name descname"><span class="pre">Relu</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.activation_fns.Relu" title="Link to this definition"></a></dt>
<dd><p>Returns a layer that computes the Rectified Linear Unit (ReLU) function.</p>
<div class="math notranslate nohighlight">
\[\begin{split}f(x) = \left\{ \begin{array}{cl}
    0 &amp; \text{if}\ x \leq 0, \\
    x &amp; \text{otherwise}.
\end{array} \right.\end{split}\]</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.activation_fns.ParametricRelu">
<span class="sig-prename descclassname"><span class="pre">trax.layers.activation_fns.</span></span><span class="sig-name descname"><span class="pre">ParametricRelu</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">a</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.activation_fns.ParametricRelu" title="Link to this definition"></a></dt>
<dd><p>Returns a layer that computes a ReLU function with the given slope.</p>
<div class="math notranslate nohighlight">
\[\begin{split}f(x) = \left\{ \begin{array}{cl}
    0  &amp; \text{if}\ x \leq 0, \\
    ax &amp; \text{otherwise}.
\end{array} \right.\end{split}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>a</strong> – Slope of line for positive inputs.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.activation_fns.LeakyRelu">
<span class="sig-prename descclassname"><span class="pre">trax.layers.activation_fns.</span></span><span class="sig-name descname"><span class="pre">LeakyRelu</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">a</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.01</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.activation_fns.LeakyRelu" title="Link to this definition"></a></dt>
<dd><p>Returns a ReLU-like layer with linear nonzero outputs for negative inputs.</p>
<div class="math notranslate nohighlight">
\[\begin{split}f(x) = \left\{ \begin{array}{cl}
    ax &amp; \text{if}\ x \leq 0, \\
    x  &amp; \text{otherwise}.
\end{array} \right.\end{split}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>a</strong> – Slope of line for negative inputs.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.activation_fns.Elu">
<span class="sig-prename descclassname"><span class="pre">trax.layers.activation_fns.</span></span><span class="sig-name descname"><span class="pre">Elu</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">a</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.activation_fns.Elu" title="Link to this definition"></a></dt>
<dd><p>Returns a ReLU-like layer with exponential outputs for negative inputs.</p>
<div class="math notranslate nohighlight">
\[\begin{split}f(x) = \left\{ \begin{array}{cl}
    a \cdot (e^x - 1) &amp; \text{if}\ x \leq 0, \\
    x                 &amp; \text{otherwise}.
\end{array} \right.\end{split}\]</div>
<p>(Asymptotically, <span class="math notranslate nohighlight">\(f(x)\rightarrow -a\)</span> as <span class="math notranslate nohighlight">\(x\rightarrow - \infty\)</span>.)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>a</strong> – Coefficient multiplying the exponential, for negative inputs.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.activation_fns.Selu">
<span class="sig-prename descclassname"><span class="pre">trax.layers.activation_fns.</span></span><span class="sig-name descname"><span class="pre">Selu</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.6732632423543772</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lmbda</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0507009873554805</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.activation_fns.Selu" title="Link to this definition"></a></dt>
<dd><p>Returns an <cite>Elu</cite>-like layer with an additional scaling/slope parameter.</p>
<div class="math notranslate nohighlight">
\[\begin{split}f(x) = \left\{ \begin{array}{cl}
    \lambda \cdot \alpha \cdot (e^x - 1) &amp; \text{if}\ x \leq 0, \\
    \lambda \cdot x                      &amp; \text{otherwise}.
\end{array} \right.\end{split}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>alpha</strong> – Coefficient multiplying the exponential, for negative inputs.</p></li>
<li><p><strong>lmbda</strong> – Coefficient scaling the whole function.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.activation_fns.Gelu">
<span class="sig-prename descclassname"><span class="pre">trax.layers.activation_fns.</span></span><span class="sig-name descname"><span class="pre">Gelu</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.activation_fns.Gelu" title="Link to this definition"></a></dt>
<dd><p>Returns a layer that computes the Gaussian Error Linear Unit function.</p>
<div class="math notranslate nohighlight">
\[f(x) = \frac{x}{2} \cdot (1 + \hbox{erf}(\frac{x}{\sqrt{2}}))\]</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.activation_fns.FastGelu">
<span class="sig-prename descclassname"><span class="pre">trax.layers.activation_fns.</span></span><span class="sig-name descname"><span class="pre">FastGelu</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.activation_fns.FastGelu" title="Link to this definition"></a></dt>
<dd><p>Returns a layer that computes a fast approximation to <cite>Gelu</cite>.</p>
<div class="math notranslate nohighlight">
\[f(x) = \frac{x}{2} \cdot (1 + \tanh(ax + abx^3))\]</div>
<p>where <span class="math notranslate nohighlight">\(a = 0.7978845608\)</span> and <span class="math notranslate nohighlight">\(b = 0.044715\)</span>.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.activation_fns.Sigmoid">
<span class="sig-prename descclassname"><span class="pre">trax.layers.activation_fns.</span></span><span class="sig-name descname"><span class="pre">Sigmoid</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.activation_fns.Sigmoid" title="Link to this definition"></a></dt>
<dd><p>Returns a layer that computes the sigmoid function.</p>
<div class="math notranslate nohighlight">
\[f(x) = \frac{1}{1 + e^{-x}}\]</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.activation_fns.Tanh">
<span class="sig-prename descclassname"><span class="pre">trax.layers.activation_fns.</span></span><span class="sig-name descname"><span class="pre">Tanh</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.activation_fns.Tanh" title="Link to this definition"></a></dt>
<dd><p>Returns a layer that computes the hyperbolic tangent function.</p>
<div class="math notranslate nohighlight">
\[f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}\]</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.activation_fns.HardSigmoid">
<span class="sig-prename descclassname"><span class="pre">trax.layers.activation_fns.</span></span><span class="sig-name descname"><span class="pre">HardSigmoid</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.activation_fns.HardSigmoid" title="Link to this definition"></a></dt>
<dd><p>Returns a layer that computes a linear approximation to <cite>Sigmoid</cite>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}f(x) = \left\{ \begin{array}{cl}
    0 &amp; \text{if}\ x \leq 0, \\
    x &amp; \text{if}\ 0 &lt; x &lt; 1, \\
    1 &amp; \text{otherwise}.
\end{array} \right.\end{split}\]</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.activation_fns.HardTanh">
<span class="sig-prename descclassname"><span class="pre">trax.layers.activation_fns.</span></span><span class="sig-name descname"><span class="pre">HardTanh</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.activation_fns.HardTanh" title="Link to this definition"></a></dt>
<dd><p>Returns a layer that computes a linear approximation to <cite>Tanh</cite>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}f(x) = \left\{ \begin{array}{cl}
    -1 &amp; \text{if}\ x \leq -1, \\
    x  &amp; \text{if}\ -1 &lt; x &lt; 1, \\
    1  &amp; \text{otherwise}.
\end{array} \right.\end{split}\]</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.activation_fns.Softplus">
<span class="sig-prename descclassname"><span class="pre">trax.layers.activation_fns.</span></span><span class="sig-name descname"><span class="pre">Softplus</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.activation_fns.Softplus" title="Link to this definition"></a></dt>
<dd><p>Returns a layer that computes the softplus function.</p>
<div class="math notranslate nohighlight">
\[f(x) = \ln(e^x + 1)\]</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.activation_fns.Exp">
<span class="sig-prename descclassname"><span class="pre">trax.layers.activation_fns.</span></span><span class="sig-name descname"><span class="pre">Exp</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.activation_fns.Exp" title="Link to this definition"></a></dt>
<dd><p>Returns a layer that computes the element-wise exponential of a tensor.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.activation_fns.Log">
<span class="sig-prename descclassname"><span class="pre">trax.layers.activation_fns.</span></span><span class="sig-name descname"><span class="pre">Log</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.activation_fns.Log" title="Link to this definition"></a></dt>
<dd><p>Returns a layer that computes the element-wise logarithm of a tensor.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.activation_fns.Swish">
<span class="sig-prename descclassname"><span class="pre">trax.layers.activation_fns.</span></span><span class="sig-name descname"><span class="pre">Swish</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.activation_fns.Swish" title="Link to this definition"></a></dt>
<dd><p>Returns a layer that computes the Swish function.</p>
<div class="math notranslate nohighlight">
\[f(x) = x \cdot \text{sigmoid}(x)\]</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.activation_fns.Glu">
<span class="sig-prename descclassname"><span class="pre">trax.layers.activation_fns.</span></span><span class="sig-name descname"><span class="pre">Glu</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.activation_fns.Glu" title="Link to this definition"></a></dt>
<dd><p>Returns a layer that computes the Gated Linear Unit function.</p>
<div class="math notranslate nohighlight">
\[f(x) = a \cdot \text{sigmoid}(b)\]</div>
<p>where a and b are formed by splitting input in half along axis</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="trax.layers.activation_fns.ThresholdedLinearUnit">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.activation_fns.</span></span><span class="sig-name descname"><span class="pre">ThresholdedLinearUnit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_in</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_out</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sublayers_to_print</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.activation_fns.ThresholdedLinearUnit" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#trax.layers.base.Layer" title="trax.layers.base.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Layer</span></code></a></p>
<p>Thresholded Linear Unit, c.f. <a class="reference external" href="https://arxiv.org/pdf/1911.09737.pdf">https://arxiv.org/pdf/1911.09737.pdf</a> .</p>
<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.activation_fns.ThresholdedLinearUnit.init_weights_and_state">
<span class="sig-name descname"><span class="pre">init_weights_and_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_signature</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.activation_fns.ThresholdedLinearUnit.init_weights_and_state" title="Link to this definition"></a></dt>
<dd><p>Initializes this layer’s single weight to zero.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.activation_fns.ThresholdedLinearUnit.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.activation_fns.ThresholdedLinearUnit.forward" title="Link to this definition"></a></dt>
<dd><p>Executes this layer as part of a forward pass through the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>inputs</strong> – Tensor.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Tensor of same shape and dtype as the input.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-trax.layers.attention">
<span id="attention"></span><h2>attention<a class="headerlink" href="#module-trax.layers.attention" title="Link to this heading"></a></h2>
<p>Attention-related layers, as used in Transformer(-like) models.</p>
<p>Attention is a trainable mechanism for mapping between collections of vectors:</p>
<div class="math notranslate nohighlight">
\[\text{Attention}: \mathbf{X}^{n} \rightarrow \mathbf{X}^{n}\!,
\ \text{for} \ \mathbf{X} \in \mathbb{R}^d\]</div>
<p>Whereas classic neural networks assemble nodes of <em>numbers</em> with weighted
connections:</p>
<blockquote>
<div><ul class="simple">
<li><p>node activations: floating point values (one float per node)</p></li>
<li><p>inter-node connections: trainable weights (one float per connection),</p></li>
</ul>
</div></blockquote>
<p>attention lets one assemble nodes of <em>vectors</em> and use further vectors to
calculate connection strengths:</p>
<blockquote>
<div><ul class="simple">
<li><p>node activations: floating point vectors, and</p></li>
<li><p>inter-node connections: computed using trainable vectors.</p></li>
</ul>
</div></blockquote>
<p>Computing connection strengths involves several concepts – queries, keys,
values, masks, attention heads – that factor heavily into the API below.</p>
<p>NOTE: Attention, positional encoding, and shift layers in this module include
<code class="docutils literal notranslate"><span class="pre">mode</span></code>-dependent behavior. The possible modes are:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">'train'</span></code>: in training – dropouts and position shifts active</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'eval'</span></code>:  in evals – dropouts inactive, position shifts active</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'predict'</span></code>: in prediction – dropouts and position shifts inactive</p></li>
</ul>
</div></blockquote>
<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.attention.Attention">
<span class="sig-prename descclassname"><span class="pre">trax.layers.attention.</span></span><span class="sig-name descname"><span class="pre">Attention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d_feature</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.attention.Attention" title="Link to this definition"></a></dt>
<dd><p>Returns a layer that maps <cite>(vectors, mask)</cite> to <cite>(new_vectors, mask)</cite>.</p>
<p>This layer type represents one pass of multi-head self-attention, from vector
set to vector set, using masks to represent out-of-bound (e.g., padding)
positions. It:</p>
<blockquote>
<div><ul class="simple">
<li><p>makes three copies of incoming activations and maps these to multi-head
query (Q) vectors, key (K) vectors, and value (V) vectors, respectively;</p></li>
<li><p>for each head, computes the scaled dot product of each Q-K pair;</p></li>
<li><p>applies mask to screen out positions that come from padding tokens
(indicated by 0 value);</p></li>
<li><p>[in <code class="docutils literal notranslate"><span class="pre">'train'</span></code> mode] applies dropout to Q-K dot products;</p></li>
<li><p>for each head, computes Q-K attention strengths using a per-query softmax
of the Q-K dot products;</p></li>
<li><p>for each head, for each query position, combines V vectors according
to the Q-K attention strengths; and</p></li>
<li><p>concatenates and fuses resulting per-head vectors into outgoing
activations matching original input activation shapes.</p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>d_feature</strong> – Last/innermost dimension of activations in the input to and
output from this layer.</p></li>
<li><p><strong>n_heads</strong> – Number of attention heads. Attention heads effectively split
activation vectors into <code class="docutils literal notranslate"><span class="pre">n_heads</span></code> subvectors, of size
<code class="docutils literal notranslate"><span class="pre">d_feature</span> <span class="pre">/</span> <span class="pre">n_heads</span></code>.</p></li>
<li><p><strong>dropout</strong> – Probababilistic rate for attention dropout, which overrides
(sets to zero) some attention strengths derived from query-key
matching. As a result, on a given forward pass, some value vectors
don’t contribute to the output, analogous to how regular dropout can
cause some node activations to be ignored. Applies only if layer is
created in <code class="docutils literal notranslate"><span class="pre">'train'</span></code> mode.</p></li>
<li><p><strong>mode</strong> – One of <code class="docutils literal notranslate"><span class="pre">'train'</span></code>, <code class="docutils literal notranslate"><span class="pre">'eval'</span></code>, or <code class="docutils literal notranslate"><span class="pre">'predict'</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.attention.AttentionQKV">
<span class="sig-prename descclassname"><span class="pre">trax.layers.attention.</span></span><span class="sig-name descname"><span class="pre">AttentionQKV</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d_feature</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache_KV_in_predict</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">q_sparsity</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">result_sparsity</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.attention.AttentionQKV" title="Link to this definition"></a></dt>
<dd><p>Returns a layer that maps <cite>(AQ, AK, AV, mask)</cite> to <cite>(new-A, mask)</cite>.</p>
<p>Unlike <a class="reference internal" href="#trax.layers.attention.Attention" title="trax.layers.attention.Attention"><code class="xref py py-class docutils literal notranslate"><span class="pre">Attention</span></code></a> above, <a class="reference internal" href="#trax.layers.attention.AttentionQKV" title="trax.layers.attention.AttentionQKV"><code class="xref py py-class docutils literal notranslate"><span class="pre">AttentionQKV</span></code></a> allows the
incoming activations (<cite>AQ</cite>, <cite>AK</cite>, and <cite>AV</cite>) to come from different sources.
This is used, for instance, in encoder-decoder attention (Q-related
activations <cite>AQ</cite> from the decoder, K- and V-related activations – <cite>AK</cite> and
<cite>AV</cite> – from the encoder). Otherwise, see the <a class="reference internal" href="#trax.layers.attention.Attention" title="trax.layers.attention.Attention"><code class="xref py py-class docutils literal notranslate"><span class="pre">Attention</span></code></a>
description for further context/details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>d_feature</strong> – Last/innermost dimension of activations in the input to and
output from this layer.</p></li>
<li><p><strong>n_heads</strong> – Number of attention heads. Attention heads effectively split
activation vectors into <code class="docutils literal notranslate"><span class="pre">n_heads</span></code> subvectors, of size
<code class="docutils literal notranslate"><span class="pre">d_feature</span> <span class="pre">/</span> <span class="pre">n_heads</span></code>.</p></li>
<li><p><strong>dropout</strong> – Probababilistic rate for attention dropout, which overrides
(sets to zero) some attention strengths derived from query-key
matching. As a result, on a given forward pass, some value vectors
don’t contribute to the output, analogous to how regular dropout can
cause some node activations to be ignored. Applies only if layer is
created in <code class="docutils literal notranslate"><span class="pre">'train'</span></code> mode.</p></li>
<li><p><strong>mode</strong> – One of <code class="docutils literal notranslate"><span class="pre">'train'</span></code>, <code class="docutils literal notranslate"><span class="pre">'eval'</span></code>, or <code class="docutils literal notranslate"><span class="pre">'predict'</span></code>.</p></li>
<li><p><strong>cache_KV_in_predict</strong> – Whether to cache K/V arrays in <code class="docutils literal notranslate"><span class="pre">'predict'</span></code> mode.</p></li>
<li><p><strong>q_sparsity</strong> – Sparsity with which to process queries. If <code class="docutils literal notranslate"><span class="pre">None</span></code>,
<code class="xref py py-class docutils literal notranslate"><span class="pre">Dense</span></code> is used; if <code class="docutils literal notranslate"><span class="pre">'noop'</span></code>, no processing is used.</p></li>
<li><p><strong>result_sparsity</strong> – Sparsity with which to process result of the attention.
If <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dense</span></code> is used; if <code class="docutils literal notranslate"><span class="pre">'noop'</span></code>, no processing is
used.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="trax.layers.attention.PureAttention">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.attention.</span></span><span class="sig-name descname"><span class="pre">PureAttention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.attention.PureAttention" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#trax.layers.base.Layer" title="trax.layers.base.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Layer</span></code></a></p>
<p>Returns a layer that maps <cite>(Q, K, V, mask)</cite> to <cite>(activations, mask)</cite>.</p>
<p>This layer type performs the inner workings of one pass of multi-head
self-attention. It:</p>
<blockquote>
<div><ul class="simple">
<li><p>subdivides incoming Q/K/V activations into multi-head versions;</p></li>
<li><p>for each head, computes the scaled dot product of each Q-K pair;</p></li>
<li><p>applies mask to screen out positions that come from padding tokens
(indicated by 0 value);</p></li>
<li><p>[in <code class="docutils literal notranslate"><span class="pre">'train'</span></code> mode] applies dropout to Q-K dot products;</p></li>
<li><p>for each head, computes Q-K attention strengths using a per-query softmax
of the Q-K dot products;</p></li>
<li><p>for each head, for each query position, combines V vectors according
to the Q-K attention strengths; and</p></li>
<li><p>concatenates and fuses resulting per-head vectors into outgoing
activations matching original input activation shapes.</p></li>
</ul>
</div></blockquote>
<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.attention.PureAttention.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.attention.PureAttention.__init__" title="Link to this definition"></a></dt>
<dd><p>Returns a new <a class="reference internal" href="#trax.layers.attention.PureAttention" title="trax.layers.attention.PureAttention"><code class="xref py py-class docutils literal notranslate"><span class="pre">PureAttention</span></code></a> instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_heads</strong> – Number of attention heads.</p></li>
<li><p><strong>dropout</strong> – Probababilistic rate for attention dropout, which overrides
(sets to zero) some attention strengths derived from query-key
matching. As a result, on a given forward pass, some value vectors
don’t contribute to the output, analogous to how regular dropout can
cause some node activations to be ignored. Applies only if layer is
created in <code class="docutils literal notranslate"><span class="pre">'train'</span></code> mode.</p></li>
<li><p><strong>mode</strong> – One of <code class="docutils literal notranslate"><span class="pre">'train'</span></code>, <code class="docutils literal notranslate"><span class="pre">'eval'</span></code>, or <code class="docutils literal notranslate"><span class="pre">'predict'</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.attention.PureAttention.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.attention.PureAttention.forward" title="Link to this definition"></a></dt>
<dd><p>Returns attention-computed activations and unmodified mask.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>inputs</strong> – A <cite>(Q, K, V, mask)</cite> tuple, whose query, key, and value
activations have not yet been subdivided into heads.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="trax.layers.attention.DotProductAttention">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.attention.</span></span><span class="sig-name descname"><span class="pre">DotProductAttention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.attention.DotProductAttention" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#trax.layers.base.Layer" title="trax.layers.base.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Layer</span></code></a></p>
<p>Returns a layer that computes per-head attention (via scaled dot-product).</p>
<p>This layer computes the core of the attention mechanism. Given per-head
queries (Q), keys (K), values (V), and mask, it:</p>
<blockquote>
<div><ul class="simple">
<li><p>computes the scaled dot product of each Q-K pair;</p></li>
<li><p>applies mask to screen out positions that come from padding tokens
(indicated by 0 value);</p></li>
<li><p>[if created in <code class="docutils literal notranslate"><span class="pre">'train'</span></code> mode] applies dropout to Q-K dot products;</p></li>
<li><p>computes Q-K attention strengths using a per-query softmax of the Q-K dot
products; and</p></li>
<li><p>for each query position, combines V vectors according to the Q-K
attention strengths.</p></li>
</ul>
</div></blockquote>
<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.attention.DotProductAttention.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.attention.DotProductAttention.__init__" title="Link to this definition"></a></dt>
<dd><p>Creates a <a class="reference internal" href="#trax.layers.attention.DotProductAttention" title="trax.layers.attention.DotProductAttention"><code class="xref py py-class docutils literal notranslate"><span class="pre">DotProductAttention</span></code></a> instance in a specific mode.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dropout</strong> – Probababilistic rate for attention dropout, which overrides
(sets to zero) some attention strengths derived from query-key
matching. As a result, on a given forward pass, some value vectors
don’t contribute to the output, analogous to how regular dropout can
cause some node activations to be ignored. Applies only if layer is
created in <code class="docutils literal notranslate"><span class="pre">'train'</span></code> mode.</p></li>
<li><p><strong>mode</strong> – One of <code class="docutils literal notranslate"><span class="pre">'train'</span></code>, <code class="docutils literal notranslate"><span class="pre">'eval'</span></code>, <code class="docutils literal notranslate"><span class="pre">'predict'</span></code> or <code class="docutils literal notranslate"><span class="pre">'viz'</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.attention.DotProductAttention.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.attention.DotProductAttention.forward" title="Link to this definition"></a></dt>
<dd><p>Returns attention-computed per-head activations and unchanged mask.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>inputs</strong> – A <cite>(Q, K, V, mask)</cite> tuple, whose query, key, and value
activations have been subdivided into heads.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.attention.SplitIntoHeads">
<span class="sig-prename descclassname"><span class="pre">trax.layers.attention.</span></span><span class="sig-name descname"><span class="pre">SplitIntoHeads</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">merged_batch_and_head</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.attention.SplitIntoHeads" title="Link to this definition"></a></dt>
<dd><p>Returns a layer that reshapes an array for multi-head computation.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.attention.MergeHeads">
<span class="sig-prename descclassname"><span class="pre">trax.layers.attention.</span></span><span class="sig-name descname"><span class="pre">MergeHeads</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">merged_batch_and_head</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.attention.MergeHeads" title="Link to this definition"></a></dt>
<dd><p>Returns a layer that rejoins heads, after multi-head computation.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.attention.ConfigurableAttention">
<span class="sig-prename descclassname"><span class="pre">trax.layers.attention.</span></span><span class="sig-name descname"><span class="pre">ConfigurableAttention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">q_layer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k_layer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">v_layer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">final_layer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qkv_attention_layer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.attention.ConfigurableAttention" title="Link to this definition"></a></dt>
<dd><p>Returns a configured multi-head self-attention layer.</p>
<p>A <a class="reference internal" href="#trax.layers.attention.ConfigurableAttention" title="trax.layers.attention.ConfigurableAttention"><code class="xref py py-class docutils literal notranslate"><span class="pre">ConfigurableAttention</span></code></a> layer acts similarly to
<a class="reference internal" href="#trax.layers.attention.Attention" title="trax.layers.attention.Attention"><code class="xref py py-class docutils literal notranslate"><span class="pre">Attention</span></code></a> layers, but with configurable components. It</p>
<blockquote>
<div><ul class="simple">
<li><p>makes three copies of incoming activations and uses <code class="docutils literal notranslate"><span class="pre">q_layer</span></code>,
<code class="docutils literal notranslate"><span class="pre">k_layer</span></code>, and <code class="docutils literal notranslate"><span class="pre">v_layer</span></code> to map activations to multi-head query (Q)
vectors, key (K) vectors, and value (V) vectors, respectively;</p></li>
<li><p>uses <code class="docutils literal notranslate"><span class="pre">qkv_attention_layer</span></code> to compute per-head attention, similar to
<a class="reference internal" href="#trax.layers.attention.DotProductAttention" title="trax.layers.attention.DotProductAttention"><code class="xref py py-class docutils literal notranslate"><span class="pre">DotProductAttention</span></code></a> or <a class="reference internal" href="#trax.layers.attention.DotProductCausalAttention" title="trax.layers.attention.DotProductCausalAttention"><code class="xref py py-class docutils literal notranslate"><span class="pre">DotProductCausalAttention</span></code></a>;</p></li>
<li><p>concatenates and fuses resulting per-head vectors into activations
matching original input activation shapes; and</p></li>
<li><p>applies a final layer, <code class="docutils literal notranslate"><span class="pre">final_layer</span></code>, mapping activations to
activations (with shape matching the original input activations).</p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>q_layer</strong> – Layer that maps input activations to per-head query activations.</p></li>
<li><p><strong>k_layer</strong> – Layer that maps input activations to per-head key activations.</p></li>
<li><p><strong>v_layer</strong> – Layer that maps input activations to per-head value activations.</p></li>
<li><p><strong>final_layer</strong> – After main multi-head computation and rejoining of heads,
layer that maps activations to activations (with shape matching the
original input activations).</p></li>
<li><p><strong>qkv_attention_layer</strong> – Layer the does the core multi-head self-attention
computation.</p></li>
<li><p><strong>n_heads</strong> – Number of attention heads. Attention heads effectively split
activation vectors into <code class="docutils literal notranslate"><span class="pre">n_heads</span></code> subvectors, of size
<code class="docutils literal notranslate"><span class="pre">d_feature</span> <span class="pre">/</span> <span class="pre">n_heads</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.attention.CausalAttention">
<span class="sig-prename descclassname"><span class="pre">trax.layers.attention.</span></span><span class="sig-name descname"><span class="pre">CausalAttention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d_feature</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_inference_length</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2048</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_dconv</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.attention.CausalAttention" title="Link to this definition"></a></dt>
<dd><p>Returns a layer that maps activations to activations, with causal masking.</p>
<p>Like <a class="reference internal" href="#trax.layers.attention.Attention" title="trax.layers.attention.Attention"><code class="xref py py-class docutils literal notranslate"><span class="pre">Attention</span></code></a>, this layer type represents one pass of multi-head
self-attention, but with causal masking rather than padding-based masking.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>d_feature</strong> – Last/innermost dimension of activations in the input to and
output from this layer.</p></li>
<li><p><strong>n_heads</strong> – Number of attention heads. Attention heads effectively split
activation vectors into <code class="docutils literal notranslate"><span class="pre">n_heads</span></code> subvectors, of size
<code class="docutils literal notranslate"><span class="pre">d_feature</span> <span class="pre">/</span> <span class="pre">n_heads</span></code>.</p></li>
<li><p><strong>dropout</strong> – Probababilistic rate for attention dropout, which overrides
(sets to zero) some attention strengths derived from query-key
matching. As a result, on a given forward pass, some value vectors
don’t contribute to the output, analogous to how regular dropout can
cause some node activations to be ignored. Applies only if layer is
created in <code class="docutils literal notranslate"><span class="pre">'train'</span></code> mode.</p></li>
<li><p><strong>max_inference_length</strong> – Maximum sequence length allowed in non-training
modes.</p></li>
<li><p><strong>use_dconv</strong> – if True, use depthwise convolutions on top of dense layers
for Q, K and V.</p></li>
<li><p><strong>mode</strong> – One of <code class="docutils literal notranslate"><span class="pre">'train'</span></code>, <code class="docutils literal notranslate"><span class="pre">'eval'</span></code>, or <code class="docutils literal notranslate"><span class="pre">'predict'</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="trax.layers.attention.DotProductCausalAttention">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.attention.</span></span><span class="sig-name descname"><span class="pre">DotProductCausalAttention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_inference_length</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2048</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.attention.DotProductCausalAttention" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#trax.layers.base.Layer" title="trax.layers.base.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Layer</span></code></a></p>
<p>Layer that computes attention strengths by masking out the “future”.</p>
<p>Causal attention uses masking to prevent a given sequence position from
attending to positions greater than / following it. This is used, for
example, when training autoregressive sequence models, or when decoding a
sequence symbol by symbol.</p>
<p>This layer performs the core per-head attention calculation. The layer
assumes that any splitting into attention heads precedes it, and that any
merging of attention heads will follow it.</p>
<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.attention.DotProductCausalAttention.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_inference_length</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2048</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.attention.DotProductCausalAttention.__init__" title="Link to this definition"></a></dt>
<dd><p>Creates a <a class="reference internal" href="#trax.layers.attention.DotProductCausalAttention" title="trax.layers.attention.DotProductCausalAttention"><code class="xref py py-class docutils literal notranslate"><span class="pre">DotProductCausalAttention</span></code></a> instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dropout</strong> – Probababilistic rate for attention dropout, which overrides
(sets to zero) some attention strengths derived from query-key
matching. As a result, on a given forward pass, some value vectors
don’t contribute to the output, analogous to how regular dropout can
cause some node activations to be ignored. Applies only if layer is
created in <code class="docutils literal notranslate"><span class="pre">'train'</span></code> mode.</p></li>
<li><p><strong>max_inference_length</strong> – Maximum sequence length allowed in non-training
modes.</p></li>
<li><p><strong>mode</strong> – One of <code class="docutils literal notranslate"><span class="pre">'train'</span></code>, <code class="docutils literal notranslate"><span class="pre">'eval'</span></code>, or <code class="docutils literal notranslate"><span class="pre">'predict'</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.attention.DotProductCausalAttention.monkey_patched_mask">
<span class="sig-name descname"><span class="pre">monkey_patched_mask</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.attention.DotProductCausalAttention.monkey_patched_mask" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.attention.DotProductCausalAttention.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.attention.DotProductCausalAttention.forward" title="Link to this definition"></a></dt>
<dd><p>Returns attention-computed activations.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>inputs</strong> – A (queries, keys, values) tuple.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.attention.DotProductCausalAttention.init_weights_and_state">
<span class="sig-name descname"><span class="pre">init_weights_and_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_signature</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.attention.DotProductCausalAttention.init_weights_and_state" title="Link to this definition"></a></dt>
<dd><p>Initializes this layer for fast inference, if in <code class="docutils literal notranslate"><span class="pre">'predict'</span></code> mode.</p>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.attention.ShiftRight">
<span class="sig-prename descclassname"><span class="pre">trax.layers.attention.</span></span><span class="sig-name descname"><span class="pre">ShiftRight</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_positions</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.attention.ShiftRight" title="Link to this definition"></a></dt>
<dd><p>Returns a layer that can insert padding to shift the input sequence.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_positions</strong> – Number of positions to shift the input sequence rightward;
initial positions freed by the shift get padded with zeros. Applies
only if layer is created in a non-<code class="docutils literal notranslate"><span class="pre">'eval'</span></code> mode.</p></li>
<li><p><strong>mode</strong> – One of <code class="docutils literal notranslate"><span class="pre">'train'</span></code>, <code class="docutils literal notranslate"><span class="pre">'eval'</span></code>, or <code class="docutils literal notranslate"><span class="pre">'predict'</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.attention.PaddingMask">
<span class="sig-prename descclassname"><span class="pre">trax.layers.attention.</span></span><span class="sig-name descname"><span class="pre">PaddingMask</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.attention.PaddingMask" title="Link to this definition"></a></dt>
<dd><p>Returns a layer that maps integer sequences to padding masks.</p>
<p>The layer expects as input a batch of integer sequences. The layer output is
an N-D array that marks for each sequence position whether the integer (e.g.,
a token ID) in that position represents padding – value <code class="docutils literal notranslate"><span class="pre">pad</span></code> – versus
text/content – all other values. The padding mask shape is
(batch_size, 1, 1, encoder_sequence_length), such that axis 1 will broadcast
to cover any number of attention heads and axis 2 will broadcast to cover
decoder sequence positions.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>pad</strong> – Integer that represents padding rather than a token/content ID.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.attention.EncoderDecoderMask">
<span class="sig-prename descclassname"><span class="pre">trax.layers.attention.</span></span><span class="sig-name descname"><span class="pre">EncoderDecoderMask</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.attention.EncoderDecoderMask" title="Link to this definition"></a></dt>
<dd><p>Returns a layer that creates a mask for encoder-decoder cross attention.</p>
<p>The layer expects two inputs:</p>
<blockquote>
<div><ul class="simple">
<li><p>decoder_input: batch of integer (e.g., token ID) sequences</p></li>
<li><p>mask: padding mask from the encoder</p></li>
</ul>
</div></blockquote>
<p>The layer output is a mask that marks for each sequence position (for both
encoder and decoder) whether that position can be attended to or not. The
encoder-decoder mask shape is (batch_size, 1, decoder_sequence_length,
encoder_sequence_length), such that axis 1 will automatically broadcast to
cover any number of attention heads.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="trax.layers.attention.PositionalEncoding">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.attention.</span></span><span class="sig-name descname"><span class="pre">PositionalEncoding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">max_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2048</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout_broadcast_dims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(-2,)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_bfloat16</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">start_from_zero_prob</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_offset_to_add</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_feature</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.attention.PositionalEncoding" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#trax.layers.base.Layer" title="trax.layers.base.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Layer</span></code></a></p>
<p>Implements bare positional encoding.</p>
<p>Positional encoding includes a kind of dropout, if the layer is created in
<code class="docutils literal notranslate"><span class="pre">'train'</span></code> mode with a nonzero <code class="docutils literal notranslate"><span class="pre">dropout</span></code> value. For such a layer, on each
forward pass a subset of sequence positions selected at random will <em>not</em>
receive positional marking.</p>
<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.attention.PositionalEncoding.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">max_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2048</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout_broadcast_dims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(-2,)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_bfloat16</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">start_from_zero_prob</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_offset_to_add</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_feature</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.attention.PositionalEncoding.__init__" title="Link to this definition"></a></dt>
<dd><p>Creates a <a class="reference internal" href="#trax.layers.attention.PositionalEncoding" title="trax.layers.attention.PositionalEncoding"><code class="xref py py-class docutils literal notranslate"><span class="pre">PositionalEncoding</span></code></a> instance in a given mode.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>max_len</strong> – Maximum input sequence length.</p></li>
<li><p><strong>dropout</strong> – Probability of <em>not</em> adding positional encoding to a sequence
position. Applies only if layer is created in <code class="docutils literal notranslate"><span class="pre">'train'</span></code> mode.</p></li>
<li><p><strong>dropout_broadcast_dims</strong> – Axes along which dropout mask values are
broadcast rather than individually set at random.</p></li>
<li><p><strong>use_bfloat16</strong> – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, use bfloat16 weights instead of the default
float32; this can save memory but may (rarely) lead to numerical issues.</p></li>
<li><p><strong>start_from_zero_prob</strong> – how often to start from 0 during training,
(if 1.0, we always start from position 0, if less, we randomize).</p></li>
<li><p><strong>max_offset_to_add</strong> – maximum offset to add to the positions during training
when randomizing; this offset plus input length must still be less than
max_len for all training examples.</p></li>
<li><p><strong>d_feature</strong> – int or None; have this dimension for embeddings + shared FF if
not None.</p></li>
<li><p><strong>mode</strong> – One of <code class="docutils literal notranslate"><span class="pre">'train'</span></code>, <code class="docutils literal notranslate"><span class="pre">'eval'</span></code>, or <code class="docutils literal notranslate"><span class="pre">'predict'</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.attention.PositionalEncoding.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.attention.PositionalEncoding.forward" title="Link to this definition"></a></dt>
<dd><p>Returns the input activations, with added positional information.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.attention.PositionalEncoding.init_weights_and_state">
<span class="sig-name descname"><span class="pre">init_weights_and_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_signature</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.attention.PositionalEncoding.init_weights_and_state" title="Link to this definition"></a></dt>
<dd><p>Randomly initializes the positional encoding vectors.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input_signature</strong> – <code class="xref py py-class docutils literal notranslate"><span class="pre">ShapeDtype</span></code> instance characterizing the input
this layer should compute on.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-trax.layers.base">
<span id="base"></span><h2>base<a class="headerlink" href="#module-trax.layers.base" title="Link to this heading"></a></h2>
<p>The key layer abstraction (Layer class) and supporting machinery.</p>
<dl class="py class">
<dt class="sig sig-object py" id="trax.layers.base.Layer">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.base.</span></span><span class="sig-name descname"><span class="pre">Layer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_in</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_out</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sublayers_to_print</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.base.Layer" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Base class for composable layers in a deep learning network.</p>
<p>Layers are the basic building blocks for deep learning models. A layer
computes a function from zero or more inputs to zero or more outputs,
optionally using trainable weights (common) and non-parameter state (not
common).</p>
<p>Layer subclasses typically override at most two methods of the training <cite>Layer</cite>
class:</p>
<blockquote>
<div><dl class="simple">
<dt><cite>forward(inputs)</cite>:</dt><dd><p>Computes the layer’s output as part of a forward pass through the model.</p>
</dd>
<dt><cite>init_weights_and_state(self, input_signature)</cite>:</dt><dd><p>Initializes the layer’s weights and state to handle input with the given
signature (number, shapes and dtypes of input arguments).</p>
</dd>
</dl>
</div></blockquote>
<p>A small number of layer types are combinators – they organize the computation
of their sublayers, e.g., applying their sublayers in series or in parallel.</p>
<p>All layers have the following properties, with default values implemented
in the training <cite>Layer</cite> class:</p>
<blockquote>
<div><ul class="simple">
<li><p><cite>n_in</cite>: int (default 1)</p></li>
<li><p><cite>n_out</cite>: int (default 1)</p></li>
<li><p><cite>weights</cite>: tuple (default empty – the layer has no weights)</p></li>
<li><p><cite>state</cite>: tuple (default empty – the layer has no non-parameter state)</p></li>
<li><p><cite>sublayers</cite>: tuple (default empty – the layer has no sublayers)</p></li>
</ul>
</div></blockquote>
<p>The inputs to a layer are tensors, packaged according to how many there are:</p>
<blockquote>
<div><ul class="simple">
<li><p><cite>n_in = 0</cite>: an empty tuple</p></li>
<li><p><cite>n_in = 1</cite>: one tensor (NOT wrapped in a tuple)</p></li>
<li><p><cite>n_in &gt; 1</cite>: a tuple of tensors</p></li>
</ul>
</div></blockquote>
<p>(The special treatment of the single-input case is meant to simplify the
work of layer writers; this design choice may be revisited in the future.)</p>
<p>The outputs from a layer are also tensors, packaged the same as layer inputs:</p>
<blockquote>
<div><ul class="simple">
<li><p><cite>n_out = 0</cite>: an empty tuple</p></li>
<li><p><cite>n_out = 1</cite>: the tensor (NOT wrapped in a tuple)</p></li>
<li><p><cite>n_out &gt; 1</cite>: a tuple of tensors</p></li>
</ul>
</div></blockquote>
<p>The Trax runtime maintains a data stack with which layer calls are composed.
For more complex data network architectures, possibly involving multiple data
flows, one can view each layer as a function from stack state to stack state,
where the function’s inputs are a slice from the stack, and the function’s
outputs are spliced back into the stack.</p>
<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.base.Layer.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_in</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_out</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sublayers_to_print</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.base.Layer.__init__" title="Link to this definition"></a></dt>
<dd><p>Creates a partially initialized, unconnected layer instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_in</strong> – Number of inputs expected by this layer.</p></li>
<li><p><strong>n_out</strong> – Number of outputs promised by this layer.</p></li>
<li><p><strong>name</strong> – Class-like name for this layer; for use when printing this layer.</p></li>
<li><p><strong>sublayers_to_print</strong> – Sublayers to display when printing out this layer;
if None (the default), display all sublayers.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.base.Layer.__call__">
<span class="sig-name descname"><span class="pre">__call__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rng</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.base.Layer.__call__" title="Link to this definition"></a></dt>
<dd><p>Makes layers callable; for use in tests or interactive settings.</p>
<p>This convenience method helps library users play with, test, or otherwise
probe the behavior of layers outside of a full training environment. It
presents the layer as callable function from inputs to outputs, with the
option of manually specifying weights and non-parameter state per individual
call. For convenience, weights and non-parameter state are cached per layer
instance, starting from default values of <cite>EMPTY_WEIGHTS</cite> and <cite>EMPTY_STATE</cite>,
and acquiring non-empty values either by initialization or from values
explicitly provided via the weights and state keyword arguments, in which
case the old weights will be preserved, and the state will be updated.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> – Zero or more input tensors, packaged as described in the <cite>Layer</cite> class
docstring.</p></li>
<li><p><strong>weights</strong> – Weights or <cite>None</cite>; if <cite>None</cite>, use self’s cached weights value.</p></li>
<li><p><strong>state</strong> – State or <cite>None</cite>; if <cite>None</cite>, use self’s cached state value.</p></li>
<li><p><strong>rng</strong> – Single-use random number generator (JAX PRNG key), or <cite>None</cite>;
if <cite>None</cite>, use a default computed from an integer 0 seed.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Zero or more output tensors, packaged as described in the <cite>Layer</cite> class
docstring.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.base.Layer.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.base.Layer.forward" title="Link to this definition"></a></dt>
<dd><p>Computes this layer’s output as part of a forward pass through the model.</p>
<p>A layer subclass overrides this method to define how the layer computes
outputs from inputs. If the layer depends on weights, state, or randomness
as part of the computation, the needed information can be accessed as
properties of the layer object: <cite>self.weights</cite>, <cite>self.state</cite>, and
<cite>self.rng</cite>. (See numerous examples in <cite>trax.layers.core</cite>.)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>inputs</strong> – Zero or more input tensors, packaged as described in the <cite>Layer</cite>
class docstring.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Zero or more output tensors, packaged as described in the <cite>Layer</cite> class
docstring.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.base.Layer.init_weights_and_state">
<span class="sig-name descname"><span class="pre">init_weights_and_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_signature</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.base.Layer.init_weights_and_state" title="Link to this definition"></a></dt>
<dd><p>Initializes weights and state, to handle input with the given signature.</p>
<p>A layer subclass must override this method if the layer uses weights or
state. To initialize weights, set <cite>self.weights</cite> to desired (typically
random) values. To initialize state (uncommon), set <cite>self.state</cite> to desired
starting values.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input_signature</strong> – A <cite>ShapeDtype</cite> instance (if this layer takes one input)
or a list/tuple of <cite>ShapeDtype</cite> instances.</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="trax.layers.base.Layer.has_backward">
<span class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></span><span class="sig-name descname"><span class="pre">has_backward</span></span><a class="headerlink" href="#trax.layers.base.Layer.has_backward" title="Link to this definition"></a></dt>
<dd><p>Returns <cite>True</cite> if this layer provides its own custom backward pass code.</p>
<p>A layer subclass that provides custom backward pass code (for custom
gradients) must override this method to return <cite>True</cite>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.base.Layer.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">new_state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rng</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.base.Layer.backward" title="Link to this definition"></a></dt>
<dd><p>Custom backward pass to propagate gradients in a custom way.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> – Input tensors; can be a (possibly nested) tuple.</p></li>
<li><p><strong>output</strong> – The result of running this layer on inputs.</p></li>
<li><p><strong>grad</strong> – Gradient signal computed based on subsequent layers; its structure
and shape must match output.</p></li>
<li><p><strong>weights</strong> – This layer’s weights.</p></li>
<li><p><strong>state</strong> – This layer’s state prior to the current forward pass.</p></li>
<li><p><strong>new_state</strong> – This layer’s state after the current forward pass.</p></li>
<li><p><strong>rng</strong> – Single-use random number generator (JAX PRNG key).</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The custom gradient signal for the input. Note that we need to return
a gradient for each argument of forward, so it will usually be a tuple
of signals: the gradient for inputs and weights.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.base.Layer.init">
<span class="sig-name descname"><span class="pre">init</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_signature</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rng</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_cache</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.base.Layer.init" title="Link to this definition"></a></dt>
<dd><p>Initializes weights/state of this layer and its sublayers recursively.</p>
<p>Initialization creates layer weights and state, for layers that use them.
It derives the necessary array shapes and data types from the layer’s input
signature, which is itself just shape and data type information.</p>
<p>For layers without weights or state, this method safely does nothing.</p>
<p>This method is designed to create weights/state only once for each layer
instance, even if the same layer instance occurs in multiple places in the
network. This enables weight sharing to be implemented as layer sharing.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_signature</strong> – <cite>ShapeDtype</cite> instance (if this layer takes one input)
or list/tuple of <cite>ShapeDtype</cite> instances.</p></li>
<li><p><strong>rng</strong> – Single-use random number generator (JAX PRNG key), or <cite>None</cite>;
if <cite>None</cite>, use a default computed from an integer 0 seed.</p></li>
<li><p><strong>use_cache</strong> – If <cite>True</cite>, and if this layer instance has already been
initialized elsewhere in the network, then return special marker
values – tuple <cite>(GET_WEIGHTS_FROM_CACHE, GET_STATE_FROM_CACHE)</cite>.
Else return this layer’s newly initialized weights and state.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A <cite>(weights, state)</cite> tuple.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.base.Layer.init_from_file">
<span class="sig-name descname"><span class="pre">init_from_file</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights_only</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_signature</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.base.Layer.init_from_file" title="Link to this definition"></a></dt>
<dd><p>Initializes this layer and its sublayers from a pickled checkpoint.</p>
<p>In the common case (<cite>weights_only=False</cite>), the file must be a gziped pickled
dictionary containing items with keys <cite>‘flat_weights’, `’flat_state’</cite> and
<cite>‘input_signature’</cite>, which are used to initialize this layer.
If <cite>input_signature</cite> is specified, it’s used instead of the one in the file.
If <cite>weights_only</cite> is <cite>True</cite>, the dictionary does not need to have the
<cite>‘flat_state’</cite> item and the state it not restored either.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>file_name</strong> – Name/path of the pickled weights/state file.</p></li>
<li><p><strong>weights_only</strong> – If <cite>True</cite>, initialize only the layer’s weights. Else
initialize both weights and state.</p></li>
<li><p><strong>input_signature</strong> – Input signature to be used instead of the one from file.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A <cite>(weights, state)</cite> tuple.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.base.Layer.save_to_file">
<span class="sig-name descname"><span class="pre">save_to_file</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights_only</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_signature</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.base.Layer.save_to_file" title="Link to this definition"></a></dt>
<dd><p>Saves this layer and its sublayers to a pickled checkpoint.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>file_name</strong> – Name/path of the pickled weights/state file.</p></li>
<li><p><strong>weights_only</strong> – If <cite>True</cite>, save only the layer’s weights. Else
save both weights and state.</p></li>
<li><p><strong>input_signature</strong> – Input signature to be used.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.base.Layer.flatten_tuple">
<span class="sig-name descname"><span class="pre">flatten_tuple</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.base.Layer.flatten_tuple" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="trax.layers.base.Layer.name">
<span class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></span><span class="sig-name descname"><span class="pre">name</span></span><a class="headerlink" href="#trax.layers.base.Layer.name" title="Link to this definition"></a></dt>
<dd><p>Returns the name of this layer.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="trax.layers.base.Layer.n_in">
<span class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></span><span class="sig-name descname"><span class="pre">n_in</span></span><a class="headerlink" href="#trax.layers.base.Layer.n_in" title="Link to this definition"></a></dt>
<dd><p>Returns how many tensors this layer expects as input.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="trax.layers.base.Layer.n_out">
<span class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></span><span class="sig-name descname"><span class="pre">n_out</span></span><a class="headerlink" href="#trax.layers.base.Layer.n_out" title="Link to this definition"></a></dt>
<dd><p>Returns how many tensors this layer promises as output.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="trax.layers.base.Layer.sublayers">
<span class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></span><span class="sig-name descname"><span class="pre">sublayers</span></span><a class="headerlink" href="#trax.layers.base.Layer.sublayers" title="Link to this definition"></a></dt>
<dd><p>Returns a tuple containing this layer’s sublayers; may be empty.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="trax.layers.base.Layer.weights">
<span class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></span><span class="sig-name descname"><span class="pre">weights</span></span><a class="headerlink" href="#trax.layers.base.Layer.weights" title="Link to this definition"></a></dt>
<dd><p>Returns this layer’s weights.</p>
<p>Depending on the layer, the weights can be in the form of:</p>
<blockquote>
<div><ul class="simple">
<li><p>an empty tuple</p></li>
<li><p>a tensor (ndarray)</p></li>
<li><p>a nested structure of tuples and tensors</p></li>
</ul>
</div></blockquote>
<p>If the layer has sublayers, the weights by convention will be
a tuple of length <cite>len(sublayers)</cite> containing the weights of sublayers.
Note that in this case self._weights only marks which ones are shared.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="trax.layers.base.Layer.state">
<span class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></span><span class="sig-name descname"><span class="pre">state</span></span><a class="headerlink" href="#trax.layers.base.Layer.state" title="Link to this definition"></a></dt>
<dd><p>Returns a tuple containing this layer’s state; may be empty.</p>
<p>If the layer has sublayers, the state by convention will be
a tuple of length <cite>len(sublayers)</cite> containing sublayer states.
Note that in this case self._state only marks which ones are shared.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.base.Layer.weights_and_state_signature">
<span class="sig-name descname"><span class="pre">weights_and_state_signature</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_signature</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">unsafe</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.base.Layer.weights_and_state_signature" title="Link to this definition"></a></dt>
<dd><p>Return a pair containing the signatures of weights and state.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="trax.layers.base.Layer.rng">
<span class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></span><span class="sig-name descname"><span class="pre">rng</span></span><a class="headerlink" href="#trax.layers.base.Layer.rng" title="Link to this definition"></a></dt>
<dd><p>Returns this layer’s current single-use random number generator.</p>
<p>Code that wants to training random samples on this generator must explicitly
split off new generators from it. (See, for example, the <cite>rng</cite> setter code
below.)</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.base.Layer.pure_fn">
<span class="sig-name descname"><span class="pre">pure_fn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rng</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_cache</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.base.Layer.pure_fn" title="Link to this definition"></a></dt>
<dd><p>Applies this layer as a pure function with no optional args.</p>
<p>This method exposes the layer’s computation as a pure function. This is
especially useful for JIT compilation. Do not override, use <cite>forward</cite>
instead.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> – Zero or more input tensors, packaged as described in the <cite>Layer</cite> class
docstring.</p></li>
<li><p><strong>weights</strong> – A tuple or list of trainable weights, with one element for this
layer if this layer has no sublayers, or one for each sublayer if
this layer has sublayers. If a layer (or sublayer) has no trainable
weights, the corresponding weights element is an empty tuple.</p></li>
<li><p><strong>state</strong> – Layer-specific non-parameter state that can update between batches.</p></li>
<li><p><strong>rng</strong> – Single-use random number generator (JAX PRNG key).</p></li>
<li><p><strong>use_cache</strong> – if <cite>True</cite>, cache weights and state in the layer object; used
to implement layer sharing in combinators.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A tuple of <cite>(tensors, state)</cite>. The tensors match the number (<cite>n_out</cite>)
promised by this layer, and are packaged as described in the <cite>Layer</cite>
class docstring.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.base.Layer.output_signature">
<span class="sig-name descname"><span class="pre">output_signature</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_signature</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.base.Layer.output_signature" title="Link to this definition"></a></dt>
<dd><p>Returns output signature this layer would give for <cite>input_signature</cite>.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="trax.layers.base.PureLayer">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.base.</span></span><span class="sig-name descname"><span class="pre">PureLayer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">forward_fn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_in</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_out</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'PureLayer'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.base.PureLayer" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#trax.layers.base.Layer" title="trax.layers.base.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Layer</span></code></a></p>
<p>Pure function from inputs to outputs, packaged as neural network layer.</p>
<p>The <cite>PureLayer</cite> class represents the simplest kinds of layers: layers with
no trainable weights and no randomness, hence pure functions from inputs to
outputs.</p>
<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.base.PureLayer.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">forward_fn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_in</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_out</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'PureLayer'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.base.PureLayer.__init__" title="Link to this definition"></a></dt>
<dd><p>Creates an unconnected <cite>PureLayer</cite> instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>forward_fn</strong> – Pure function from input tensors to output tensors, where
inputs and outputs are packaged as specified for <cite>forward</cite>.</p></li>
<li><p><strong>n_in</strong> – Number of inputs expected by this layer.</p></li>
<li><p><strong>n_out</strong> – Number of outputs promised by this layer.</p></li>
<li><p><strong>name</strong> – Class-like name for this layer; for use only in debugging.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.base.PureLayer.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.base.PureLayer.forward" title="Link to this definition"></a></dt>
<dd><p>Overrides <cite>Layer.forward</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>inputs</strong> – Zero or more input tensors, packaged as described in the <cite>Layer</cite>
class docstring.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Zero or more output tensors, packaged as described in the <cite>Layer</cite> class
docstring.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.base.Fn">
<span class="sig-prename descclassname"><span class="pre">trax.layers.base.</span></span><span class="sig-name descname"><span class="pre">Fn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">f</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_out</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.base.Fn" title="Link to this definition"></a></dt>
<dd><p>Returns a layer with no weights that applies the function <cite>f</cite>.</p>
<p><cite>f</cite> can take and return any number of arguments, and takes only positional
arguments – no default or keyword arguments. It often uses JAX-numpy (<cite>jnp</cite>).
The following, for example, would create a layer that takes two inputs and
returns two outputs – element-wise sums and maxima:</p>
<blockquote>
<div><p><cite>Fn(‘SumAndMax’, lambda x0, x1: (x0 + x1, jnp.maximum(x0, x1)), n_out=2)</cite></p>
</div></blockquote>
<p>The layer’s number of inputs (<cite>n_in</cite>) is automatically set to number of
positional arguments in <cite>f</cite>, but you must explicitly set the number of
outputs (<cite>n_out</cite>) whenever it’s not the default value 1.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> – Class-like name for the resulting layer; for use in debugging.</p></li>
<li><p><strong>f</strong> – Pure function from input tensors to output tensors, where each input
tensor is a separate positional arg, e.g., <cite>f(x0, x1) –&gt; x0 + x1</cite>.
Output tensors must be packaged as specified in the <cite>Layer</cite> class
docstring.</p></li>
<li><p><strong>n_out</strong> – Number of outputs promised by the layer; default value 1.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Layer executing the function <cite>f</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py exception">
<dt class="sig sig-object py" id="trax.layers.base.LayerError">
<span class="property"><span class="k"><span class="pre">exception</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.base.</span></span><span class="sig-name descname"><span class="pre">LayerError</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">layer_name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">function_name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">caller</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_signature</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">traceback_string</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.base.LayerError" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Exception</span></code></p>
<p>Exception raised in the layer stack.</p>
<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.base.LayerError.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">layer_name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">function_name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">caller</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_signature</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">traceback_string</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.base.LayerError.__init__" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="trax.layers.base.LayerError.message">
<span class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></span><span class="sig-name descname"><span class="pre">message</span></span><a class="headerlink" href="#trax.layers.base.LayerError.message" title="Link to this definition"></a></dt>
<dd><p>Assembles current layer context into an error message.</p>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.base.flatten_weights_and_state">
<span class="sig-prename descclassname"><span class="pre">trax.layers.base.</span></span><span class="sig-name descname"><span class="pre">flatten_weights_and_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">weights</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.base.flatten_weights_and_state" title="Link to this definition"></a></dt>
<dd><p>Flatten weights and state into lists, excluding empty and cached ones.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.base.unflatten_weights_and_state">
<span class="sig-prename descclassname"><span class="pre">trax.layers.base.</span></span><span class="sig-name descname"><span class="pre">unflatten_weights_and_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">flat_weights</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">flat_state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights_and_state_signature</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights_only</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.base.unflatten_weights_and_state" title="Link to this definition"></a></dt>
<dd><p>Unflatten weights and state given their signatures.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.base.np_to_file">
<span class="sig-prename descclassname"><span class="pre">trax.layers.base.</span></span><span class="sig-name descname"><span class="pre">np_to_file</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">list_of_nparrays</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">file_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">compresslevel</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.base.np_to_file" title="Link to this definition"></a></dt>
<dd><p>Save numpy arrays to file_path with gzipping and failure protection.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.base.np_from_file">
<span class="sig-prename descclassname"><span class="pre">trax.layers.base.</span></span><span class="sig-name descname"><span class="pre">np_from_file</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">compresslevel</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.base.np_from_file" title="Link to this definition"></a></dt>
<dd><p>Load numpy arrays from file_path with gzipping.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.base.to_list">
<span class="sig-prename descclassname"><span class="pre">trax.layers.base.</span></span><span class="sig-name descname"><span class="pre">to_list</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.base.to_list" title="Link to this definition"></a></dt>
<dd><p>Converts layer outputs to a nested list, for easier equality testing.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>outputs</strong> – A tensor or tuple/list of tensors coming from the forward
application of a layer. Each tensor is NumPy ndarray-like, which
complicates simple equality testing (e.g., via <cite>assertEquals</cite>):
such tensors require equality testing to use either <cite>all</cite> (all
elements match) or <cite>any</cite> (at least one element matches), which is not
directly supported in <cite>absltest</cite>.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A nested list structure containing all the output values, but now directly
testable using <cite>assertEquals</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.base.shard">
<span class="sig-prename descclassname"><span class="pre">trax.layers.base.</span></span><span class="sig-name descname"><span class="pre">shard</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensors</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_shards</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.base.shard" title="Link to this definition"></a></dt>
<dd><p>Shard tensors across n_shards.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.base.unshard_in_pmap">
<span class="sig-prename descclassname"><span class="pre">trax.layers.base.</span></span><span class="sig-name descname"><span class="pre">unshard_in_pmap</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensors</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_shards</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.base.unshard_in_pmap" title="Link to this definition"></a></dt>
<dd><p>Unshard tensors that were sharded into n_shards (call inside pmap).</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.base.unshard">
<span class="sig-prename descclassname"><span class="pre">trax.layers.base.</span></span><span class="sig-name descname"><span class="pre">unshard</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensors</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_shards</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.base.unshard" title="Link to this definition"></a></dt>
<dd><p>Unshard tensors that were sharded into n_shards (outside of pmap).</p>
</dd></dl>

</section>
<section id="module-trax.layers.combinators">
<span id="combinators"></span><h2>combinators<a class="headerlink" href="#module-trax.layers.combinators" title="Link to this heading"></a></h2>
<p>Combinators for composing layers.</p>
<dl class="py class">
<dt class="sig sig-object py" id="trax.layers.combinators.Serial">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.combinators.</span></span><span class="sig-name descname"><span class="pre">Serial</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">sublayers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sublayers_to_print</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.combinators.Serial" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#trax.layers.base.Layer" title="trax.layers.base.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Layer</span></code></a></p>
<p>Combinator that applies layers serially (by function composition).</p>
<p>This combinator is commonly used to construct deep networks, e.g., like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">mlp</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">Serial</span><span class="p">(</span>
    <span class="n">tl</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">),</span>
    <span class="n">tl</span><span class="o">.</span><span class="n">Relu</span><span class="p">(),</span>
    <span class="n">tl</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
<p>A Serial combinator uses stack semantics to manage data for its sublayers.
Each sublayer sees only the inputs it needs and returns only the outputs it
has generated. The sublayers interact via the data stack. For instance, a
sublayer k, following sublayer j, gets called with the data stack in the
state left after layer j has applied. The Serial combinator then:</p>
<blockquote>
<div><ul class="simple">
<li><p>takes n_in items off the top of the stack (n_in = k.n_in) and calls
layer k, passing those items as arguments; and</p></li>
<li><p>takes layer k’s n_out return values (n_out = k.n_out) and pushes
them onto the data stack.</p></li>
</ul>
</div></blockquote>
<p>A Serial instance with no sublayers acts as a special-case (but useful)
1-input 1-output no-op.</p>
<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.combinators.Serial.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">sublayers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sublayers_to_print</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.combinators.Serial.__init__" title="Link to this definition"></a></dt>
<dd><p>Creates a partially initialized, unconnected layer instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_in</strong> – Number of inputs expected by this layer.</p></li>
<li><p><strong>n_out</strong> – Number of outputs promised by this layer.</p></li>
<li><p><strong>name</strong> – Class-like name for this layer; for use when printing this layer.</p></li>
<li><p><strong>sublayers_to_print</strong> – Sublayers to display when printing out this layer;
if None (the default), display all sublayers.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.combinators.Serial.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">xs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.combinators.Serial.forward" title="Link to this definition"></a></dt>
<dd><p>Executes this layer as part of a forward pass through the model.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.combinators.Serial.init_weights_and_state">
<span class="sig-name descname"><span class="pre">init_weights_and_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_signature</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.combinators.Serial.init_weights_and_state" title="Link to this definition"></a></dt>
<dd><p>Initializes weights and state for inputs with the given signature.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="trax.layers.combinators.Parallel">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.combinators.</span></span><span class="sig-name descname"><span class="pre">Parallel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">sublayers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.combinators.Parallel" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#trax.layers.base.Layer" title="trax.layers.base.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Layer</span></code></a></p>
<p>Combinator that applies a list of layers in parallel to its inputs.</p>
<p>Layers in the list apply to successive spans of inputs, where the spans are
determined how many inputs each layer takes. The resulting output is the
(flattened) concatenation of the respective layer outputs.</p>
<p>For example, suppose one has three layers:</p>
<blockquote>
<div><ul class="simple">
<li><p>F: 1 input, 1 output</p></li>
<li><p>G: 3 inputs, 1 output</p></li>
<li><p>H: 2 inputs, 2 outputs (h1, h2)</p></li>
</ul>
</div></blockquote>
<p>Then Parallel(F, G, H) will take 6 inputs and give 4 outputs:</p>
<blockquote>
<div><ul class="simple">
<li><p>inputs: a, b, c, d, e, f</p></li>
<li><p>outputs: F(a), G(b, c, d), h1, h2     where h1, h2 = H(e, f)</p></li>
</ul>
</div></blockquote>
<p>As an important special case, a None argument to Parallel acts as if it takes
one argument, which it leaves unchanged. (It acts as a one-arg no-op.) For
.. rubric:: Example</p>
<p>Parallel(None, F)</p>
<p>creates a layer that passes its first input unchanged and applies F to the
following input(s).</p>
<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.combinators.Parallel.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">sublayers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.combinators.Parallel.__init__" title="Link to this definition"></a></dt>
<dd><p>The constructor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>*sublayers</strong> – A list of sublayers.</p></li>
<li><p><strong>name</strong> – Descriptive name for this layer.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A new layer in which each of the given sublayers applies to its
corresponding span of elements in the dataflow stack.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.combinators.Parallel.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.combinators.Parallel.forward" title="Link to this definition"></a></dt>
<dd><p>Executes this layer as part of a forward pass through the model.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.combinators.Parallel.init_weights_and_state">
<span class="sig-name descname"><span class="pre">init_weights_and_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_signature</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.combinators.Parallel.init_weights_and_state" title="Link to this definition"></a></dt>
<dd><p>Initializes weights and state for inputs with the given signature.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="trax.layers.combinators.Concatenate">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.combinators.</span></span><span class="sig-name descname"><span class="pre">Concatenate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_items</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.combinators.Concatenate" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#trax.layers.base.Layer" title="trax.layers.base.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Layer</span></code></a></p>
<p>Concatenates a number of tensors into a single tensor.</p>
<p>For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="n">concat3</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">Concatenate</span><span class="p">(</span><span class="n">n_items</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">concat3</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">))</span>  <span class="c1"># z = [1, 2, 3, 4, 5, 6]</span>
</pre></div>
</div>
<p>Use the <cite>axis</cite> argument to specify on which axis to concatenate the tensors.
By default it’s the last axis, <cite>axis=-1</cite>, and <cite>n_items=2</cite>.</p>
<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.combinators.Concatenate.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_items</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.combinators.Concatenate.__init__" title="Link to this definition"></a></dt>
<dd><p>Creates a partially initialized, unconnected layer instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_in</strong> – Number of inputs expected by this layer.</p></li>
<li><p><strong>n_out</strong> – Number of outputs promised by this layer.</p></li>
<li><p><strong>name</strong> – Class-like name for this layer; for use when printing this layer.</p></li>
<li><p><strong>sublayers_to_print</strong> – Sublayers to display when printing out this layer;
if None (the default), display all sublayers.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.combinators.Concatenate.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">xs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.combinators.Concatenate.forward" title="Link to this definition"></a></dt>
<dd><p>Executes this layer as part of a forward pass through the model.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="trax.layers.combinators.Split">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.combinators.</span></span><span class="sig-name descname"><span class="pre">Split</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_items</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.combinators.Split" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#trax.layers.base.Layer" title="trax.layers.base.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Layer</span></code></a></p>
<p>Splits the input into n items along an axis.</p>
<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.combinators.Split.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_items</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.combinators.Split.__init__" title="Link to this definition"></a></dt>
<dd><p>Creates a partially initialized, unconnected layer instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_in</strong> – Number of inputs expected by this layer.</p></li>
<li><p><strong>n_out</strong> – Number of outputs promised by this layer.</p></li>
<li><p><strong>name</strong> – Class-like name for this layer; for use when printing this layer.</p></li>
<li><p><strong>sublayers_to_print</strong> – Sublayers to display when printing out this layer;
if None (the default), display all sublayers.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.combinators.Split.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.combinators.Split.forward" title="Link to this definition"></a></dt>
<dd><p>Executes this layer as part of a forward pass through the model.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="trax.layers.combinators.Scan">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.combinators.</span></span><span class="sig-name descname"><span class="pre">Scan</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">layer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_carry</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">remat</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.combinators.Scan" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#trax.layers.base.Layer" title="trax.layers.base.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Layer</span></code></a></p>
<p>Applies a layer progressively/cumulatively to an axis-derived sequence.</p>
<p>Conceptually, this is a function from a list to a same-length list of partial
(cumulative) results. For instance, a list of values (<cite>[1, 2, 3, 4, 5]</cite>) can
transform to a list of cumulative sums (<cite>[1, 3, 6, 10, 15]</cite>). Functions for
the same concept are called <cite>scan</cite> in Scala, <cite>scanl</cite> in Haskell, and
<cite>accumulate*</cite> in Factor.</p>
<p>In more detail, we assume the layer takes a tuple of inputs of the following
form:</p>
<blockquote>
<div><p>(input1, …, inputN, carry1, …, carryM)</p>
</div></blockquote>
<p>and returns:</p>
<blockquote>
<div><p>(output1, …, outputK, new_carry1, …, new_carryM)</p>
</div></blockquote>
<p>The scanned version applies the layer iteratively to a tensor treating values
at the given axis as if they were a list. For example, to calculate all
sums of prefixes of a tensor, we can do this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">carry</span><span class="p">):</span>
  <span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">carry</span><span class="p">):</span>
    <span class="n">res</span> <span class="o">=</span> <span class="nb">input</span> <span class="o">+</span> <span class="n">carry</span>
    <span class="k">return</span> <span class="n">res</span><span class="p">,</span> <span class="n">res</span>  <span class="c1"># output and carry are the same</span>
  <span class="k">return</span> <span class="n">tl</span><span class="o">.</span><span class="n">Fn</span><span class="p">(</span><span class="s1">&#39;add&#39;</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">n_out</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">Scan</span><span class="p">(</span><span class="n">add</span><span class="p">)([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="mi">6</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.combinators.Scan.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">layer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_carry</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">remat</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.combinators.Scan.__init__" title="Link to this definition"></a></dt>
<dd><p>Creates a partially initialized, unconnected layer instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_in</strong> – Number of inputs expected by this layer.</p></li>
<li><p><strong>n_out</strong> – Number of outputs promised by this layer.</p></li>
<li><p><strong>name</strong> – Class-like name for this layer; for use when printing this layer.</p></li>
<li><p><strong>sublayers_to_print</strong> – Sublayers to display when printing out this layer;
if None (the default), display all sublayers.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="trax.layers.combinators.Scan.sublayer">
<span class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></span><span class="sig-name descname"><span class="pre">sublayer</span></span><a class="headerlink" href="#trax.layers.combinators.Scan.sublayer" title="Link to this definition"></a></dt>
<dd><p>Returns the unique sublayer managed by this layer.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="trax.layers.combinators.Scan.state">
<span class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></span><span class="sig-name descname"><span class="pre">state</span></span><a class="headerlink" href="#trax.layers.combinators.Scan.state" title="Link to this definition"></a></dt>
<dd><p>Returns a tuple containing this layer’s state.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.combinators.Scan.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.combinators.Scan.forward" title="Link to this definition"></a></dt>
<dd><p>Executes this layer as part of a forward pass through the model.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.combinators.Scan.init_weights_and_state">
<span class="sig-name descname"><span class="pre">init_weights_and_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_signature</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.combinators.Scan.init_weights_and_state" title="Link to this definition"></a></dt>
<dd><p>Initializes weights and state for inputs with the given signature.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="trax.layers.combinators.Cond">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.combinators.</span></span><span class="sig-name descname"><span class="pre">Cond</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cond</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">false</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.combinators.Cond" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#trax.layers.base.Layer" title="trax.layers.base.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Layer</span></code></a></p>
<p>Applies layers conditionally.</p>
<p>For parameters <cite>cond</cite>, <cite>true</cite>, and <cite>false</cite> runs the equivalent of <cite>true(y)
if cond(x) else false(y)</cite>, where <cite>x</cite> is <cite>cond.n_in</cite> elements from front of the
stack and <cite>y</cite> is the rest of the stack.
Exactly one of <cite>true</cite> and <cite>false</cite> functions is executed, so it can be used to
conditionally run long computations. The state of non-executed function is not
updated. Note that different branches may be executed on different devices
if <cite>cond</cite> returns different values on them.
By default ‘false’ function is an identity.</p>
<p><cite>cond</cite> must return exactly one element: a Boolean value.
<cite>true</cite> and <cite>false</cite> must have the same n_in, and the same n_out.</p>
<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.combinators.Cond.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cond</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">false</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.combinators.Cond.__init__" title="Link to this definition"></a></dt>
<dd><p>Creates a partially initialized, unconnected layer instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_in</strong> – Number of inputs expected by this layer.</p></li>
<li><p><strong>n_out</strong> – Number of outputs promised by this layer.</p></li>
<li><p><strong>name</strong> – Class-like name for this layer; for use when printing this layer.</p></li>
<li><p><strong>sublayers_to_print</strong> – Sublayers to display when printing out this layer;
if None (the default), display all sublayers.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.combinators.Cond.init_weights_and_state">
<span class="sig-name descname"><span class="pre">init_weights_and_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_signature</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.combinators.Cond.init_weights_and_state" title="Link to this definition"></a></dt>
<dd><p>Initializes weights and state for inputs with the given signature.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.combinators.Cond.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">xs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.combinators.Cond.forward" title="Link to this definition"></a></dt>
<dd><p>Executes this layer as part of a forward pass through the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>xs</strong> – Tensors of as required by the branches of this conditional.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Tensors resulting from running the chosen branch.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.combinators.Chunk">
<span class="sig-prename descclassname"><span class="pre">trax.layers.combinators.</span></span><span class="sig-name descname"><span class="pre">Chunk</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">layer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chunk_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pass_unchunkable</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.combinators.Chunk" title="Link to this definition"></a></dt>
<dd><p>Executes <cite>layer</cite> using batch chunks of size <cite>chunk_size</cite> to save memory.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.combinators.Branch">
<span class="sig-prename descclassname"><span class="pre">trax.layers.combinators.</span></span><span class="sig-name descname"><span class="pre">Branch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">layers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'Branch'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.combinators.Branch" title="Link to this definition"></a></dt>
<dd><p>Combinator that applies a list of layers in parallel to copies of inputs.</p>
<p>Each layer in the input list is applied to as many inputs from the stack
as it needs, and their outputs are successively combined on stack.</p>
<p>For example, suppose one has three layers:</p>
<blockquote>
<div><ul class="simple">
<li><p>F: 1 input, 1 output</p></li>
<li><p>G: 3 inputs, 1 output</p></li>
<li><p>H: 2 inputs, 2 outputs (h1, h2)</p></li>
</ul>
</div></blockquote>
<p>Then Branch(F, G, H) will take 3 inputs and give 4 outputs:</p>
<blockquote>
<div><ul class="simple">
<li><p>inputs: a, b, c</p></li>
<li><p>outputs: F(a), G(a, b, c), h1, h2    where h1, h2 = H(a, b)</p></li>
</ul>
</div></blockquote>
<p>As an important special case, a None argument to Branch acts as if it takes
one argument, which it leaves unchanged. (It acts as a one-arg no-op.)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>*layers</strong> – List of layers.</p></li>
<li><p><strong>name</strong> – Descriptive name for this layer.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A branch layer built from the given sublayers.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.combinators.Residual">
<span class="sig-prename descclassname"><span class="pre">trax.layers.combinators.</span></span><span class="sig-name descname"><span class="pre">Residual</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">layers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shortcut</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.combinators.Residual" title="Link to this definition"></a></dt>
<dd><p>Wraps a series of layers with a residual connection.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>*layers</strong> – One or more layers, to be applied in series.</p></li>
<li><p><strong>shortcut</strong> – If None (the usual case), the Residual layer computes the
element-wise sum of the stack-top input with the output of the layer
series. If specified, the <cite>shortcut</cite> layer applies to a copy of the
inputs and (elementwise) adds its output to the output from the main
layer series.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A layer representing a residual connection paired with a layer series.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.combinators.Select">
<span class="sig-prename descclassname"><span class="pre">trax.layers.combinators.</span></span><span class="sig-name descname"><span class="pre">Select</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">indices</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_in</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.combinators.Select" title="Link to this definition"></a></dt>
<dd><p>Copies, reorders, or deletes stack elements according to <cite>indices</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>indices</strong> – A list or tuple of 0-based indices to select elements relative to
the top of the stack.</p></li>
<li><p><strong>n_in</strong> – Number of input elements to pop from the stack, and replace with
those specified by <cite>indices</cite>. If not specified, its value will be
calculated as <cite>max(indices) + 1</cite>.</p></li>
<li><p><strong>name</strong> – Descriptive name for this layer.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><p>Tensors, matching the number selected (<cite>n_out = len(indices)</cite>).
Specifically:</p>
<blockquote>
<div><ul class="simple">
<li><p>n_out = 0: an empty tuple</p></li>
<li><p>n_out = 1: one tensor (NOT wrapped in a tuple)</p></li>
<li><p>n_out &gt; 1: a tuple of tensors, with n_out items</p></li>
</ul>
</div></blockquote>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.combinators.Drop">
<span class="sig-prename descclassname"><span class="pre">trax.layers.combinators.</span></span><span class="sig-name descname"><span class="pre">Drop</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.combinators.Drop" title="Link to this definition"></a></dt>
<dd><p>Drops the top stack element.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.combinators.Dup">
<span class="sig-prename descclassname"><span class="pre">trax.layers.combinators.</span></span><span class="sig-name descname"><span class="pre">Dup</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.combinators.Dup" title="Link to this definition"></a></dt>
<dd><p>Duplicates (copies) the top element on the data stack.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.combinators.Swap">
<span class="sig-prename descclassname"><span class="pre">trax.layers.combinators.</span></span><span class="sig-name descname"><span class="pre">Swap</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.combinators.Swap" title="Link to this definition"></a></dt>
<dd><p>Swaps the top two stack elements.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.combinators.SerialWithSideOutputs">
<span class="sig-prename descclassname"><span class="pre">trax.layers.combinators.</span></span><span class="sig-name descname"><span class="pre">SerialWithSideOutputs</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">layers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_side_outputs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.combinators.SerialWithSideOutputs" title="Link to this definition"></a></dt>
<dd><p>Serial layer with side outputs.</p>
<p>This layer makes it easier to manage the stack when layers have side outputs.</p>
<p>In the simplest case of layers with n_in=1, n_out=2 and with
n_side_outputs=1, this layer runs the following computation on x:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">side_outputs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">layers</span><span class="p">)):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">side_output</span> <span class="o">=</span> <span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
    <span class="n">side_outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">side_output</span><span class="p">)</span>
<span class="k">return</span> <span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="o">+</span> <span class="n">side_outputs</span>
</pre></div>
</div>
<p>In the general case of layers with variable n_in and n_out and
n_side_outputs being a list of N integers, it does the following:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">side_outputs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">layer</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">cur_stack</span><span class="p">)</span>  <span class="c1"># remove n_in from stack</span>
    <span class="n">cur_stack</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">res</span><span class="p">[:</span> <span class="n">n_side_outputs</span><span class="p">[</span><span class="n">i</span><span class="p">]])</span>  <span class="c1"># put back some on stack</span>
    <span class="n">side_outputs</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="n">n_side_outputs</span><span class="p">:])</span>
<span class="k">return</span> <span class="n">cur_stack</span> <span class="o">+</span> <span class="n">side_outputs</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>layers</strong> – a list of layers to execute</p></li>
<li><p><strong>n_side_outputs</strong> – an int or a list of ints, how many outputs of each layer
to put aside</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A layer that performs the above computation.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.combinators.FlattenList">
<span class="sig-prename descclassname"><span class="pre">trax.layers.combinators.</span></span><span class="sig-name descname"><span class="pre">FlattenList</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.combinators.FlattenList" title="Link to this definition"></a></dt>
<dd><p>Flatten lists.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.combinators.Add">
<span class="sig-prename descclassname"><span class="pre">trax.layers.combinators.</span></span><span class="sig-name descname"><span class="pre">Add</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.combinators.Add" title="Link to this definition"></a></dt>
<dd><p>Adds two tensors.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.combinators.SubtractTop">
<span class="sig-prename descclassname"><span class="pre">trax.layers.combinators.</span></span><span class="sig-name descname"><span class="pre">SubtractTop</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.combinators.SubtractTop" title="Link to this definition"></a></dt>
<dd><p>Subtracts the first tensor from the second.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.combinators.Multiply">
<span class="sig-prename descclassname"><span class="pre">trax.layers.combinators.</span></span><span class="sig-name descname"><span class="pre">Multiply</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.combinators.Multiply" title="Link to this definition"></a></dt>
<dd><p>Multiplies two tensors.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.combinators.Gate">
<span class="sig-prename descclassname"><span class="pre">trax.layers.combinators.</span></span><span class="sig-name descname"><span class="pre">Gate</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.combinators.Gate" title="Link to this definition"></a></dt>
<dd><p>Returns a gating layer on a (memory, gate, candidate) tuple.</p>
<p>Final update is memory * gate + (1 - gate) * candidate</p>
<p>This gating equation may also be referred to as Highway Network.
Highway Networks: <a class="reference external" href="https://arxiv.org/abs/1505.00387">https://arxiv.org/abs/1505.00387</a></p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="trax.layers.combinators.Cache">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.combinators.</span></span><span class="sig-name descname"><span class="pre">Cache</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">layer</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.combinators.Cache" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#trax.layers.base.Layer" title="trax.layers.base.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Layer</span></code></a></p>
<p>Applies a layer on the first run and returns the outputs on next calls.</p>
<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.combinators.Cache.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">layer</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.combinators.Cache.__init__" title="Link to this definition"></a></dt>
<dd><p>Creates a partially initialized, unconnected layer instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_in</strong> – Number of inputs expected by this layer.</p></li>
<li><p><strong>n_out</strong> – Number of outputs promised by this layer.</p></li>
<li><p><strong>name</strong> – Class-like name for this layer; for use when printing this layer.</p></li>
<li><p><strong>sublayers_to_print</strong> – Sublayers to display when printing out this layer;
if None (the default), display all sublayers.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="trax.layers.combinators.Cache.sublayer">
<span class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></span><span class="sig-name descname"><span class="pre">sublayer</span></span><a class="headerlink" href="#trax.layers.combinators.Cache.sublayer" title="Link to this definition"></a></dt>
<dd><p>Returns the unique sublayer managed by this layer.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="trax.layers.combinators.Cache.state">
<span class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></span><span class="sig-name descname"><span class="pre">state</span></span><a class="headerlink" href="#trax.layers.combinators.Cache.state" title="Link to this definition"></a></dt>
<dd><p>Returns a tuple containing this layer’s state; may be empty.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.combinators.Cache.init_weights_and_state">
<span class="sig-name descname"><span class="pre">init_weights_and_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_signature</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.combinators.Cache.init_weights_and_state" title="Link to this definition"></a></dt>
<dd><p>Initializes weights and state for inputs with the given signature.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.combinators.Cache.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.combinators.Cache.forward" title="Link to this definition"></a></dt>
<dd><p>Executes this layer as part of a forward pass through the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>inputs</strong> – Tensors required by the sublayer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Tensors resulting from running the sublayer the first time.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="trax.layers.combinators.BatchLeadingAxes">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.combinators.</span></span><span class="sig-name descname"><span class="pre">BatchLeadingAxes</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">layer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_last_axes_to_keep</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.combinators.BatchLeadingAxes" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#trax.layers.base.Layer" title="trax.layers.base.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Layer</span></code></a></p>
<p>Applies a layer after flattening all but n_last_axes_to_keep to batch.</p>
<p>This can be used to make layers accept an arbitrary number of leading
axes (dimensions) as batch. For example, a Convolution layer may normally
only operate on tensors of shape [B, W, H, C]. In this case, the layer</p>
<blockquote>
<div><p>BatchLeadingAxes(Convolution(), n_last_axes_to_keep=3)</p>
</div></blockquote>
<p>will operate on any tensor […, W, H, C] and treat the leading axes as batch.</p>
<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.combinators.BatchLeadingAxes.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">layer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_last_axes_to_keep</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.combinators.BatchLeadingAxes.__init__" title="Link to this definition"></a></dt>
<dd><p>Creates a partially initialized, unconnected layer instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_in</strong> – Number of inputs expected by this layer.</p></li>
<li><p><strong>n_out</strong> – Number of outputs promised by this layer.</p></li>
<li><p><strong>name</strong> – Class-like name for this layer; for use when printing this layer.</p></li>
<li><p><strong>sublayers_to_print</strong> – Sublayers to display when printing out this layer;
if None (the default), display all sublayers.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="trax.layers.combinators.BatchLeadingAxes.sublayer">
<span class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></span><span class="sig-name descname"><span class="pre">sublayer</span></span><a class="headerlink" href="#trax.layers.combinators.BatchLeadingAxes.sublayer" title="Link to this definition"></a></dt>
<dd><p>Returns the unique sublayer managed by this layer.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.combinators.BatchLeadingAxes.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.combinators.BatchLeadingAxes.forward" title="Link to this definition"></a></dt>
<dd><p>Executes this layer as part of a forward pass through the model.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.combinators.BatchLeadingAxes.init_weights_and_state">
<span class="sig-name descname"><span class="pre">init_weights_and_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_signature</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.combinators.BatchLeadingAxes.init_weights_and_state" title="Link to this definition"></a></dt>
<dd><p>Initializes weights and state for inputs with the given signature.</p>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.combinators.Bidirectional">
<span class="sig-prename descclassname"><span class="pre">trax.layers.combinators.</span></span><span class="sig-name descname"><span class="pre">Bidirectional</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">forward_layer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">merge_layer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">Concatenate_in2</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.combinators.Bidirectional" title="Link to this definition"></a></dt>
<dd><p>Bidirectional combinator for RNNs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>forward_layer</strong> – A layer, such as <cite>trax.layers.LSTM</cite> or <cite>trax.layers.GRU</cite>.</p></li>
<li><p><strong>axis</strong> – a time axis of the inputs. Default value is <cite>1</cite>.</p></li>
<li><p><strong>merge_layer</strong> – A combinator used to combine outputs of the forward
and backward RNNs. Default value is ‘trax.layers.Concatenate’.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<p>Bidirectional(RNN(n_units=8))</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The Bidirectional combinator for RNNs.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.combinators.inputs_from_stack">
<span class="sig-prename descclassname"><span class="pre">trax.layers.combinators.</span></span><span class="sig-name descname"><span class="pre">inputs_from_stack</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stack</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.combinators.inputs_from_stack" title="Link to this definition"></a></dt>
<dd><p>Returns n inputs from stack.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.combinators.outputs_onto_stack">
<span class="sig-prename descclassname"><span class="pre">trax.layers.combinators.</span></span><span class="sig-name descname"><span class="pre">outputs_onto_stack</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stack</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.combinators.outputs_onto_stack" title="Link to this definition"></a></dt>
<dd><p>“Returns the new stack after removing n items and pushing outputs there.</p>
</dd></dl>

</section>
<section id="module-trax.layers.convolution">
<span id="convolution"></span><h2>convolution<a class="headerlink" href="#module-trax.layers.convolution" title="Link to this heading"></a></h2>
<p>Trax convolution layers.</p>
<dl class="py class">
<dt class="sig sig-object py" id="trax.layers.convolution.Conv">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.convolution.</span></span><span class="sig-name descname"><span class="pre">Conv</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">filters</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strides</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'VALID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dimension_numbers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">('NHWC'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">'HWIO'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">'NHWC')</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias_initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">&lt;function</span> <span class="pre">RandomNormalInitializer.&lt;locals&gt;.&lt;lambda&gt;&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.convolution.Conv" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#trax.layers.base.Layer" title="trax.layers.base.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Layer</span></code></a></p>
<p>Layer constructor function for a general convolution layer.</p>
<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.convolution.Conv.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">filters</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strides</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'VALID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dimension_numbers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">('NHWC'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">'HWIO'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">'NHWC')</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias_initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">&lt;function</span> <span class="pre">RandomNormalInitializer.&lt;locals&gt;.&lt;lambda&gt;&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.convolution.Conv.__init__" title="Link to this definition"></a></dt>
<dd><p>Creates a partially initialized, unconnected layer instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_in</strong> – Number of inputs expected by this layer.</p></li>
<li><p><strong>n_out</strong> – Number of outputs promised by this layer.</p></li>
<li><p><strong>name</strong> – Class-like name for this layer; for use when printing this layer.</p></li>
<li><p><strong>sublayers_to_print</strong> – Sublayers to display when printing out this layer;
if None (the default), display all sublayers.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.convolution.Conv.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.convolution.Conv.forward" title="Link to this definition"></a></dt>
<dd><p>Computes this layer’s output as part of a forward pass through the model.</p>
<p>A layer subclass overrides this method to define how the layer computes
outputs from inputs. If the layer depends on weights, state, or randomness
as part of the computation, the needed information can be accessed as
properties of the layer object: <cite>self.weights</cite>, <cite>self.state</cite>, and
<cite>self.rng</cite>. (See numerous examples in <cite>trax.layers.core</cite>.)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>inputs</strong> – Zero or more input tensors, packaged as described in the <cite>Layer</cite>
class docstring.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Zero or more output tensors, packaged as described in the <cite>Layer</cite> class
docstring.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.convolution.Conv.init_weights_and_state">
<span class="sig-name descname"><span class="pre">init_weights_and_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_signature</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.convolution.Conv.init_weights_and_state" title="Link to this definition"></a></dt>
<dd><p>Initializes weights and state, to handle input with the given signature.</p>
<p>A layer subclass must override this method if the layer uses weights or
state. To initialize weights, set <cite>self.weights</cite> to desired (typically
random) values. To initialize state (uncommon), set <cite>self.state</cite> to desired
starting values.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input_signature</strong> – A <cite>ShapeDtype</cite> instance (if this layer takes one input)
or a list/tuple of <cite>ShapeDtype</cite> instances.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="trax.layers.convolution.CausalConv">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.convolution.</span></span><span class="sig-name descname"><span class="pre">CausalConv</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">filters</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_width</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias_initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">&lt;function</span> <span class="pre">RandomNormalInitializer.&lt;locals&gt;.&lt;lambda&gt;&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.convolution.CausalConv" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#trax.layers.convolution.Conv" title="trax.layers.convolution.Conv"><code class="xref py py-class docutils literal notranslate"><span class="pre">Conv</span></code></a></p>
<p>Causal (masked) convolution for [batch x time x depth] sequences.</p>
<p>Maintains causality along time axis. Used in language modeling tasks.</p>
<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.convolution.CausalConv.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">filters</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_width</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias_initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">&lt;function</span> <span class="pre">RandomNormalInitializer.&lt;locals&gt;.&lt;lambda&gt;&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.convolution.CausalConv.__init__" title="Link to this definition"></a></dt>
<dd><p>Creates a partially initialized, unconnected layer instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_in</strong> – Number of inputs expected by this layer.</p></li>
<li><p><strong>n_out</strong> – Number of outputs promised by this layer.</p></li>
<li><p><strong>name</strong> – Class-like name for this layer; for use when printing this layer.</p></li>
<li><p><strong>sublayers_to_print</strong> – Sublayers to display when printing out this layer;
if None (the default), display all sublayers.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.convolution.CausalConv.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.convolution.CausalConv.forward" title="Link to this definition"></a></dt>
<dd><p>Computes this layer’s output as part of a forward pass through the model.</p>
<p>A layer subclass overrides this method to define how the layer computes
outputs from inputs. If the layer depends on weights, state, or randomness
as part of the computation, the needed information can be accessed as
properties of the layer object: <cite>self.weights</cite>, <cite>self.state</cite>, and
<cite>self.rng</cite>. (See numerous examples in <cite>trax.layers.core</cite>.)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>inputs</strong> – Zero or more input tensors, packaged as described in the <cite>Layer</cite>
class docstring.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Zero or more output tensors, packaged as described in the <cite>Layer</cite> class
docstring.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.convolution.Conv1d">
<span class="sig-prename descclassname"><span class="pre">trax.layers.convolution.</span></span><span class="sig-name descname"><span class="pre">Conv1d</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">filters</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'VALID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias_initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">&lt;function</span> <span class="pre">RandomNormalInitializer.&lt;locals&gt;.&lt;lambda&gt;&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.convolution.Conv1d" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="trax.layers.convolution.CausalDepthwiseConv">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.convolution.</span></span><span class="sig-name descname"><span class="pre">CausalDepthwiseConv</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">&lt;function</span> <span class="pre">ScaledInitializer.&lt;locals&gt;.Init&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_bfloat16</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.convolution.CausalDepthwiseConv" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#trax.layers.base.Layer" title="trax.layers.base.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Layer</span></code></a></p>
<p>A causal depthwise convolution layer.</p>
<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.convolution.CausalDepthwiseConv.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">&lt;function</span> <span class="pre">ScaledInitializer.&lt;locals&gt;.Init&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_bfloat16</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.convolution.CausalDepthwiseConv.__init__" title="Link to this definition"></a></dt>
<dd><p>Returns a causal depthwise convolution layer.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.convolution.CausalDepthwiseConv.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.convolution.CausalDepthwiseConv.forward" title="Link to this definition"></a></dt>
<dd><p>Executes this layer as part of a forward pass through the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> – Tensor of same shape and dtype as the input signature used to
initialize this layer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Tensor of same shape and dtype as the input.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.convolution.CausalDepthwiseConv.init_weights_and_state">
<span class="sig-name descname"><span class="pre">init_weights_and_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_signature</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.convolution.CausalDepthwiseConv.init_weights_and_state" title="Link to this definition"></a></dt>
<dd><p>Randomly initializes this layer’s weights.</p>
</dd></dl>

</dd></dl>

</section>
<section id="module-trax.layers.core">
<span id="core"></span><h2>core<a class="headerlink" href="#module-trax.layers.core" title="Link to this heading"></a></h2>
<p>Core layer types and key functions used by various layers.</p>
<dl class="py class">
<dt class="sig sig-object py" id="trax.layers.core.Dense">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.core.</span></span><span class="sig-name descname"><span class="pre">Dense</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_units</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">&lt;function</span> <span class="pre">ScaledInitializer.&lt;locals&gt;.Init&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias_initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">&lt;function</span> <span class="pre">RandomNormalInitializer.&lt;locals&gt;.&lt;lambda&gt;&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_bfloat16</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.core.Dense" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#trax.layers.base.Layer" title="trax.layers.base.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Layer</span></code></a></p>
<p>A dense (a.k.a. fully-connected, affine) layer.</p>
<p>Dense layers are the prototypical example of a trainable layer, i.e., a layer
with trainable weights. Each node in a dense layer computes a weighted sum of
all node values from the preceding layer and adds to that sum a node-specific
bias term. The full layer computation is expressed compactly in linear
algebra as an affine map <cite>y = Wx + b</cite>, where <cite>W</cite> is a matrix and <cite>y</cite>, <cite>x</cite>,
and <cite>b</cite> are vectors. The layer is trained, or “learns”, by updating the
values in <cite>W</cite> and <cite>b</cite>.</p>
<p>Less commonly, a dense layer can omit the bias term and be a pure linear map:
<cite>y = Wx</cite>.</p>
<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.core.Dense.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_units</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">&lt;function</span> <span class="pre">ScaledInitializer.&lt;locals&gt;.Init&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias_initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">&lt;function</span> <span class="pre">RandomNormalInitializer.&lt;locals&gt;.&lt;lambda&gt;&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_bfloat16</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.core.Dense.__init__" title="Link to this definition"></a></dt>
<dd><p>Returns a dense (fully connected) layer of width <cite>n_units</cite>.</p>
<p>A dense layer maps collections of <cite>R^m</cite> vectors to <cite>R^n</cite>, where <cite>n</cite>
(<cite>= n_units</cite>) is fixed at layer creation time, and <cite>m</cite> is set at layer
initialization time.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_units</strong> – Number of nodes in the layer, also known as the width of the
layer.</p></li>
<li><p><strong>kernel_initializer</strong> – Function that creates a matrix of (random) initial
connection weights <cite>W</cite> for the layer.</p></li>
<li><p><strong>bias_initializer</strong> – Function that creates a vector of (random) initial
bias weights <cite>b</cite> for the layer.</p></li>
<li><p><strong>use_bias</strong> – If <cite>True</cite>, compute an affine map <cite>y = Wx + b</cite>; else compute
a linear map <cite>y = Wx</cite>.</p></li>
<li><p><strong>use_bfloat16</strong> – If <cite>True</cite>, use bfloat16 weights instead of the default
float32; this can save memory but may (rarely) lead to numerical issues.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.core.Dense.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.core.Dense.forward" title="Link to this definition"></a></dt>
<dd><p>Executes this layer as part of a forward pass through the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> – Tensor of same shape and dtype as the input signature used to
initialize this layer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Tensor of same shape and dtype as the input, except the final dimension
is the layer’s <cite>n_units</cite> value.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.core.Dense.init_weights_and_state">
<span class="sig-name descname"><span class="pre">init_weights_and_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_signature</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.core.Dense.init_weights_and_state" title="Link to this definition"></a></dt>
<dd><p>Randomly initializes this layer’s weights.</p>
<p>Weights are a <cite>(w, b)</cite> tuple for layers created with <cite>use_bias=True</cite> (the
default case), or a <cite>w</cite> tensor for layers created with <cite>use_bias=False</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input_signature</strong> – <cite>ShapeDtype</cite> instance characterizing the input this layer
should compute on.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="trax.layers.core.Embedding">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.core.</span></span><span class="sig-name descname"><span class="pre">Embedding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">vocab_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_feature</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_bfloat16</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">&lt;function</span> <span class="pre">ScaledInitializer.&lt;locals&gt;.Init&gt;</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.core.Embedding" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#trax.layers.base.Layer" title="trax.layers.base.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Layer</span></code></a></p>
<p>Trainable layer that maps discrete tokens/IDs to vectors.</p>
<p>Embedding layers are commonly used to map discrete data, like words in NLP,
into vectors. Here is a canonical example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">word_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>  <span class="c1"># word_ids &lt; vocab_size</span>
<span class="n">embedding_layer</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
<span class="n">embedding_layer</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">trax</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">shapes</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="n">word_ids</span><span class="p">))</span>
<span class="n">embedded</span> <span class="o">=</span> <span class="n">embedding_layer</span><span class="p">(</span><span class="n">word_ids</span><span class="p">)</span>  <span class="c1"># embedded.shape = (4, 32)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.core.Embedding.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">vocab_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_feature</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_bfloat16</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">&lt;function</span> <span class="pre">ScaledInitializer.&lt;locals&gt;.Init&gt;</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.core.Embedding.__init__" title="Link to this definition"></a></dt>
<dd><p>Returns an embedding layer with given vocabulary size and vector size.</p>
<p>The layer clips input values (token IDs) to the range <cite>[0, vocab_size)</cite>.
That is, negative token IDs all clip to <cite>0</cite> before being mapped to a
vector, and token IDs with value <cite>vocab_size</cite> or greater all clip to
<cite>vocab_size - 1</cite> before being mapped to a vector.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>vocab_size</strong> – Size of the input vocabulary. The layer will assign a unique
vector to each id in <cite>range(vocab_size)</cite>.</p></li>
<li><p><strong>d_feature</strong> – Dimensionality/depth of the output vectors.</p></li>
<li><p><strong>use_bfloat16</strong> – If <cite>True</cite>, use bfloat16 weights instead of the default
float32; this can save memory but may (rarely) lead to numerical issues.</p></li>
<li><p><strong>kernel_initializer</strong> – Function that creates (random) initial vectors for
the embedding.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.core.Embedding.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.core.Embedding.forward" title="Link to this definition"></a></dt>
<dd><p>Returns embedding vectors corresponding to input token IDs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> – Tensor of token IDs.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Tensor of embedding vectors.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.core.Embedding.init_weights_and_state">
<span class="sig-name descname"><span class="pre">init_weights_and_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_signature</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.core.Embedding.init_weights_and_state" title="Link to this definition"></a></dt>
<dd><p>Randomly initializes this layer’s weights.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="trax.layers.core.Dropout">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.core.</span></span><span class="sig-name descname"><span class="pre">Dropout</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shared_axes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.core.Dropout" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#trax.layers.base.Layer" title="trax.layers.base.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Layer</span></code></a></p>
<p>A layer that stochastically ignores a subset of inputs each training step.</p>
<p>In training, to compensate for the fraction of input values dropped (<cite>rate</cite>),
all surviving values are multiplied by <cite>1 / (1 - rate)</cite>.</p>
<p>The parameter <cite>shared_axes</cite> allows to specify a list of axes on which
the mask will be shared: we will use size 1 on those axes for dropout mask
and broadcast it. Sharing reduces randomness, but can save memory.</p>
<p>This layer is active only during training (<cite>mode=’train’</cite>). In other
circumstances it is a no-op.</p>
<p>Originally introduced in the paper “Dropout: A Simple Way to Prevent Neural
Networks from Overfitting” available under the following link:
<a class="reference external" href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf">https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf</a></p>
<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.core.Dropout.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shared_axes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.core.Dropout.__init__" title="Link to this definition"></a></dt>
<dd><p>Creates a dropout layer with the given target drop rate.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>rate</strong> – Stochastic rate (probability) for dropping an activation value
from the preceding layer (setting it to zero).</p></li>
<li><p><strong>shared_axes</strong> – List of axes on which the mask is shared.</p></li>
<li><p><strong>mode</strong> – If <cite>‘train’</cite>, this layer will perform dropout; else, it will pass
all values through unaltered.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.core.Dropout.init_weights_and_state">
<span class="sig-name descname"><span class="pre">init_weights_and_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_signature</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.core.Dropout.init_weights_and_state" title="Link to this definition"></a></dt>
<dd><p>Sets layer-specific internal state.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.core.Dropout.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.core.Dropout.forward" title="Link to this definition"></a></dt>
<dd><p>Executes this layer as part of a forward pass through the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> – Tensor of activations.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Tensor of same shape and dtype as the input.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="trax.layers.core.Weights">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.core.</span></span><span class="sig-name descname"><span class="pre">Weights</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">initializer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_bfloat16</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.core.Weights" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#trax.layers.base.Layer" title="trax.layers.base.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Layer</span></code></a></p>
<p>Learnable weights as a layer.</p>
<p>It takes no input and returns a single tensor: weights.</p>
<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.core.Weights.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">initializer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_bfloat16</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.core.Weights.__init__" title="Link to this definition"></a></dt>
<dd><p>Returns a learnable tensor of shape <cite>shape</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>initializer</strong> – Function taking shape and rng as arguments.</p></li>
<li><p><strong>shape</strong> – Shape of the learnable weights.</p></li>
<li><p><strong>use_bfloat16</strong> – If <cite>True</cite>, use bfloat16 weights instead of the default
float32; this can save memory but may (rarely) lead to numerical issues.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.core.Weights.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.core.Weights.forward" title="Link to this definition"></a></dt>
<dd><p>Executes this layer as part of a forward pass through the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> – Tensor of same shape and dtype as the input signature used to
initialize this layer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Tensor with previously specified shape and dtype.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.core.Weights.init_weights_and_state">
<span class="sig-name descname"><span class="pre">init_weights_and_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_signature</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.core.Weights.init_weights_and_state" title="Link to this definition"></a></dt>
<dd><p>Returns newly initialized weights for this layer.</p>
<p>Weights is a single  <cite>w</cite> tensor with previously specified shape.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input_signature</strong> – <cite>ShapeDtype</cite> instance characterizing the input this layer
should compute on. Unused.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.core.PrintShape">
<span class="sig-prename descclassname"><span class="pre">trax.layers.core.</span></span><span class="sig-name descname"><span class="pre">PrintShape</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_in</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">msg</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.core.PrintShape" title="Link to this definition"></a></dt>
<dd><p>Prints the shapes of <cite>n_in</cite> inputs and returns then unchanged.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="trax.layers.core.SummaryImage">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.core.</span></span><span class="sig-name descname"><span class="pre">SummaryImage</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_in</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_summaries</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">recover_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.core.SummaryImage" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#trax.layers.base.Layer" title="trax.layers.base.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Layer</span></code></a></p>
<p>A layer receiving a tensor, and adding it to TensorBoard as an image.</p>
<p>It takes an input and returns it unchanged. It stores this input as a state to
be used as a metric in TensorBoard.
It converts a tensor to a scalar by running a given aggregation function (mean
by default). On TensorBoard, results for each device will be reported
separately.</p>
<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.core.SummaryImage.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_in</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_summaries</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">recover_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.core.SummaryImage.__init__" title="Link to this definition"></a></dt>
<dd><p>Takes a tensor and returns it.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> – Name of the metric to be reported.</p></li>
<li><p><strong>n_in</strong> – Number of inputs.</p></li>
<li><p><strong>num_summaries</strong> – Number of images to show.</p></li>
<li><p><strong>recover_fn</strong> – the function for converting a tensor to a dipslayable image.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.core.SummaryImage.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.core.SummaryImage.forward" title="Link to this definition"></a></dt>
<dd><p>Executes this layer as part of a forward pass through the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> – Tensor of same shape and dtype as the input signature used to
initialize this layer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Tensor with previously specified shape and dtype.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.core.SummaryImage.init_weights_and_state">
<span class="sig-name descname"><span class="pre">init_weights_and_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_signature</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.core.SummaryImage.init_weights_and_state" title="Link to this definition"></a></dt>
<dd><p>Returns newly initialized weights for this layer.</p>
<p>Weights is a single  <cite>w</cite> tensor with previously specified shape.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input_signature</strong> – <cite>ShapeDtype</cite> instance characterizing the input this layer
should compute on. Unused.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="trax.layers.core.SummaryScalar">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.core.</span></span><span class="sig-name descname"><span class="pre">SummaryScalar</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">aggregation_fun</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">gin.configurable.np.mean</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.core.SummaryScalar" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#trax.layers.base.Layer" title="trax.layers.base.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Layer</span></code></a></p>
<p>A layer receiving a tensor, and adding it to TensorBoard as a scalar.</p>
<p>It takes an input and returns it unchanged. It stores this input as a state to
be used as a metric in TensorBoard.
It converts a tensor to a scalar by running a given aggregation function (mean
by default). On TensorBoard, results for each device will be reported
separately.</p>
<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.core.SummaryScalar.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">aggregation_fun</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">gin.configurable.np.mean</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.core.SummaryScalar.__init__" title="Link to this definition"></a></dt>
<dd><p>Takes a tensor and returns it.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> – Name of the metric to be reported.</p></li>
<li><p><strong>aggregation_fun</strong> – Aggregation function to be used.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.core.SummaryScalar.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.core.SummaryScalar.forward" title="Link to this definition"></a></dt>
<dd><p>Executes this layer as part of a forward pass through the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> – Tensor of same shape and dtype as the input signature used to
initialize this layer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Tensor with previously specified shape and dtype.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.core.SummaryScalar.init_weights_and_state">
<span class="sig-name descname"><span class="pre">init_weights_and_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_signature</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.core.SummaryScalar.init_weights_and_state" title="Link to this definition"></a></dt>
<dd><p>Returns newly initialized weights for this layer.</p>
<p>Weights is a single  <cite>w</cite> tensor with previously specified shape.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input_signature</strong> – <cite>ShapeDtype</cite> instance characterizing the input this layer
should compute on. Unused.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="trax.layers.core.RandomUniform">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.core.</span></span><span class="sig-name descname"><span class="pre">RandomUniform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">min_val</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_val</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">gin.configurable.np.float32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.core.RandomUniform" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#trax.layers.base.Layer" title="trax.layers.base.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Layer</span></code></a></p>
<p>Layer returning a tensor with random values distributed uniformly.</p>
<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.core.RandomUniform.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">min_val</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_val</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">gin.configurable.np.float32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.core.RandomUniform.__init__" title="Link to this definition"></a></dt>
<dd><p>Layer returning a tensor with random values distributed uniformly.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>min_val</strong> – Lower end of uniform distribution.</p></li>
<li><p><strong>max_val</strong> – Upper end of uniform distribution.</p></li>
<li><p><strong>shape</strong> – Shape of the tensor to return. Values are sampled independently.</p></li>
<li><p><strong>dtype</strong> – Type of value to return.</p></li>
<li><p><strong>sync</strong> – Whether to synchronise <cite>rng</cite> across devices.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.core.RandomUniform.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">xs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.core.RandomUniform.forward" title="Link to this definition"></a></dt>
<dd><p>Executes this layer as part of a forward pass through the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>xs</strong> – Unused tensors.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Random uniform tensor of the shape and type specified in constructor.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="trax.layers.core.LocallyConnected1d">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.core.</span></span><span class="sig-name descname"><span class="pre">LocallyConnected1d</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">filters</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">&lt;function</span> <span class="pre">ScaledInitializer.&lt;locals&gt;.Init&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias_initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">&lt;function</span> <span class="pre">RandomNormalInitializer.&lt;locals&gt;.&lt;lambda&gt;&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'VALID'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.core.LocallyConnected1d" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#trax.layers.base.Layer" title="trax.layers.base.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Layer</span></code></a></p>
<p>Locally-connected layer for 1D inputs.</p>
<p>The LocallyConnected1d layer applies a different set of filters to each patch
of the input. This is similar to applying a convolution layer, except that
locally-connected layer uses a different set of weights for each patch.</p>
<p>The size of patch is determined by the kernel size. The stride is currently
not modifiable and set to one. This means for the input of shape (…, L, D)
the output shape for paddings ‘SAME’ and ‘WRAP’ will be (…, L, filters) and
for padding ‘VALID’ (…, L-kernel_size+1, filters); where L is the number of
“pixels” or “steps” in the input, D is the size of the embedding.</p>
<p>Note that, since the weights for different patches are not shared, the number
of “pixels” or “steps” cannot change after calling init_weights_and_state.
This is because each “pixel” is assigned its own set of weights.</p>
<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.core.LocallyConnected1d.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">filters</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">&lt;function</span> <span class="pre">ScaledInitializer.&lt;locals&gt;.Init&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias_initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">&lt;function</span> <span class="pre">RandomNormalInitializer.&lt;locals&gt;.&lt;lambda&gt;&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'VALID'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.core.LocallyConnected1d.__init__" title="Link to this definition"></a></dt>
<dd><p>Returns a locally-connected conv-like layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>filters</strong> – Number of output filters in the convolution.</p></li>
<li><p><strong>kernel_size</strong> – A length of the convolution window. Must be an odd number.</p></li>
<li><p><strong>kernel_initializer</strong> – Function that creates a matrix of (random) initial
connection weights <cite>W</cite> for the layer.</p></li>
<li><p><strong>bias_initializer</strong> – Function that creates a vector of (random) initial
bias weights <cite>b</cite> for the layer.</p></li>
<li><p><strong>use_bias</strong> – If <cite>True</cite>, the layer uses a bias vector.</p></li>
<li><p><strong>padding</strong> – The type of padding to use; must be ‘VALID’, ‘SAME’, or ‘WRAP’.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.core.LocallyConnected1d.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.core.LocallyConnected1d.forward" title="Link to this definition"></a></dt>
<dd><p>Executes this layer as part of a forward pass through the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> – Tensor of same shape and dtype as the input signature used to
initialize this layer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Tensor of same shape and dtype as the input, except the final dimension
is the layer’s <cite>filters</cite> value, and the second to last dimension is
shrinked if ‘VALID’ padding is used with kernel_size bigger than one.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.core.LocallyConnected1d.init_weights_and_state">
<span class="sig-name descname"><span class="pre">init_weights_and_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_signature</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.core.LocallyConnected1d.init_weights_and_state" title="Link to this definition"></a></dt>
<dd><p>Randomly initializes this layer’s weights.</p>
<p>Weights are a <cite>(w, b)</cite> tuple for layers created with <cite>use_bias=True</cite> (the
default case), or a <cite>w</cite> tensor for layers created with <cite>use_bias=False</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input_signature</strong> – <cite>ShapeDtype</cite> instance characterizing the input this layer
should compute on.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.core.Flatten">
<span class="sig-prename descclassname"><span class="pre">trax.layers.core.</span></span><span class="sig-name descname"><span class="pre">Flatten</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_axes_to_keep</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.core.Flatten" title="Link to this definition"></a></dt>
<dd><p>Returns a layer that combines one or more trailing axes of a tensor.</p>
<p>Flattening keeps all the values of the input tensor, but reshapes it by
collapsing one or more trailing axes into a single axis. For example, a
<cite>Flatten(n_axes_to_keep=2)</cite> layer would map a tensor with shape
<cite>(2, 3, 5, 7, 11)</cite> to the same values with shape <cite>(2, 3, 385)</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>n_axes_to_keep</strong> – Number of leading axes to leave unchanged when reshaping;
collapse only the axes after these.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.core.LogSoftmax">
<span class="sig-prename descclassname"><span class="pre">trax.layers.core.</span></span><span class="sig-name descname"><span class="pre">LogSoftmax</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.core.LogSoftmax" title="Link to this definition"></a></dt>
<dd><p>Returns a layer that applies log softmax along one tensor axis.</p>
<p>Note that the implementation actually computes x - LogSumExp(x),
which is mathematically equal to LogSoftmax(x).</p>
<p><cite>LogSoftmax</cite> acts on a group of values and normalizes them to look like a set
of log probability values. (Probability values must be non-negative, and as
a set must sum to 1. A group of log probability values can be seen as the
natural logarithm function applied to a set of probability values.)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>axis</strong> – Axis along which values are grouped for computing log softmax.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.core.LogSumExp">
<span class="sig-prename descclassname"><span class="pre">trax.layers.core.</span></span><span class="sig-name descname"><span class="pre">LogSumExp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.core.LogSumExp" title="Link to this definition"></a></dt>
<dd><p>Returns a layer that computes log(sum(exp(x))) along one tensor axis.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>axis</strong> – Axis along which values are grouped for computing log-sum-exp.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.core.Softmax">
<span class="sig-prename descclassname"><span class="pre">trax.layers.core.</span></span><span class="sig-name descname"><span class="pre">Softmax</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.core.Softmax" title="Link to this definition"></a></dt>
<dd><p>Returns a layer that applies softmax along one tensor axis.</p>
<p><cite>Softmax</cite> acts on a group of values and normalizes them to look like a set
of probability values. (Probability values must be non-negative, and as a
set must sum to 1.)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>axis</strong> – Axis along which values are grouped for computing softmax.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.core.ToFloat">
<span class="sig-prename descclassname"><span class="pre">trax.layers.core.</span></span><span class="sig-name descname"><span class="pre">ToFloat</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.core.ToFloat" title="Link to this definition"></a></dt>
<dd><p>Returns a layer that changes the dtype of a tensor to <cite>float32</cite>.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.core.Mean">
<span class="sig-prename descclassname"><span class="pre">trax.layers.core.</span></span><span class="sig-name descname"><span class="pre">Mean</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.core.Mean" title="Link to this definition"></a></dt>
<dd><p>Returns a layer that computes mean values using one tensor axis.</p>
<p><cite>Mean</cite> uses one tensor axis to form groups of values and replaces each group
with the mean value of that group. The resulting values can either remain
in their own size 1 axis (<cite>keepdims=True</cite>), or that axis can be removed from
the overall tensor (default <cite>keepdims=False</cite>), lowering the rank of the
tensor by one.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>axis</strong> – Axis along which values are grouped for computing a mean.</p></li>
<li><p><strong>keepdims</strong> – If <cite>True</cite>, keep the resulting size 1 axis as a separate tensor
axis; else, remove that axis.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.core.Min">
<span class="sig-prename descclassname"><span class="pre">trax.layers.core.</span></span><span class="sig-name descname"><span class="pre">Min</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.core.Min" title="Link to this definition"></a></dt>
<dd><p>Returns a layer that applies min along one tensor axis.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>axis</strong> – Axis along which values are grouped for computing minimum.</p></li>
<li><p><strong>keepdims</strong> – If <cite>True</cite>, keep the resulting size 1 axis as a separate tensor
axis; else, remove that axis.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.core.Max">
<span class="sig-prename descclassname"><span class="pre">trax.layers.core.</span></span><span class="sig-name descname"><span class="pre">Max</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.core.Max" title="Link to this definition"></a></dt>
<dd><p>Returns a layer that applies max along one tensor axis.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>axis</strong> – Axis along which values are grouped for computing maximum.</p></li>
<li><p><strong>keepdims</strong> – If <cite>True</cite>, keep the resulting size 1 axis as a separate tensor
axis; else, remove that axis.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.core.Sum">
<span class="sig-prename descclassname"><span class="pre">trax.layers.core.</span></span><span class="sig-name descname"><span class="pre">Sum</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.core.Sum" title="Link to this definition"></a></dt>
<dd><p>Returns a layer that computes sums using one tensor axis.</p>
<p><cite>Sum</cite> uses one tensor axis to form groups of values and replaces each group
with the sum of that group. The resulting sum values can either remain in
their own size 1 axis (<cite>keepdims=True</cite>), or that axis can be removed from the
overall tensor (default <cite>keepdims=False</cite>), lowering the rank of the tensor by
one.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>axis</strong> – Axis along which values are grouped for computing a sum; if None,
compute sum over all elements in tensor.</p></li>
<li><p><strong>keepdims</strong> – If <cite>True</cite>, keep the resulting size 1 axis as a separate tensor
axis; else, remove that axis.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.core.ThresholdToBinary">
<span class="sig-prename descclassname"><span class="pre">trax.layers.core.</span></span><span class="sig-name descname"><span class="pre">ThresholdToBinary</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.core.ThresholdToBinary" title="Link to this definition"></a></dt>
<dd><p>Returns a layer that thresholds inputs to yield outputs in {0, 1}.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.core.ArgMax">
<span class="sig-prename descclassname"><span class="pre">trax.layers.core.</span></span><span class="sig-name descname"><span class="pre">ArgMax</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.core.ArgMax" title="Link to this definition"></a></dt>
<dd><p>Returns a layer that calculates argmax along the given axis.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.core.Negate">
<span class="sig-prename descclassname"><span class="pre">trax.layers.core.</span></span><span class="sig-name descname"><span class="pre">Negate</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.core.Negate" title="Link to this definition"></a></dt>
<dd><p>Returns a layer that computes the element-wise negation of a tensor.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.core.StopGradient">
<span class="sig-prename descclassname"><span class="pre">trax.layers.core.</span></span><span class="sig-name descname"><span class="pre">StopGradient</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.core.StopGradient" title="Link to this definition"></a></dt>
<dd><p>Returns an identity layer with a stop gradient.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.core.one_hot">
<span class="sig-prename descclassname"><span class="pre">trax.layers.core.</span></span><span class="sig-name descname"><span class="pre">one_hot</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_categories</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">gin.configurable.np.float32</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.core.one_hot" title="Link to this definition"></a></dt>
<dd><p>Makes a one-hot array (n+1 dims) from an int-categorical array (n dims).</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.core.log_softmax">
<span class="sig-prename descclassname"><span class="pre">trax.layers.core.</span></span><span class="sig-name descname"><span class="pre">log_softmax</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.core.log_softmax" title="Link to this definition"></a></dt>
<dd><p>Transforms activation vectors to log-probability vectors.</p>
<p>Log probability vectors are derived by, in effect, applying softmax to raw
activation vectors and then applying log element-wise. The actual
implementation uses a mathematically valid simplification of this.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> – An ndarray with activation vectors along the given axis.</p></li>
<li><p><strong>axis</strong> – Axis along which values are grouped for computing log softmax.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>An ndarray containing log-probability vectors derived from the raw
activation vectors in <cite>x</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.core.log_gaussian_pdf">
<span class="sig-prename descclassname"><span class="pre">trax.layers.core.</span></span><span class="sig-name descname"><span class="pre">log_gaussian_pdf</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mu</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sigma</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.core.log_gaussian_pdf" title="Link to this definition"></a></dt>
<dd><p>Returns <cite>log N(x | mu, sigma)</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> – &lt;tbd&gt;</p></li>
<li><p><strong>mu</strong> – &lt;tbd&gt;</p></li>
<li><p><strong>sigma</strong> – &lt;tbd&gt;</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.core.log_gaussian_diag_pdf">
<span class="sig-prename descclassname"><span class="pre">trax.layers.core.</span></span><span class="sig-name descname"><span class="pre">log_gaussian_diag_pdf</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mu</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">diag_sigma</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.core.log_gaussian_diag_pdf" title="Link to this definition"></a></dt>
<dd><p>Returns <cite>log N(x | mu, eye(diag_sigma))</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> – &lt;tbd&gt;</p></li>
<li><p><strong>mu</strong> – &lt;tbd&gt;</p></li>
<li><p><strong>diag_sigma</strong> – &lt;tbd&gt;</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.core.multigaussian_loss">
<span class="sig-prename descclassname"><span class="pre">trax.layers.core.</span></span><span class="sig-name descname"><span class="pre">multigaussian_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">preds</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">targets</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ngauss</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.core.multigaussian_loss" title="Link to this definition"></a></dt>
<dd><p>Returns a mixture of gaussians loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>preds</strong> – &lt;tbd&gt;</p></li>
<li><p><strong>targets</strong> – &lt;tbd&gt;</p></li>
<li><p><strong>ngauss</strong> – &lt;tbd&gt;</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.core.logsoftmax_sample">
<span class="sig-prename descclassname"><span class="pre">trax.layers.core.</span></span><span class="sig-name descname"><span class="pre">logsoftmax_sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">log_probs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">temperature</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.core.logsoftmax_sample" title="Link to this definition"></a></dt>
<dd><p>Returns a sample from a log-softmax output, with temperature.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>log_probs</strong> – Logarithms of probabilities (often coming from LogSoftmax)</p></li>
<li><p><strong>temperature</strong> – For scaling before sampling (1.0 = default, 0.0 = pick argmax)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-trax.layers.initializers">
<span id="initializers"></span><h2>initializers<a class="headerlink" href="#module-trax.layers.initializers" title="Link to this heading"></a></h2>
<p>Trax initializers.</p>
<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.initializers.InitializerFromFile">
<span class="sig-prename descclassname"><span class="pre">trax.layers.initializers.</span></span><span class="sig-name descname"><span class="pre">InitializerFromFile</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">path</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.initializers.InitializerFromFile" title="Link to this definition"></a></dt>
<dd><p>Loads parameters from .npy file.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.initializers.RandomNormalInitializer">
<span class="sig-prename descclassname"><span class="pre">trax.layers.initializers.</span></span><span class="sig-name descname"><span class="pre">RandomNormalInitializer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stddev</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.01</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.initializers.RandomNormalInitializer" title="Link to this definition"></a></dt>
<dd><p>Returns an initializer for random normal coefficients.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.initializers.RandomUniformInitializer">
<span class="sig-prename descclassname"><span class="pre">trax.layers.initializers.</span></span><span class="sig-name descname"><span class="pre">RandomUniformInitializer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">lim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.initializers.RandomUniformInitializer" title="Link to this definition"></a></dt>
<dd><p>Returns an initializer for random uniform coefficients.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.initializers.ScaledInitializer">
<span class="sig-prename descclassname"><span class="pre">trax.layers.initializers.</span></span><span class="sig-name descname"><span class="pre">ScaledInitializer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">out_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">in_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">distribution</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.initializers.ScaledInitializer" title="Link to this definition"></a></dt>
<dd><p>Returns an initializer that adjusts its scale based on weight shapes.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.initializers.GlorotNormalInitializer">
<span class="sig-prename descclassname"><span class="pre">trax.layers.initializers.</span></span><span class="sig-name descname"><span class="pre">GlorotNormalInitializer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">out_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">in_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.initializers.GlorotNormalInitializer" title="Link to this definition"></a></dt>
<dd><p>Returns an initializer for random Glorot-scaled coefficients.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.initializers.GlorotUniformInitializer">
<span class="sig-prename descclassname"><span class="pre">trax.layers.initializers.</span></span><span class="sig-name descname"><span class="pre">GlorotUniformInitializer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">out_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">in_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.initializers.GlorotUniformInitializer" title="Link to this definition"></a></dt>
<dd><p>Returns an initializer for random uniform Glorot-scaled coefficients.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.initializers.LeCunNormalInitializer">
<span class="sig-prename descclassname"><span class="pre">trax.layers.initializers.</span></span><span class="sig-name descname"><span class="pre">LeCunNormalInitializer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">out_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">in_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.initializers.LeCunNormalInitializer" title="Link to this definition"></a></dt>
<dd><p>Returns an initializer for random LeCun-scaled coefficients.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.initializers.LeCunUniformInitializer">
<span class="sig-prename descclassname"><span class="pre">trax.layers.initializers.</span></span><span class="sig-name descname"><span class="pre">LeCunUniformInitializer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">out_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">in_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.initializers.LeCunUniformInitializer" title="Link to this definition"></a></dt>
<dd><p>Returns an initializer for random uniform LeCun-scaled coefficients.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.initializers.KaimingNormalInitializer">
<span class="sig-prename descclassname"><span class="pre">trax.layers.initializers.</span></span><span class="sig-name descname"><span class="pre">KaimingNormalInitializer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">out_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">in_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">param</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.initializers.KaimingNormalInitializer" title="Link to this definition"></a></dt>
<dd><p>Returns an initializer for random Kaiming-scaled coefficients.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.initializers.KaimingUniformInitializer">
<span class="sig-prename descclassname"><span class="pre">trax.layers.initializers.</span></span><span class="sig-name descname"><span class="pre">KaimingUniformInitializer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">out_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">in_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">param</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.initializers.KaimingUniformInitializer" title="Link to this definition"></a></dt>
<dd><p>Returns an initializer for random uniform Kaiming-scaled coefficients.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.initializers.OrthogonalInitializer">
<span class="sig-prename descclassname"><span class="pre">trax.layers.initializers.</span></span><span class="sig-name descname"><span class="pre">OrthogonalInitializer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stddev</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.initializers.OrthogonalInitializer" title="Link to this definition"></a></dt>
<dd><p>Returns an orthogonal initializer.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.initializers.AtariConvInit">
<span class="sig-prename descclassname"><span class="pre">trax.layers.initializers.</span></span><span class="sig-name descname"><span class="pre">AtariConvInit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">kernel_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rng</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">gin.configurable.np.float32</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.initializers.AtariConvInit" title="Link to this definition"></a></dt>
<dd><p>The standard init for Conv laters and Atari.</p>
</dd></dl>

</section>
<section id="module-trax.layers.metrics">
<span id="metrics"></span><h2>metrics<a class="headerlink" href="#module-trax.layers.metrics" title="Link to this heading"></a></h2>
<p>Layers for computing loss functions and evaluation metrics.</p>
<p>A metric layer computes a scalar value from two or three ndarray inputs:</p>
<blockquote>
<div><ul class="simple">
<li><p>model outputs: Batch of predicted values (typically vectors).</p></li>
<li><p>targets: Batch of target values (e.g., categories or vectors).</p></li>
<li><p>weights: Float values that allow for uneven weighting of batch items,
sequence positions, or vector components when computing an overall scalar
value for the batch.</p></li>
</ul>
</div></blockquote>
<p>Most metric computations take into account the items that make up a batch. For
each item in a batch, a raw metric value is computed by comparing (item-wise)
the model output to the target value. These item-wise values are then combined
into a single scalar for the batch by a function such as sum, average, or
weighted-average. For example:</p>
<blockquote>
<div><ul class="simple">
<li><p><cite>CategoryAccuracy</cite>: Treat model output as vectors whose components
correspond to the possible categories; measure a vector as correct (value
1) if its largest component is the target category, else as incorrect
(value 0). The accuracy for the batch is then the average across vectors of
these 1’s and 0’s.</p></li>
<li><p><cite>CategoryCrossEntropy</cite>: Treat model output and target values as the source
of two probability distributions; measure the cross-entropy of the model’s
predicted distribution relative to the (assumed true) target distribution.
The scalar value for the batch is then the average of the item-wise
cross-entropy values.</p></li>
</ul>
</div></blockquote>
<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.metrics.CategoryAccuracy">
<span class="sig-prename descclassname"><span class="pre">trax.layers.metrics.</span></span><span class="sig-name descname"><span class="pre">CategoryAccuracy</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.metrics.CategoryAccuracy" title="Link to this definition"></a></dt>
<dd><p>Returns a layer that computes category prediction accuracy.</p>
<p>The layer takes two inputs:</p>
<blockquote>
<div><ul class="simple">
<li><p>A batch of activation vectors. The components in a given vector should
be mappable to a probability distribution in the following loose sense:
within a vector, a higher component value corresponds to a higher
probability, such that argmax within a vector (<code class="docutils literal notranslate"><span class="pre">axis=-1</span></code>) picks the
index (category) having the highest probablity.</p></li>
<li><p>A batch of target categories; each target is an integer in
<span class="math notranslate nohighlight">\(\{0, ..., N-1\}\)</span>.</p></li>
</ul>
</div></blockquote>
<p>The predicted category from each vector is the index of the highest-valued
vector component. The layer returns the accuracy of these predictions
averaged over the batch.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.metrics.WeightedCategoryAccuracy">
<span class="sig-prename descclassname"><span class="pre">trax.layers.metrics.</span></span><span class="sig-name descname"><span class="pre">WeightedCategoryAccuracy</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.metrics.WeightedCategoryAccuracy" title="Link to this definition"></a></dt>
<dd><p>Returns a layer that computes a weighted category prediction accuracy.</p>
<p>The layer takes three inputs:</p>
<blockquote>
<div><ul class="simple">
<li><p>A batch of activation vectors. The components in a given vector should
be mappable to a probability distribution in the following loose sense:
within a vector, a higher component value corresponds to a higher
probability, such that argmax within a vector (<code class="docutils literal notranslate"><span class="pre">axis=-1</span></code>) picks the
index (category) having the highest probablity.</p></li>
<li><p>A batch of target categories; each target is an integer in
<span class="math notranslate nohighlight">\(\{0, ..., N-1\}\)</span>, where <span class="math notranslate nohighlight">\(N\)</span> is the activation vector
depth/dimensionality.</p></li>
<li><p>A batch of weights, which matches or can be broadcast to match the shape
of the target ndarray. This arg can give uneven weighting to different
items in the batch (depending, for instance, on the item’s target
category).</p></li>
</ul>
</div></blockquote>
<p>The predicted category from each vector is the index of the highest-valued
vector component. The layer returns a weighted average accuracy of these
predictions.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.metrics.CategoryCrossEntropy">
<span class="sig-prename descclassname"><span class="pre">trax.layers.metrics.</span></span><span class="sig-name descname"><span class="pre">CategoryCrossEntropy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">label_smoothing</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.metrics.CategoryCrossEntropy" title="Link to this definition"></a></dt>
<dd><p>Returns a layer that computes cross-entropy from activations and integers.</p>
<p>The layer takes two inputs:</p>
<blockquote>
<div><ul class="simple">
<li><p>A batch of activation vectors. The components in a given vector should
be pre-softmax activations (mappable to a probability distribution via
softmax). For performance reasons, the softmax and cross-entropy
computations are combined inside the layer.</p></li>
<li><p>A batch of target categories; each target is an integer in
<span class="math notranslate nohighlight">\(\{0, ..., N-1\}\)</span>, where <span class="math notranslate nohighlight">\(N\)</span> is the activation vector
depth/dimensionality.</p></li>
</ul>
</div></blockquote>
<p>To compute cross-entropy per batch item, the layer derives probability
distributions:</p>
<blockquote>
<div><ul class="simple">
<li><p>from model output (vectors): <span class="math notranslate nohighlight">\(\ q = \text{softmax}(v)\)</span></p></li>
<li><p>from target categories (integers): <span class="math notranslate nohighlight">\(\ p = \text{one_hot}(n)\)</span> or
<span class="math notranslate nohighlight">\(p = (1-\varepsilon)\cdot\text{one_hot}(n) + \frac{\varepsilon}{N}\)</span>,
where <span class="math notranslate nohighlight">\(\varepsilon\)</span> is the label smoothing factor.</p></li>
</ul>
</div></blockquote>
<p>(The conversion of integer category targets to one-hot vectors amounts to
assigning all the probability mass to the target category.) Cross-entropy
per batch item is computed between the resulting distributions:</p>
<div class="math notranslate nohighlight">
\[\text{cross_entropy} = - \sum_{i=0}^{N-1} p_i \log q_i\]</div>
<p>The layer returns the average of these cross-entropy values over all items in
the batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>label_smoothing</strong> – Creates soft targets if provided. Must be between 0 and 1.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.metrics.WeightedCategoryCrossEntropy">
<span class="sig-prename descclassname"><span class="pre">trax.layers.metrics.</span></span><span class="sig-name descname"><span class="pre">WeightedCategoryCrossEntropy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">label_smoothing</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cutoff</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.metrics.WeightedCategoryCrossEntropy" title="Link to this definition"></a></dt>
<dd><p>Returns a layer like <code class="docutils literal notranslate"><span class="pre">CategoryCrossEntropy</span></code>, with weights as third input.</p>
<p>The layer takes three inputs:</p>
<blockquote>
<div><ul class="simple">
<li><p>A batch of activation vectors. The components in a given vector should
be pre-softmax activations (mappable to a probability distribution via
softmax). For performance reasons, the softmax and cross-entropy
computations are combined inside the layer.</p></li>
<li><p>A batch of target categories; each target is an integer in
<span class="math notranslate nohighlight">\(\{0, ..., N-1\}\)</span>, where <span class="math notranslate nohighlight">\(N\)</span> is the activation vector
depth/dimensionality.</p></li>
<li><p>A batch of weights, which matches or can be broadcast to match the shape
of the target ndarray. This arg can give uneven weighting to different
items in the batch (depending, for instance, on the item’s target
category).</p></li>
</ul>
</div></blockquote>
<p>The layer returns the weighted average of these cross-entropy values over all
items in the batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>label_smoothing</strong> – Creates soft targets if provided. Must be between 0 and 1.</p></li>
<li><p><strong>cutoff</strong> – Prevent loss lower than this cutoff (0.0 meaning none by default).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.metrics.BinaryCrossEntropy">
<span class="sig-prename descclassname"><span class="pre">trax.layers.metrics.</span></span><span class="sig-name descname"><span class="pre">BinaryCrossEntropy</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.metrics.BinaryCrossEntropy" title="Link to this definition"></a></dt>
<dd><p>Returns a layer that computes cross-entropy for binary classification.</p>
<p>The layer takes two inputs:</p>
<blockquote>
<div><ul class="simple">
<li><p>A batch of activation values; each batch item <span class="math notranslate nohighlight">\(x\)</span> is a float in
<span class="math notranslate nohighlight">\((-\infty, \infty)\)</span>.</p></li>
<li><p>A batch of binary targets; each target <span class="math notranslate nohighlight">\(t\)</span> is an integer in
<span class="math notranslate nohighlight">\(\{0, 1\}\)</span>.</p></li>
</ul>
</div></blockquote>
<p>The layer maps each activation value into the range <span class="math notranslate nohighlight">\((0, 1)\)</span>,
interpreted as the model-predicted probability that item’s category is 1:</p>
<div class="math notranslate nohighlight">
\[q = \frac 1 {1 + e^{-x}} \ \ \text{[model-predicted probability]}\]</div>
<p>and computes cross-entropy (per batch item) by treating the target category
as having probability 1:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\text{cross_entropy} = \left\{ \begin{array}{cl}
    - \log q       &amp; \text{if}\ t = 1, \\
    - \log (1 - q) &amp; \text{if}\ t = 0.
\end{array} \right.\end{split}\]</div>
<p>The layer returns the average of these cross-entropy values over all items in
the batch.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.metrics.MaskedSequenceAccuracy">
<span class="sig-prename descclassname"><span class="pre">trax.layers.metrics.</span></span><span class="sig-name descname"><span class="pre">MaskedSequenceAccuracy</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.metrics.MaskedSequenceAccuracy" title="Link to this definition"></a></dt>
<dd><p>Returns a layer that computes sequence prediction accuracy with masking.</p>
<p>This layer type is intended for variable length sequences, especially text,
represented as a batch of fixed-length sequences via padding for unused
positions.</p>
<p>The layer takes three inputs:</p>
<blockquote>
<div><ul class="simple">
<li><p>A batch of sequences of activation vectors. The components in a given
vector should be mappable to a probability distribution in the following
loose sense: within a vector, a higher component value corresponds to a
higher probability, such that argmax within a vector (<code class="docutils literal notranslate"><span class="pre">axis=-1</span></code>) picks
the index having the highest probablity. In text modeling, the index
represents a token id from a predetermined token vocabulary (or padding).</p></li>
<li><p>A batch of target integer sequences, with values in
<span class="math notranslate nohighlight">\(\{0, ..., N-1\}\)</span>, where <span class="math notranslate nohighlight">\(N\)</span> is the activation vector
depth/dimensionality. In text modeling, these sequences typically
represent token ids from a predetermined token vocabulary (or padding).</p></li>
<li><p>A batch of weights/masks, which matches or can be broadcast to match the
shape of the target ndarray. This arg is used to give weight 0 to padding
positions, which masks those positions out of the calculation. Only the
zero/non-zero distinction matters; all non-zero values are treated alike
as signaling non-masked (i.e., valid/in-use) positions.</p></li>
</ul>
</div></blockquote>
<p>The predicted integer value for each sequence position is the index of the
highest-valued component of the position’s vector. A predicted integer
sequence is judged correct if it matches the target integer sequence in all
non-zero-weighted positions. The layer returns the accuracy of predicted
sequences averaged over the batch.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.metrics.Accuracy">
<span class="sig-prename descclassname"><span class="pre">trax.layers.metrics.</span></span><span class="sig-name descname"><span class="pre">Accuracy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">classifier</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">ArgMax</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.metrics.Accuracy" title="Link to this definition"></a></dt>
<dd><p>Returns a layer that computes mean category prediction accuracy.</p>
<p>DEPRECATED; use <code class="docutils literal notranslate"><span class="pre">WeightedCategoryAccuracy</span></code> instead.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>classifier</strong> – Layer that transforms activation vectors into category
predictions.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.metrics.SequenceAccuracy">
<span class="sig-prename descclassname"><span class="pre">trax.layers.metrics.</span></span><span class="sig-name descname"><span class="pre">SequenceAccuracy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">classifier</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">ArgMax</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.metrics.SequenceAccuracy" title="Link to this definition"></a></dt>
<dd><p>Returns a layer that computes mean sequence prediction accuracy.</p>
<p>DEPRECATED; use <code class="docutils literal notranslate"><span class="pre">MaskedSequenceAccuracy</span></code> instead.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>classifier</strong> – Layer that transforms activation vectors into category
predictions.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.metrics.CrossEntropyLoss">
<span class="sig-prename descclassname"><span class="pre">trax.layers.metrics.</span></span><span class="sig-name descname"><span class="pre">CrossEntropyLoss</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.metrics.CrossEntropyLoss" title="Link to this definition"></a></dt>
<dd><p>Returns a layer that outputs multiclass prediction-target cross-entropy.</p>
<p>DEPRECATED; refactor to use <code class="docutils literal notranslate"><span class="pre">WeightedCategoryCrossEntropy</span></code> or
<code class="docutils literal notranslate"><span class="pre">CategoryCrossEntropy</span></code> instead.</p>
<p>(<code class="docutils literal notranslate"><span class="pre">CrossEntropyLoss</span></code> by itself does not compute cross-entropy. In older
code, this layer had to be preceded by <code class="docutils literal notranslate"><span class="pre">LogSoftmax</span></code>, and the two layers
together did the work of converting category information to probability
distributions and computing the cross-entropy between those distributions.
All this is now done by <code class="docutils literal notranslate"><span class="pre">WeightedCategoryCrossEntropy</span></code>.)</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.metrics.CrossEntropyLossWithLogSoftmax">
<span class="sig-prename descclassname"><span class="pre">trax.layers.metrics.</span></span><span class="sig-name descname"><span class="pre">CrossEntropyLossWithLogSoftmax</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.metrics.CrossEntropyLossWithLogSoftmax" title="Link to this definition"></a></dt>
<dd><p>Mean prediction-target cross-entropy for multiclass classification.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.metrics.BinaryCrossEntropyLoss">
<span class="sig-prename descclassname"><span class="pre">trax.layers.metrics.</span></span><span class="sig-name descname"><span class="pre">BinaryCrossEntropyLoss</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.metrics.BinaryCrossEntropyLoss" title="Link to this definition"></a></dt>
<dd><p>Returns a layer that outputs binary prediction-target cross-entropy.</p>
<p>DEPRECATED; refactor to use <code class="docutils literal notranslate"><span class="pre">BinaryCrossEntropy</span></code> instead. (The newer
<code class="docutils literal notranslate"><span class="pre">BinaryCrossEntropy</span></code> does not use weights, so refactor accordingly. Unless
and until clear motivating use cases arise, the library will not include a
binary cross-entropy function with weights.)</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.metrics.L2Loss">
<span class="sig-prename descclassname"><span class="pre">trax.layers.metrics.</span></span><span class="sig-name descname"><span class="pre">L2Loss</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.metrics.L2Loss" title="Link to this definition"></a></dt>
<dd><p>Returns a layer that computes an L2-like loss for one batch.</p>
<p>The layer takes three inputs:</p>
<blockquote>
<div><ul class="simple">
<li><p>Model output from one batch, an ndarray of float-valued elements.</p></li>
<li><p>A batch of element-wise target values, which matches the shape of the
model output.</p></li>
<li><p>A batch of weights, which matches the shape of the model output.</p></li>
</ul>
</div></blockquote>
<p>The layer returns a weighted average of element-wise squared error terms
<span class="math notranslate nohighlight">\((y_i - t_i)^2\)</span>.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.metrics.SmoothL1Loss">
<span class="sig-prename descclassname"><span class="pre">trax.layers.metrics.</span></span><span class="sig-name descname"><span class="pre">SmoothL1Loss</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.metrics.SmoothL1Loss" title="Link to this definition"></a></dt>
<dd><p>Returns a layer that computes a weighted, smoothed L1 loss for one batch.</p>
<p>The layer takes three inputs:</p>
<blockquote>
<div><ul class="simple">
<li><p>Model output from one batch, an ndarray of float-valued elements.</p></li>
<li><p>A batch of element-wise target values, which matches the shape of the
model output.</p></li>
<li><p>A batch of weights, which matches the shape of the model output.</p></li>
</ul>
</div></blockquote>
<p>The layer computes a “smooth” L1 loss (a.k.a. Huber loss), for model output
float <span class="math notranslate nohighlight">\(y_i\)</span> and target float <span class="math notranslate nohighlight">\(t_i\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\text{output} = \left\{ \begin{array}{cl}
    \frac 1 2 (y_i - t_i)^2, &amp; \text{if}\ |y_i - t_i| &lt; 1, \\
    |y_i - t_i| - \frac 1 2, &amp; \text{otherwise}.
\end{array} \right.\end{split}\]</div>
<p>The layer returns a weighted average of these element-wise values.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.metrics.MacroAveragedFScore">
<span class="sig-prename descclassname"><span class="pre">trax.layers.metrics.</span></span><span class="sig-name descname"><span class="pre">MacroAveragedFScore</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">beta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initial_category_index</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.metrics.MacroAveragedFScore" title="Link to this definition"></a></dt>
<dd><p>Returns a layer that computes a macro-averaged F-score.</p>
<p>The macro-averaged F-score summarize how well the classifier’s <cite>k</cite> predictions
align with the observed/gold instances of <cite>k</cite>. It additionally cares about
all the classes equally regardless of their size.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>beta</strong> – a parameter that determines the weight of recall in the F-score.</p></li>
<li><p><strong>initial_category_index</strong> – an index of the initial category.</p></li>
</ul>
</dd>
</dl>
<p>The layer takes two inputs:</p>
<blockquote>
<div><ul class="simple">
<li><p>Model output from one batch, an ndarray of float-valued elements.</p></li>
<li><p>A batch of element-wise target values, which matches the shape of the
model output.</p></li>
</ul>
</div></blockquote>
<p>The layer returns an macro-averaged F-score across all the classes.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.metrics.WeightedFScore">
<span class="sig-prename descclassname"><span class="pre">trax.layers.metrics.</span></span><span class="sig-name descname"><span class="pre">WeightedFScore</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">beta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initial_category_index</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.metrics.WeightedFScore" title="Link to this definition"></a></dt>
<dd><p>Returns a layer that computes a weighted F-score.</p>
<p>The weighted F-score summarize how well the classifier’s <cite>k</cite> predictions
align with the observed/gold instances of <cite>k</cite>. It additionally
weights the summary by the number of observed/gold and predicted examples
in each class.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>beta</strong> – a parameter that determines the weight of recall in the F-score.</p></li>
<li><p><strong>initial_category_index</strong> – an index of the initial category.</p></li>
</ul>
</dd>
</dl>
<p>The layer takes two inputs:</p>
<blockquote>
<div><ul class="simple">
<li><p>Model output from one batch, an ndarray of float-valued elements.</p></li>
<li><p>A batch of element-wise target values, which matches the shape of the
model output.</p></li>
</ul>
</div></blockquote>
<p>The layer returns a weighted F-score across all the classes.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.metrics.WeightedSum">
<span class="sig-prename descclassname"><span class="pre">trax.layers.metrics.</span></span><span class="sig-name descname"><span class="pre">WeightedSum</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.metrics.WeightedSum" title="Link to this definition"></a></dt>
<dd><p>Returns a layer that computes a weighted sum of the given values.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.metrics.CrossEntropySum">
<span class="sig-prename descclassname"><span class="pre">trax.layers.metrics.</span></span><span class="sig-name descname"><span class="pre">CrossEntropySum</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.metrics.CrossEntropySum" title="Link to this definition"></a></dt>
<dd><p>Sum of prediction-target cross entropies for multiclass classification.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.metrics.BinaryCrossEntropySum">
<span class="sig-prename descclassname"><span class="pre">trax.layers.metrics.</span></span><span class="sig-name descname"><span class="pre">BinaryCrossEntropySum</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.metrics.BinaryCrossEntropySum" title="Link to this definition"></a></dt>
<dd><p>Sum of prediction-target cross entropies for binary classification.</p>
</dd></dl>

</section>
<section id="module-trax.layers.normalization">
<span id="normalization"></span><h2>normalization<a class="headerlink" href="#module-trax.layers.normalization" title="Link to this heading"></a></h2>
<p>Trax normalization layers.</p>
<dl class="py class">
<dt class="sig sig-object py" id="trax.layers.normalization.BatchNorm">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.normalization.</span></span><span class="sig-name descname"><span class="pre">BatchNorm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(0,</span> <span class="pre">1,</span> <span class="pre">2)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">center</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.999</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.normalization.BatchNorm" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#trax.layers.base.Layer" title="trax.layers.base.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Layer</span></code></a></p>
<p>Layer that performs batch normalization.</p>
<p>In training, batch normalization keeps smoothed cumulative statistics across
batches of input data and modifies each new batch so that its components are
normally distributed. In eval or inference, a <cite>BatchNorm</cite> instance uses its
stored mean and variance to approximately normalize each new batch of data.</p>
<p>See <a class="reference external" href="https://arxiv.org/abs/1502.03167">https://arxiv.org/abs/1502.03167</a> for original presentation and motivation
of batch normalization).</p>
<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.normalization.BatchNorm.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(0,</span> <span class="pre">1,</span> <span class="pre">2)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">center</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.999</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.normalization.BatchNorm.__init__" title="Link to this definition"></a></dt>
<dd><p>Creates a partially initialized, unconnected layer instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_in</strong> – Number of inputs expected by this layer.</p></li>
<li><p><strong>n_out</strong> – Number of outputs promised by this layer.</p></li>
<li><p><strong>name</strong> – Class-like name for this layer; for use when printing this layer.</p></li>
<li><p><strong>sublayers_to_print</strong> – Sublayers to display when printing out this layer;
if None (the default), display all sublayers.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.normalization.BatchNorm.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.normalization.BatchNorm.forward" title="Link to this definition"></a></dt>
<dd><p>Computes batch normalization as part of a forward pass in the model.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.normalization.BatchNorm.init_weights_and_state">
<span class="sig-name descname"><span class="pre">init_weights_and_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_signature</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.normalization.BatchNorm.init_weights_and_state" title="Link to this definition"></a></dt>
<dd><p>Helper to initialize batch norm weights and state.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="trax.layers.normalization.LayerNorm">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.normalization.</span></span><span class="sig-name descname"><span class="pre">LayerNorm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">center</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-06</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.normalization.LayerNorm" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#trax.layers.base.Layer" title="trax.layers.base.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Layer</span></code></a></p>
<p>Layer normalization.</p>
<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.normalization.LayerNorm.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">center</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-06</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.normalization.LayerNorm.__init__" title="Link to this definition"></a></dt>
<dd><p>Creates a partially initialized, unconnected layer instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_in</strong> – Number of inputs expected by this layer.</p></li>
<li><p><strong>n_out</strong> – Number of outputs promised by this layer.</p></li>
<li><p><strong>name</strong> – Class-like name for this layer; for use when printing this layer.</p></li>
<li><p><strong>sublayers_to_print</strong> – Sublayers to display when printing out this layer;
if None (the default), display all sublayers.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.normalization.LayerNorm.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.normalization.LayerNorm.forward" title="Link to this definition"></a></dt>
<dd><p>Computes this layer’s output as part of a forward pass through the model.</p>
<p>A layer subclass overrides this method to define how the layer computes
outputs from inputs. If the layer depends on weights, state, or randomness
as part of the computation, the needed information can be accessed as
properties of the layer object: <cite>self.weights</cite>, <cite>self.state</cite>, and
<cite>self.rng</cite>. (See numerous examples in <cite>trax.layers.core</cite>.)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>inputs</strong> – Zero or more input tensors, packaged as described in the <cite>Layer</cite>
class docstring.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Zero or more output tensors, packaged as described in the <cite>Layer</cite> class
docstring.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.normalization.LayerNorm.init_weights_and_state">
<span class="sig-name descname"><span class="pre">init_weights_and_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_signature</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.normalization.LayerNorm.init_weights_and_state" title="Link to this definition"></a></dt>
<dd><p>Initializes weights and state, to handle input with the given signature.</p>
<p>A layer subclass must override this method if the layer uses weights or
state. To initialize weights, set <cite>self.weights</cite> to desired (typically
random) values. To initialize state (uncommon), set <cite>self.state</cite> to desired
starting values.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input_signature</strong> – A <cite>ShapeDtype</cite> instance (if this layer takes one input)
or a list/tuple of <cite>ShapeDtype</cite> instances.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="trax.layers.normalization.FilterResponseNorm">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.normalization.</span></span><span class="sig-name descname"><span class="pre">FilterResponseNorm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learn_epsilon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_epsilon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-06</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_learnt_epsilon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0001</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.normalization.FilterResponseNorm" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#trax.layers.base.Layer" title="trax.layers.base.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Layer</span></code></a></p>
<p>Filter Response Normalization layer without Threshold Linear Unit.</p>
<p>c.f. <a class="reference external" href="https://arxiv.org/pdf/1911.09737.pdf">https://arxiv.org/pdf/1911.09737.pdf</a></p>
<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.normalization.FilterResponseNorm.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learn_epsilon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_epsilon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-06</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_learnt_epsilon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0001</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.normalization.FilterResponseNorm.__init__" title="Link to this definition"></a></dt>
<dd><p>Creates a partially initialized, unconnected layer instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_in</strong> – Number of inputs expected by this layer.</p></li>
<li><p><strong>n_out</strong> – Number of outputs promised by this layer.</p></li>
<li><p><strong>name</strong> – Class-like name for this layer; for use when printing this layer.</p></li>
<li><p><strong>sublayers_to_print</strong> – Sublayers to display when printing out this layer;
if None (the default), display all sublayers.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.normalization.FilterResponseNorm.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.normalization.FilterResponseNorm.forward" title="Link to this definition"></a></dt>
<dd><p>Computes this layer’s output as part of a forward pass through the model.</p>
<p>A layer subclass overrides this method to define how the layer computes
outputs from inputs. If the layer depends on weights, state, or randomness
as part of the computation, the needed information can be accessed as
properties of the layer object: <cite>self.weights</cite>, <cite>self.state</cite>, and
<cite>self.rng</cite>. (See numerous examples in <cite>trax.layers.core</cite>.)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>inputs</strong> – Zero or more input tensors, packaged as described in the <cite>Layer</cite>
class docstring.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Zero or more output tensors, packaged as described in the <cite>Layer</cite> class
docstring.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.normalization.FilterResponseNorm.init_weights_and_state">
<span class="sig-name descname"><span class="pre">init_weights_and_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_signature</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.normalization.FilterResponseNorm.init_weights_and_state" title="Link to this definition"></a></dt>
<dd><p>Initializes weights and state, to handle input with the given signature.</p>
<p>A layer subclass must override this method if the layer uses weights or
state. To initialize weights, set <cite>self.weights</cite> to desired (typically
random) values. To initialize state (uncommon), set <cite>self.state</cite> to desired
starting values.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input_signature</strong> – A <cite>ShapeDtype</cite> instance (if this layer takes one input)
or a list/tuple of <cite>ShapeDtype</cite> instances.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-trax.layers.pooling">
<span id="pooling"></span><h2>pooling<a class="headerlink" href="#module-trax.layers.pooling" title="Link to this heading"></a></h2>
<p>Trax pooling layers.</p>
<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.pooling.MaxPool">
<span class="sig-prename descclassname"><span class="pre">trax.layers.pooling.</span></span><span class="sig-name descname"><span class="pre">MaxPool</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pool_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(2,</span> <span class="pre">2)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strides</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'VALID'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.pooling.MaxPool" title="Link to this definition"></a></dt>
<dd><p>Reduces each multi-dimensional window to the max of the window’s values.</p>
<p>Windows, as specified by <cite>pool_size</cite> and <cite>strides</cite>, involve all axes of an
n-dimensional array except the first and last: <span class="math notranslate nohighlight">\((d_1, ..., d_{n-2})\)</span>
from shape <span class="math notranslate nohighlight">\((d_0, d_1, ..., d_{n-2}, d_{n-1})\)</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pool_size</strong> – Shape of window that gets reduced to a single vector value.
If the layer inputs are <span class="math notranslate nohighlight">\(n\)</span>-dimensional arrays, then <cite>pool_size</cite>
must be a tuple of length <span class="math notranslate nohighlight">\(n-2\)</span>.</p></li>
<li><p><strong>strides</strong> – Offsets from the location of one window to the locations of
neighboring windows along each axis. If specified, must be a tuple of
the same length as <cite>pool_size</cite>. If None, then offsets of 1 along each
window axis, <span class="math notranslate nohighlight">\((1, ..., 1)\)</span>, will be used.</p></li>
<li><p><strong>padding</strong> – ‘VALID’ or ‘SAME’. If ‘VALID’, no padding is done, and only
full windows get reduced; partial windows are discarded. If ‘SAME’,
padding is added at array edges as needed to avoid partial windows
but does not otherwise affect the selection of max values.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>N-dimensional array in which each valid (or padded-valid) window position
in the input is reduced to / replaced by the max value from that window.
An output array has the same number of dimensions as its input, but has
fewer elements.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.pooling.SumPool">
<span class="sig-prename descclassname"><span class="pre">trax.layers.pooling.</span></span><span class="sig-name descname"><span class="pre">SumPool</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pool_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(2,</span> <span class="pre">2)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strides</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'VALID'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.pooling.SumPool" title="Link to this definition"></a></dt>
<dd><p>Reduces each multi-dimensional window to the sum of the window’s values.</p>
<p>Windows, as specified by <cite>pool_size</cite> and <cite>strides</cite>, involve all axes of an
n-dimensional array except the first and last: <span class="math notranslate nohighlight">\((d_1, ..., d_{n-2})\)</span>
from shape <span class="math notranslate nohighlight">\((d_0, d_1, ..., d_{n-2}, d_{n-1})\)</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pool_size</strong> – Shape of window that gets reduced to a single vector value.
If the layer inputs are <span class="math notranslate nohighlight">\(n\)</span>-dimensional arrays, then <cite>pool_size</cite>
must be a tuple of length <span class="math notranslate nohighlight">\(n-2\)</span>.</p></li>
<li><p><strong>strides</strong> – Offsets from the location of one window to the locations of
neighboring windows along each axis. If specified, must be a tuple of
the same length as <cite>pool_size</cite>. If None, then offsets of 1 along each
window axis, <span class="math notranslate nohighlight">\((1, ..., 1)\)</span>, will be used.</p></li>
<li><p><strong>padding</strong> – ‘VALID’ or ‘SAME’. If ‘VALID’, no padding is done, and only
full windows get reduced; partial windows are discarded. If ‘SAME’,
padding is added at array edges as needed to avoid partial
windows but does not otherwise affect the computation of sums.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>N-dimensional array in which each valid (or padded-valid) window position
in the input is reduced to / replaced by the sum of values in that window.
An output array has the same number of dimensions as its input, but has
fewer elements.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.pooling.AvgPool">
<span class="sig-prename descclassname"><span class="pre">trax.layers.pooling.</span></span><span class="sig-name descname"><span class="pre">AvgPool</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pool_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(2,</span> <span class="pre">2)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strides</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'VALID'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.pooling.AvgPool" title="Link to this definition"></a></dt>
<dd><p>Reduces each multi-dimensional window to the mean of the window’s values.</p>
<p>Windows, as specified by <cite>pool_size</cite> and <cite>strides</cite>, involve all axes of an
n-dimensional array except the first and last: <span class="math notranslate nohighlight">\((d_1, ..., d_{n-2})\)</span>
from shape <span class="math notranslate nohighlight">\((d_0, d_1, ..., d_{n-2}, d_{n-1})\)</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pool_size</strong> – Shape of window that gets reduced to a single vector value.
If the layer inputs are <span class="math notranslate nohighlight">\(n\)</span>-dimensional arrays, then <cite>pool_size</cite>
must be a tuple of length <span class="math notranslate nohighlight">\(n-2\)</span>.</p></li>
<li><p><strong>strides</strong> – Offsets from the location of one window to the locations of
neighboring windows along each axis. If specified, must be a tuple of
the same length as <cite>pool_size</cite>. If None, then offsets of 1 along each
window axis, <span class="math notranslate nohighlight">\((1, ..., 1)\)</span>, will be used.</p></li>
<li><p><strong>padding</strong> – ‘VALID’ or ‘SAME’. If ‘VALID’, no padding is done, and only
full windows get reduced; partial windows are discarded. If ‘SAME’,
padding is added at array edges as needed but is not counted in the
computation of averages.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>N-dimensional array in which each valid (or padded-valid) window position
in the input is reduced to / replaced by the mean of values in that window.
An output array has the same number of dimensions as its input, but has
fewer elements.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-trax.layers.reversible">
<span id="reversible"></span><h2>reversible<a class="headerlink" href="#module-trax.layers.reversible" title="Link to this heading"></a></h2>
<p>Layers that can run in reverse to compute inputs from outputs.</p>
<p>Reversible layers reduce the memory required for backpropagation-based
training, especially for <em>deep</em> networks. In a series of reversible layers,
input activations from a forward pass don’t need to be stored: they can be
reconstructed on the backward pass, layer by layer, from outputs to inputs.</p>
<p>See, e.g., [The Reversible Residual Network: Backpropagation Without Storing
Activations](<a class="reference external" href="https://arxiv.org/abs/1707.04585">https://arxiv.org/abs/1707.04585</a>) and [Reformer: The Efficient
Transformer](<a class="reference external" href="https://arxiv.org/abs/2001.04451">https://arxiv.org/abs/2001.04451</a>).</p>
<dl class="py class">
<dt class="sig sig-object py" id="trax.layers.reversible.ReversibleLayer">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.reversible.</span></span><span class="sig-name descname"><span class="pre">ReversibleLayer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_in</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_out</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sublayers_to_print</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.reversible.ReversibleLayer" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#trax.layers.base.Layer" title="trax.layers.base.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Layer</span></code></a></p>
<p>Reversible Layer.</p>
<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.reversible.ReversibleLayer.reverse">
<span class="sig-name descname"><span class="pre">reverse</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">new_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rng</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.reversible.ReversibleLayer.reverse" title="Link to this definition"></a></dt>
<dd><p>Reverse this layer: compute input given output.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.reversible.ReversibleLayer.reverse_and_grad">
<span class="sig-name descname"><span class="pre">reverse_and_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">new_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rng</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.reversible.ReversibleLayer.reverse_and_grad" title="Link to this definition"></a></dt>
<dd><p>Backward pass: computes the inverse of a layer and propagates gradients.</p>
<p>While you may choose to only implement reverse, some layers implement this
function directly as computation may be shared between reversing and
computing gradients.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output</strong> – Output activations; can be a (possibly nested) tuple.</p></li>
<li><p><strong>grad</strong> – gradient signal (cotangent) computed based on subsequent layers.
The structure and shape must match the output.</p></li>
<li><p><strong>weights</strong> – layer weights</p></li>
<li><p><strong>state</strong> – start state</p></li>
<li><p><strong>new_state</strong> – updated state computed by the forward pass</p></li>
<li><p><strong>rng</strong> – Single-use random number generator (JAX PRNG key).</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A tuple (x, (x_grad, weights_grad)), where x is the reconstructed input,
x_grad is the gradient signal for the input, and weights_grad is the
gradient signal for the weights.</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="trax.layers.reversible.ReversibleLayer.has_backward">
<span class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></span><span class="sig-name descname"><span class="pre">has_backward</span></span><a class="headerlink" href="#trax.layers.reversible.ReversibleLayer.has_backward" title="Link to this definition"></a></dt>
<dd><p>Returns <cite>True</cite> if this layer provides its own custom backward pass code.</p>
<p>A layer subclass that provides custom backward pass code (for custom
gradients) must override this method to return <cite>True</cite>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.reversible.ReversibleLayer.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">new_state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rng</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.reversible.ReversibleLayer.backward" title="Link to this definition"></a></dt>
<dd><p>Custom backward pass to propagate gradients in a custom way.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> – Input tensors; can be a (possibly nested) tuple.</p></li>
<li><p><strong>output</strong> – The result of running this layer on inputs.</p></li>
<li><p><strong>grad</strong> – Gradient signal computed based on subsequent layers; its structure
and shape must match output.</p></li>
<li><p><strong>weights</strong> – This layer’s weights.</p></li>
<li><p><strong>state</strong> – This layer’s state prior to the current forward pass.</p></li>
<li><p><strong>new_state</strong> – This layer’s state after the current forward pass.</p></li>
<li><p><strong>rng</strong> – Single-use random number generator (JAX PRNG key).</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The custom gradient signal for the input. Note that we need to return
a gradient for each argument of forward, so it will usually be a tuple
of signals: the gradient for inputs and weights.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="trax.layers.reversible.ReversibleConcatenatePair">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.reversible.</span></span><span class="sig-name descname"><span class="pre">ReversibleConcatenatePair</span></span><a class="headerlink" href="#trax.layers.reversible.ReversibleConcatenatePair" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#trax.layers.reversible.ReversibleLayer" title="trax.layers.reversible.ReversibleLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">ReversibleLayer</span></code></a></p>
<p>Maps (x, y) -&gt; ([x, y], [x, y]);  [x, y] is concatenation on last axis.</p>
<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.reversible.ReversibleConcatenatePair.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.reversible.ReversibleConcatenatePair.__init__" title="Link to this definition"></a></dt>
<dd><p>Creates a partially initialized, unconnected layer instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_in</strong> – Number of inputs expected by this layer.</p></li>
<li><p><strong>n_out</strong> – Number of outputs promised by this layer.</p></li>
<li><p><strong>name</strong> – Class-like name for this layer; for use when printing this layer.</p></li>
<li><p><strong>sublayers_to_print</strong> – Sublayers to display when printing out this layer;
if None (the default), display all sublayers.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.reversible.ReversibleConcatenatePair.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.reversible.ReversibleConcatenatePair.forward" title="Link to this definition"></a></dt>
<dd><p>Computes this layer’s output as part of a forward pass through the model.</p>
<p>A layer subclass overrides this method to define how the layer computes
outputs from inputs. If the layer depends on weights, state, or randomness
as part of the computation, the needed information can be accessed as
properties of the layer object: <cite>self.weights</cite>, <cite>self.state</cite>, and
<cite>self.rng</cite>. (See numerous examples in <cite>trax.layers.core</cite>.)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>inputs</strong> – Zero or more input tensors, packaged as described in the <cite>Layer</cite>
class docstring.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Zero or more output tensors, packaged as described in the <cite>Layer</cite> class
docstring.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.reversible.ReversibleConcatenatePair.reverse">
<span class="sig-name descname"><span class="pre">reverse</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">new_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rng</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.reversible.ReversibleConcatenatePair.reverse" title="Link to this definition"></a></dt>
<dd><p>Reverse this layer: compute input given output.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="trax.layers.reversible.ReversibleSelect">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.reversible.</span></span><span class="sig-name descname"><span class="pre">ReversibleSelect</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">indices</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_in</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.reversible.ReversibleSelect" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#trax.layers.reversible.ReversibleLayer" title="trax.layers.reversible.ReversibleLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">ReversibleLayer</span></code></a></p>
<p>Reversible version of the Select combinator.</p>
<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.reversible.ReversibleSelect.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">indices</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_in</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.reversible.ReversibleSelect.__init__" title="Link to this definition"></a></dt>
<dd><p>Creates a partially initialized, unconnected layer instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_in</strong> – Number of inputs expected by this layer.</p></li>
<li><p><strong>n_out</strong> – Number of outputs promised by this layer.</p></li>
<li><p><strong>name</strong> – Class-like name for this layer; for use when printing this layer.</p></li>
<li><p><strong>sublayers_to_print</strong> – Sublayers to display when printing out this layer;
if None (the default), display all sublayers.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.reversible.ReversibleSelect.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.reversible.ReversibleSelect.forward" title="Link to this definition"></a></dt>
<dd><p>Computes this layer’s output as part of a forward pass through the model.</p>
<p>A layer subclass overrides this method to define how the layer computes
outputs from inputs. If the layer depends on weights, state, or randomness
as part of the computation, the needed information can be accessed as
properties of the layer object: <cite>self.weights</cite>, <cite>self.state</cite>, and
<cite>self.rng</cite>. (See numerous examples in <cite>trax.layers.core</cite>.)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>inputs</strong> – Zero or more input tensors, packaged as described in the <cite>Layer</cite>
class docstring.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Zero or more output tensors, packaged as described in the <cite>Layer</cite> class
docstring.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.reversible.ReversibleSelect.reverse">
<span class="sig-name descname"><span class="pre">reverse</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">new_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rng</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.reversible.ReversibleSelect.reverse" title="Link to this definition"></a></dt>
<dd><p>Reverse this layer: compute input given output.</p>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.reversible.ReversibleSwap">
<span class="sig-prename descclassname"><span class="pre">trax.layers.reversible.</span></span><span class="sig-name descname"><span class="pre">ReversibleSwap</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.reversible.ReversibleSwap" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="trax.layers.reversible.ReversibleReshape">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.reversible.</span></span><span class="sig-name descname"><span class="pre">ReversibleReshape</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">shape1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shape2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_in</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.reversible.ReversibleReshape" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#trax.layers.reversible.ReversibleLayer" title="trax.layers.reversible.ReversibleLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">ReversibleLayer</span></code></a></p>
<p>Reversible reshaping layer.</p>
<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.reversible.ReversibleReshape.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">shape1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shape2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_in</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.reversible.ReversibleReshape.__init__" title="Link to this definition"></a></dt>
<dd><p>Creates a partially initialized, unconnected layer instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_in</strong> – Number of inputs expected by this layer.</p></li>
<li><p><strong>n_out</strong> – Number of outputs promised by this layer.</p></li>
<li><p><strong>name</strong> – Class-like name for this layer; for use when printing this layer.</p></li>
<li><p><strong>sublayers_to_print</strong> – Sublayers to display when printing out this layer;
if None (the default), display all sublayers.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.reversible.ReversibleReshape.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.reversible.ReversibleReshape.forward" title="Link to this definition"></a></dt>
<dd><p>Computes this layer’s output as part of a forward pass through the model.</p>
<p>A layer subclass overrides this method to define how the layer computes
outputs from inputs. If the layer depends on weights, state, or randomness
as part of the computation, the needed information can be accessed as
properties of the layer object: <cite>self.weights</cite>, <cite>self.state</cite>, and
<cite>self.rng</cite>. (See numerous examples in <cite>trax.layers.core</cite>.)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>inputs</strong> – Zero or more input tensors, packaged as described in the <cite>Layer</cite>
class docstring.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Zero or more output tensors, packaged as described in the <cite>Layer</cite> class
docstring.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.reversible.ReversibleReshape.reverse">
<span class="sig-name descname"><span class="pre">reverse</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">new_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rng</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.reversible.ReversibleReshape.reverse" title="Link to this definition"></a></dt>
<dd><p>Reverse this layer: compute input given output.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="trax.layers.reversible.ReversiblePrintShape">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.reversible.</span></span><span class="sig-name descname"><span class="pre">ReversiblePrintShape</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_in</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">msg</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.reversible.ReversiblePrintShape" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#trax.layers.reversible.ReversibleLayer" title="trax.layers.reversible.ReversibleLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">ReversibleLayer</span></code></a></p>
<p>Reversible PrintShape for debugging reversible serial layers.</p>
<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.reversible.ReversiblePrintShape.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_in</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">msg</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.reversible.ReversiblePrintShape.__init__" title="Link to this definition"></a></dt>
<dd><p>Creates a partially initialized, unconnected layer instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_in</strong> – Number of inputs expected by this layer.</p></li>
<li><p><strong>n_out</strong> – Number of outputs promised by this layer.</p></li>
<li><p><strong>name</strong> – Class-like name for this layer; for use when printing this layer.</p></li>
<li><p><strong>sublayers_to_print</strong> – Sublayers to display when printing out this layer;
if None (the default), display all sublayers.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.reversible.ReversiblePrintShape.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">xs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.reversible.ReversiblePrintShape.forward" title="Link to this definition"></a></dt>
<dd><p>Computes this layer’s output as part of a forward pass through the model.</p>
<p>A layer subclass overrides this method to define how the layer computes
outputs from inputs. If the layer depends on weights, state, or randomness
as part of the computation, the needed information can be accessed as
properties of the layer object: <cite>self.weights</cite>, <cite>self.state</cite>, and
<cite>self.rng</cite>. (See numerous examples in <cite>trax.layers.core</cite>.)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>inputs</strong> – Zero or more input tensors, packaged as described in the <cite>Layer</cite>
class docstring.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Zero or more output tensors, packaged as described in the <cite>Layer</cite> class
docstring.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.reversible.ReversiblePrintShape.reverse">
<span class="sig-name descname"><span class="pre">reverse</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">new_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rng</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.reversible.ReversiblePrintShape.reverse" title="Link to this definition"></a></dt>
<dd><p>Reverse this layer: compute input given output.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="trax.layers.reversible.ReversibleSerial">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.reversible.</span></span><span class="sig-name descname"><span class="pre">ReversibleSerial</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">layers</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.reversible.ReversibleSerial" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#trax.layers.reversible.ReversibleLayer" title="trax.layers.reversible.ReversibleLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">ReversibleLayer</span></code></a>, <a class="reference internal" href="#trax.layers.combinators.Serial" title="trax.layers.combinators.Serial"><code class="xref py py-class docutils literal notranslate"><span class="pre">Serial</span></code></a></p>
<p>A reversible version of tl.Serial (requires reversible sub-layers).</p>
<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.reversible.ReversibleSerial.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">layers</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.reversible.ReversibleSerial.__init__" title="Link to this definition"></a></dt>
<dd><p>Creates a partially initialized, unconnected layer instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_in</strong> – Number of inputs expected by this layer.</p></li>
<li><p><strong>n_out</strong> – Number of outputs promised by this layer.</p></li>
<li><p><strong>name</strong> – Class-like name for this layer; for use when printing this layer.</p></li>
<li><p><strong>sublayers_to_print</strong> – Sublayers to display when printing out this layer;
if None (the default), display all sublayers.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.reversible.ReversibleSerial.reverse">
<span class="sig-name descname"><span class="pre">reverse</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">new_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rng</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.reversible.ReversibleSerial.reverse" title="Link to this definition"></a></dt>
<dd><p>Reverse this layer: compute input given output.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.reversible.ReversibleSerial.reverse_and_grad">
<span class="sig-name descname"><span class="pre">reverse_and_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">new_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rng</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.reversible.ReversibleSerial.reverse_and_grad" title="Link to this definition"></a></dt>
<dd><p>Backward pass: computes the inverse of a layer and propagates gradients.</p>
<p>While you may choose to only implement reverse, some layers implement this
function directly as computation may be shared between reversing and
computing gradients.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output</strong> – Output activations; can be a (possibly nested) tuple.</p></li>
<li><p><strong>grad</strong> – gradient signal (cotangent) computed based on subsequent layers.
The structure and shape must match the output.</p></li>
<li><p><strong>weights</strong> – layer weights</p></li>
<li><p><strong>state</strong> – start state</p></li>
<li><p><strong>new_state</strong> – updated state computed by the forward pass</p></li>
<li><p><strong>rng</strong> – Single-use random number generator (JAX PRNG key).</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A tuple (x, (x_grad, weights_grad)), where x is the reconstructed input,
x_grad is the gradient signal for the input, and weights_grad is the
gradient signal for the weights.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="trax.layers.reversible.ReversibleHalfResidual">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.reversible.</span></span><span class="sig-name descname"><span class="pre">ReversibleHalfResidual</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">residual_layers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_layer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.reversible.ReversibleHalfResidual" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#trax.layers.reversible.ReversibleLayer" title="trax.layers.reversible.ReversibleLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">ReversibleLayer</span></code></a></p>
<p>Half of a RevNet-style residual that optionally performs attention.</p>
<p>When attention_layer is None, this layer has the signature</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">accumulator</span><span class="p">,</span> <span class="o">*</span><span class="n">context</span><span class="p">]</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="n">accumulator</span> <span class="o">+</span> <span class="n">f</span><span class="p">(</span><span class="n">context</span><span class="p">),</span> <span class="o">*</span><span class="n">context</span><span class="p">]</span>
</pre></div>
</div>
<p>The attention_layer must be an instance of EfficientAttentionBase or one of
its subclasses (see efficient_attention.py), or None.</p>
<p>Attention is special-cased for the following two reasons:</p>
<ul class="simple">
<li><p>LSH attention needs to save bucket assignments from the forward pass to the
backward pass, for training stability. This requires special-casing it.</p></li>
<li><p>We can call attention_layer.forward_and_or_backward to compute its output
(needed for inverting a reversible residual layer) while simultaneously
performing the backward pass. Sharing computation between these two
operations improves training speed.</p></li>
</ul>
<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.reversible.ReversibleHalfResidual.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">residual_layers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_layer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.reversible.ReversibleHalfResidual.__init__" title="Link to this definition"></a></dt>
<dd><p>Creates a partially initialized, unconnected layer instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_in</strong> – Number of inputs expected by this layer.</p></li>
<li><p><strong>n_out</strong> – Number of outputs promised by this layer.</p></li>
<li><p><strong>name</strong> – Class-like name for this layer; for use when printing this layer.</p></li>
<li><p><strong>sublayers_to_print</strong> – Sublayers to display when printing out this layer;
if None (the default), display all sublayers.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.reversible.ReversibleHalfResidual.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">xs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.reversible.ReversibleHalfResidual.forward" title="Link to this definition"></a></dt>
<dd><p>Computes this layer’s output as part of a forward pass through the model.</p>
<p>A layer subclass overrides this method to define how the layer computes
outputs from inputs. If the layer depends on weights, state, or randomness
as part of the computation, the needed information can be accessed as
properties of the layer object: <cite>self.weights</cite>, <cite>self.state</cite>, and
<cite>self.rng</cite>. (See numerous examples in <cite>trax.layers.core</cite>.)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>inputs</strong> – Zero or more input tensors, packaged as described in the <cite>Layer</cite>
class docstring.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Zero or more output tensors, packaged as described in the <cite>Layer</cite> class
docstring.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.reversible.ReversibleHalfResidual.reverse">
<span class="sig-name descname"><span class="pre">reverse</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">new_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rng</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.reversible.ReversibleHalfResidual.reverse" title="Link to this definition"></a></dt>
<dd><p>Reverse this layer: compute input given output.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.reversible.ReversibleHalfResidual.reverse_and_grad">
<span class="sig-name descname"><span class="pre">reverse_and_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ct</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">new_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rng</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.reversible.ReversibleHalfResidual.reverse_and_grad" title="Link to this definition"></a></dt>
<dd><p>Backward pass: computes the inverse of a layer and propagates gradients.</p>
<p>While you may choose to only implement reverse, some layers implement this
function directly as computation may be shared between reversing and
computing gradients.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output</strong> – Output activations; can be a (possibly nested) tuple.</p></li>
<li><p><strong>grad</strong> – gradient signal (cotangent) computed based on subsequent layers.
The structure and shape must match the output.</p></li>
<li><p><strong>weights</strong> – layer weights</p></li>
<li><p><strong>state</strong> – start state</p></li>
<li><p><strong>new_state</strong> – updated state computed by the forward pass</p></li>
<li><p><strong>rng</strong> – Single-use random number generator (JAX PRNG key).</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A tuple (x, (x_grad, weights_grad)), where x is the reconstructed input,
x_grad is the gradient signal for the input, and weights_grad is the
gradient signal for the weights.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.reversible.ReversibleHalfResidual.init_weights_and_state">
<span class="sig-name descname"><span class="pre">init_weights_and_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_signature</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.reversible.ReversibleHalfResidual.init_weights_and_state" title="Link to this definition"></a></dt>
<dd><p>Initializes weights and state, to handle input with the given signature.</p>
<p>A layer subclass must override this method if the layer uses weights or
state. To initialize weights, set <cite>self.weights</cite> to desired (typically
random) values. To initialize state (uncommon), set <cite>self.state</cite> to desired
starting values.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input_signature</strong> – A <cite>ShapeDtype</cite> instance (if this layer takes one input)
or a list/tuple of <cite>ShapeDtype</cite> instances.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-trax.layers.rnn">
<span id="rnn"></span><h2>rnn<a class="headerlink" href="#module-trax.layers.rnn" title="Link to this heading"></a></h2>
<p>Implementations of common recurrent neural network cells (RNNs).</p>
<dl class="py class">
<dt class="sig sig-object py" id="trax.layers.rnn.LSTMCell">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.rnn.</span></span><span class="sig-name descname"><span class="pre">LSTMCell</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_units</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">forget_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">&lt;function</span> <span class="pre">ScaledInitializer.&lt;locals&gt;.Init&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias_initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">&lt;function</span> <span class="pre">RandomNormalInitializer.&lt;locals&gt;.&lt;lambda&gt;&gt;</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.rnn.LSTMCell" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#trax.layers.base.Layer" title="trax.layers.base.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Layer</span></code></a></p>
<p>LSTM Cell.</p>
<p>For a nice overview of the motivation and (i, o, f) gates, see this tutorial:
<a class="reference external" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p>
<p>See this paper for a description and detailed study of all gate types:
<a class="reference external" href="https://arxiv.org/pdf/1503.04069.pdf">https://arxiv.org/pdf/1503.04069.pdf</a></p>
<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.rnn.LSTMCell.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_units</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">forget_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">&lt;function</span> <span class="pre">ScaledInitializer.&lt;locals&gt;.Init&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias_initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">&lt;function</span> <span class="pre">RandomNormalInitializer.&lt;locals&gt;.&lt;lambda&gt;&gt;</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.rnn.LSTMCell.__init__" title="Link to this definition"></a></dt>
<dd><p>Creates a partially initialized, unconnected layer instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_in</strong> – Number of inputs expected by this layer.</p></li>
<li><p><strong>n_out</strong> – Number of outputs promised by this layer.</p></li>
<li><p><strong>name</strong> – Class-like name for this layer; for use when printing this layer.</p></li>
<li><p><strong>sublayers_to_print</strong> – Sublayers to display when printing out this layer;
if None (the default), display all sublayers.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.rnn.LSTMCell.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.rnn.LSTMCell.forward" title="Link to this definition"></a></dt>
<dd><p>Computes this layer’s output as part of a forward pass through the model.</p>
<p>A layer subclass overrides this method to define how the layer computes
outputs from inputs. If the layer depends on weights, state, or randomness
as part of the computation, the needed information can be accessed as
properties of the layer object: <cite>self.weights</cite>, <cite>self.state</cite>, and
<cite>self.rng</cite>. (See numerous examples in <cite>trax.layers.core</cite>.)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>inputs</strong> – Zero or more input tensors, packaged as described in the <cite>Layer</cite>
class docstring.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Zero or more output tensors, packaged as described in the <cite>Layer</cite> class
docstring.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.rnn.LSTMCell.init_weights_and_state">
<span class="sig-name descname"><span class="pre">init_weights_and_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_signature</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.rnn.LSTMCell.init_weights_and_state" title="Link to this definition"></a></dt>
<dd><p>Initializes weights and state, to handle input with the given signature.</p>
<p>A layer subclass must override this method if the layer uses weights or
state. To initialize weights, set <cite>self.weights</cite> to desired (typically
random) values. To initialize state (uncommon), set <cite>self.state</cite> to desired
starting values.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input_signature</strong> – A <cite>ShapeDtype</cite> instance (if this layer takes one input)
or a list/tuple of <cite>ShapeDtype</cite> instances.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.rnn.MakeZeroState">
<span class="sig-prename descclassname"><span class="pre">trax.layers.rnn.</span></span><span class="sig-name descname"><span class="pre">MakeZeroState</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">depth_multiplier</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.rnn.MakeZeroState" title="Link to this definition"></a></dt>
<dd><p>Makes zeros of shape like x but removing the length (axis 1).</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.rnn.LSTM">
<span class="sig-prename descclassname"><span class="pre">trax.layers.rnn.</span></span><span class="sig-name descname"><span class="pre">LSTM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_units</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initial_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.rnn.LSTM" title="Link to this definition"></a></dt>
<dd><p>LSTM running on axis 1.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_units</strong> – <cite>n_units</cite> for the <cite>LSTMCell</cite>.</p></li>
<li><p><strong>mode</strong> – if ‘predict’ then we save the previous state for one-by-one inference.</p></li>
<li><p><strong>return_state</strong> – Boolean. Whether to return the latest status in addition to
the output. Default: False.</p></li>
<li><p><strong>initial_state</strong> – Boolean. If the state RNN (c, h) is to be obtained from the
stack. Default: False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A LSTM layer.</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="trax.layers.rnn.GRUCell">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.rnn.</span></span><span class="sig-name descname"><span class="pre">GRUCell</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_units</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">forget_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">&lt;function</span> <span class="pre">RandomUniformInitializer.&lt;locals&gt;.&lt;lambda&gt;&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias_initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">&lt;function</span> <span class="pre">RandomNormalInitializer.&lt;locals&gt;.&lt;lambda&gt;&gt;</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.rnn.GRUCell" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#trax.layers.base.Layer" title="trax.layers.base.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Layer</span></code></a></p>
<p>Builds a traditional GRU cell with dense internal transformations.</p>
<p>Gated Recurrent Unit paper: <a class="reference external" href="https://arxiv.org/abs/1412.3555">https://arxiv.org/abs/1412.3555</a></p>
<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.rnn.GRUCell.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_units</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">forget_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">&lt;function</span> <span class="pre">RandomUniformInitializer.&lt;locals&gt;.&lt;lambda&gt;&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias_initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">&lt;function</span> <span class="pre">RandomNormalInitializer.&lt;locals&gt;.&lt;lambda&gt;&gt;</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.rnn.GRUCell.__init__" title="Link to this definition"></a></dt>
<dd><p>Creates a partially initialized, unconnected layer instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_in</strong> – Number of inputs expected by this layer.</p></li>
<li><p><strong>n_out</strong> – Number of outputs promised by this layer.</p></li>
<li><p><strong>name</strong> – Class-like name for this layer; for use when printing this layer.</p></li>
<li><p><strong>sublayers_to_print</strong> – Sublayers to display when printing out this layer;
if None (the default), display all sublayers.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.rnn.GRUCell.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.rnn.GRUCell.forward" title="Link to this definition"></a></dt>
<dd><p>Computes this layer’s output as part of a forward pass through the model.</p>
<p>A layer subclass overrides this method to define how the layer computes
outputs from inputs. If the layer depends on weights, state, or randomness
as part of the computation, the needed information can be accessed as
properties of the layer object: <cite>self.weights</cite>, <cite>self.state</cite>, and
<cite>self.rng</cite>. (See numerous examples in <cite>trax.layers.core</cite>.)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>inputs</strong> – Zero or more input tensors, packaged as described in the <cite>Layer</cite>
class docstring.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Zero or more output tensors, packaged as described in the <cite>Layer</cite> class
docstring.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.rnn.GRUCell.init_weights_and_state">
<span class="sig-name descname"><span class="pre">init_weights_and_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_signature</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.rnn.GRUCell.init_weights_and_state" title="Link to this definition"></a></dt>
<dd><p>Initializes weights and state, to handle input with the given signature.</p>
<p>A layer subclass must override this method if the layer uses weights or
state. To initialize weights, set <cite>self.weights</cite> to desired (typically
random) values. To initialize state (uncommon), set <cite>self.state</cite> to desired
starting values.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input_signature</strong> – A <cite>ShapeDtype</cite> instance (if this layer takes one input)
or a list/tuple of <cite>ShapeDtype</cite> instances.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.rnn.GRU">
<span class="sig-prename descclassname"><span class="pre">trax.layers.rnn.</span></span><span class="sig-name descname"><span class="pre">GRU</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_units</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.rnn.GRU" title="Link to this definition"></a></dt>
<dd><p>GRU running on axis 1.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.rnn.ConvGRUCell">
<span class="sig-prename descclassname"><span class="pre">trax.layers.rnn.</span></span><span class="sig-name descname"><span class="pre">ConvGRUCell</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_units</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(3,</span> <span class="pre">3)</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.rnn.ConvGRUCell" title="Link to this definition"></a></dt>
<dd><p>Builds a convolutional GRU.</p>
<p>Paper: <a class="reference external" href="https://arxiv.org/abs/1511.06432">https://arxiv.org/abs/1511.06432</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_units</strong> – Number of hidden units</p></li>
<li><p><strong>kernel_size</strong> – Kernel size for convolution</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A Stax model representing a GRU cell with convolution transforms.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.rnn.GeneralGRUCell">
<span class="sig-prename descclassname"><span class="pre">trax.layers.rnn.</span></span><span class="sig-name descname"><span class="pre">GeneralGRUCell</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">candidate_transform</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">memory_transform_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gate_nonlinearity</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">&lt;function</span> <span class="pre">Sigmoid&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">candidate_nonlinearity</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">&lt;function</span> <span class="pre">Tanh&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout_rate_c</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sigmoid_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.rnn.GeneralGRUCell" title="Link to this definition"></a></dt>
<dd><p>Parametrized Gated Recurrent Unit (GRU) cell construction.</p>
<p>GRU update equations for update gate, reset gate, candidate memory, and new
state:</p>
<div class="math notranslate nohighlight">
\[\begin{split}u_t &amp;= \sigma(U' \times s_{t-1} + B') \\
r_t &amp;= \sigma(U'' \times s_{t-1} + B'') \\
c_t &amp;= \tanh(U \times (r_t \odot s_{t-1}) + B) \\
s_t &amp;= u_t \odot s_{t-1} + (1 - u_t) \odot c_t\end{split}\]</div>
<p>See <cite>combinators.Gate</cite> for details on the gating function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>candidate_transform</strong> – Transform to apply inside the Candidate branch. Applied
before nonlinearities.</p></li>
<li><p><strong>memory_transform_fn</strong> – Optional transformation on the memory before gating.</p></li>
<li><p><strong>gate_nonlinearity</strong> – Function to use as gate activation; allows trying
alternatives to <cite>Sigmoid</cite>, such as <cite>HardSigmoid</cite>.</p></li>
<li><p><strong>candidate_nonlinearity</strong> – Nonlinearity to apply after candidate branch; allows
trying alternatives to traditional <cite>Tanh</cite>, such as <cite>HardTanh</cite>.</p></li>
<li><p><strong>dropout_rate_c</strong> – Amount of dropout on the transform (c) gate. Dropout works
best in a GRU when applied exclusively to this branch.</p></li>
<li><p><strong>sigmoid_bias</strong> – Constant to add before sigmoid gates. Generally want to start
off with a positive bias.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A model representing a GRU cell with specified transforms.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.rnn.InnerSRUCell">
<span class="sig-prename descclassname"><span class="pre">trax.layers.rnn.</span></span><span class="sig-name descname"><span class="pre">InnerSRUCell</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.rnn.InnerSRUCell" title="Link to this definition"></a></dt>
<dd><p>The inner (non-parallel) computation of an SRU.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.rnn.ScanSRUCell">
<span class="sig-prename descclassname"><span class="pre">trax.layers.rnn.</span></span><span class="sig-name descname"><span class="pre">ScanSRUCell</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mode</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">monkey_patched_mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.rnn.ScanSRUCell" title="Link to this definition"></a></dt>
<dd><p>The inner (non-parallel) computation of an SRU.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.rnn.SRU">
<span class="sig-prename descclassname"><span class="pre">trax.layers.rnn.</span></span><span class="sig-name descname"><span class="pre">SRU</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_units</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.rnn.SRU" title="Link to this definition"></a></dt>
<dd><p>SRU (Simple Recurrent Unit) layer as in <a class="reference external" href="https://arxiv.org/abs/1709.02755">https://arxiv.org/abs/1709.02755</a>.</p>
<p>As defined in the paper:</p>
<div class="math notranslate nohighlight">
\[\begin{split}y_t &amp;= W x_t + B \quad \hbox{(include $B$ optionally)} \\
f_t &amp;= \sigma(Wf x_t + bf) \\
r_t &amp;= \sigma(Wr x_t + br) \\
c_t &amp;= f_t \times c_{t-1} + (1 - f_t) \times y_t \\
h_t &amp;= r_t \times \hbox{activation}(c_t) + (1 - r_t) \times x_t\end{split}\]</div>
<p>We assume the input is of shape [batch, length, depth] and recurrence
happens on the length dimension. This returns a single layer. It’s best
to use at least 2, they say in the paper, except inside a Transformer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_units</strong> – output depth of the SRU layer.</p></li>
<li><p><strong>activation</strong> – Optional activation function.</p></li>
<li><p><strong>mode</strong> – if ‘predict’ then we save the previous state for one-by-one inference</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The SRU layer.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-trax.layers.research.efficient_attention">
<span id="research-efficient-attention"></span><h2>research.efficient_attention<a class="headerlink" href="#module-trax.layers.research.efficient_attention" title="Link to this heading"></a></h2>
<p>Attention Layers optimized for efficiency (second-pass implementation).</p>
<p>The approach taken in the first round of efficient attention implementations
revealed several limitations, which this code attempts to address:</p>
<ol class="arabic simple">
<li><p>Simultaneously instantiating queries, keys, and values for all heads can
exceed the memory budget. Transformers are typically tuned such that
n_heads * d_attention_key == d_model. Since attention involves queries, keys,
AND values, the memory to store them can be ~3x the memory needed to store
the input activations. Once the O(n^2) dot-product bottleneck is removed
– as is the case in all of our efficient attention implementations – this
becomes the next critical bottleneck for scaling up Transformer models.</p></li>
<li><p>Attention masking is implemented by associating an integer (typically, the
sequence position) with each query and key vector, and defining a function
to compute attention masks from this information. The standard attention API
(attention.py) is unscalable because it instantiates O(n^2)-size attention
masks, and the previous efficient implementations (efficient_attention.py)
only supported causal masking.</p></li>
</ol>
<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.length_normalized">
<span class="sig-prename descclassname"><span class="pre">trax.layers.research.efficient_attention.</span></span><span class="sig-name descname"><span class="pre">length_normalized</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-06</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.length_normalized" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.hash_vecs">
<span class="sig-prename descclassname"><span class="pre">trax.layers.research.efficient_attention.</span></span><span class="sig-name descname"><span class="pre">hash_vecs</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">vecs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_buckets_in</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_hashes</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rng</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.hash_vecs" title="Link to this definition"></a></dt>
<dd><p>Hash vectors into buckets.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>vecs</strong> – vectors to hash, a tensor of shape [batch_size, depth]</p></li>
<li><p><strong>n_buckets_in</strong> – an int or a list of ints, number of hash buckets;
if it is a list, we do hierarchical hashing as specified by the list</p></li>
<li><p><strong>n_hashes</strong> – number of hashes</p></li>
<li><p><strong>rng</strong> – random generator to use for hashing</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A pair (buckets, n_buckets) where buckets is a tensor of shape
[n_hashes, batch_size] of integers – the hash bucket IDs, and
n_buckets is an int, the total number of hash buckets, equal to
the product of all items in n_buckets_in.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.look_adjacent">
<span class="sig-prename descclassname"><span class="pre">trax.layers.research.efficient_attention.</span></span><span class="sig-name descname"><span class="pre">look_adjacent</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_chunks_before</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_chunks_after</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.look_adjacent" title="Link to this definition"></a></dt>
<dd><p>Used to implement attention between consecutive chunks.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> – array of shape [n_chunks, chunk_len, …]</p></li>
<li><p><strong>n_chunks_before</strong> – Number of previous chunks to attend to.</p></li>
<li><p><strong>n_chunks_after</strong> – Number of subsequent chunks to attend to.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>array of shape [n_chunks, N * chunk_len, …], where
N = (1 + n_chunks_before + n_chunks_after).</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.mask_self_attention">
<span class="sig-prename descclassname"><span class="pre">trax.layers.research.efficient_attention.</span></span><span class="sig-name descname"><span class="pre">mask_self_attention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dots</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">q_info</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kv_info</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">causal</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exclude_self</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">masked</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.mask_self_attention" title="Link to this definition"></a></dt>
<dd><p>Performs masking for self-attention.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.attend">
<span class="sig-prename descclassname"><span class="pre">trax.layers.research.efficient_attention.</span></span><span class="sig-name descname"><span class="pre">attend</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">q</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">v</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">q_chunk_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kv_chunk_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_chunks_before</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_chunks_after</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">q_info</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kv_info</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rng</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.attend" title="Link to this definition"></a></dt>
<dd><p>Dot-product attention, with optional chunking and/or masking.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>q</strong> – Query vectors, shape [q_len, d_qk]</p></li>
<li><p><strong>k</strong> – Key vectors, shape [kv_len, d_qk]; or None</p></li>
<li><p><strong>v</strong> – Value vectors, shape [kv_len, d_v]</p></li>
<li><p><strong>q_chunk_len</strong> – Set to non-zero to enable chunking for query vectors</p></li>
<li><p><strong>kv_chunk_len</strong> – Set to non-zero to enable chunking for key/value vectors</p></li>
<li><p><strong>n_chunks_before</strong> – Number of adjacent previous chunks to attend to</p></li>
<li><p><strong>n_chunks_after</strong> – Number of adjacent subsequent chunks to attend to</p></li>
<li><p><strong>mask_fn</strong> – TODO(kitaev) doc</p></li>
<li><p><strong>q_info</strong> – Query-associated metadata for masking</p></li>
<li><p><strong>kv_info</strong> – Key-associated metadata for masking</p></li>
<li><p><strong>dropout</strong> – Dropout rate</p></li>
<li><p><strong>rng</strong> – RNG for dropout</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A tuple (output, dots_logsumexp). The output has shape [q_len, d_v], and
dots_logsumexp has shape [q_len]. The logsumexp of the attention
probabilities is useful for combining multiple rounds of attention (as in
LSH attention).</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.apply_broadcasted_dropout">
<span class="sig-prename descclassname"><span class="pre">trax.layers.research.efficient_attention.</span></span><span class="sig-name descname"><span class="pre">apply_broadcasted_dropout</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">vecs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout_rate</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rng</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.apply_broadcasted_dropout" title="Link to this definition"></a></dt>
<dd><p>Apply dropout, broadcasted across all but the last dimension of <cite>vecs</cite>.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.permute_via_gather">
<span class="sig-prename descclassname"><span class="pre">trax.layers.research.efficient_attention.</span></span><span class="sig-name descname"><span class="pre">permute_via_gather</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">val</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">permutation</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inverse_permutation</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.permute_via_gather" title="Link to this definition"></a></dt>
<dd><p>Permutation helper for LSH attention.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.permute_via_sort">
<span class="sig-prename descclassname"><span class="pre">trax.layers.research.efficient_attention.</span></span><span class="sig-name descname"><span class="pre">permute_via_sort</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">val</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keys</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inverse_keys</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.permute_via_sort" title="Link to this definition"></a></dt>
<dd><p>Permutation helper for LSH attention.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.EfficientAttentionBase">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.research.efficient_attention.</span></span><span class="sig-name descname"><span class="pre">EfficientAttentionBase</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_in</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_parallel_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">incremental</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">predict_mem_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">predict_drop_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_python_loop</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_reference_code</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.EfficientAttentionBase" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#trax.layers.base.Layer" title="trax.layers.base.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Layer</span></code></a></p>
<p>Base class for efficient attention.</p>
<p>This is a training class that implements memory-efficient batching for both the
forward and backward passes. Subclasses should override
<cite>create_weights_unbatched</cite>, <cite>create_state_unbatched</cite>, <cite>forward_unbatched</cite>, and
optionally <cite>incremental_forward_unbatched</cite> to define the actual attention
mechanism.</p>
<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.EfficientAttentionBase.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_in</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_parallel_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">incremental</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">predict_mem_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">predict_drop_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_python_loop</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_reference_code</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.EfficientAttentionBase.__init__" title="Link to this definition"></a></dt>
<dd><p>Constructs an EfficientAttentionBase instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_heads</strong> – Number of attention heads.</p></li>
<li><p><strong>n_in</strong> – Number of inputs to the layer (default 1).</p></li>
<li><p><strong>n_parallel_heads</strong> – <p>Number of attention heads to compute in parallel.</p>
<ul>
<li><p>If <cite>n_parallel_heads</cite> is None (default), the entire layer is
computed with maximum parallelism. This mode is the fastest, but
also uses the most memory. Start with this mode, but switch to one
of the others if memory runs out.</p></li>
<li><p>If <cite>n_parallel_heads</cite> is 1, attention is computed one head at a
time, and one example at a time. This mode uses the least memory
but is not as fast as batched attention. Use this mode when working
with very long sequences, such that any amount of parallelism won’t
fit in memory.</p></li>
<li><p>If <cite>n_parallel_heads</cite> is a multiple of <cite>n_heads</cite>, attention is
computed for sub-batches of (<cite>n_parallel_heads // n_heads</cite>)
examples at a time.</p></li>
<li><p>If <cite>1 &lt; n_parallel_heads &lt; n_heads</cite>, attention is computed for
several heads at a time, but only within a single example. It must
be the case that <cite>n_heads</cite> is a multiple of <cite>n_parallel_heads</cite>. Use
this mode for long sequences, to strike a balance between
parallelism and memory usage.</p></li>
</ul>
</p></li>
<li><p><strong>incremental</strong> – If <cite>True</cite>, enable fast inference for self-attention types.
Note that this flag should <em>not</em> be set when doing encoder-decoder
attention, but only when doing self-attention.</p></li>
<li><p><strong>predict_mem_len</strong> – Number of input positions to remember in a cache
when doing fast inference. Whenever the cache fills up, some input
elements will be forgotten.</p></li>
<li><p><strong>predict_drop_len</strong> – Number of input elements to drop once the fast
inference input cache fills up.</p></li>
<li><p><strong>use_python_loop</strong> – Set to True to use a Python loop when iterating over
sub-batches of examples/heads (as opposed to a JAX/XLA loop).
This option will increase compilation time and jitted code size,
potentially drastically. Using it is not recommended except for
testing/debugging. In particular, note that enabling this option on
TPU can decrease the maximum model size that will fit in memory.</p></li>
<li><p><strong>use_reference_code</strong> – Set to True to fall back to the reference
implementation of batched attention. This option will increase
compilation time and jitted code size, potentially drastically. Using
it is not recommended except for testing/debugging.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.EfficientAttentionBase.init_weights_and_state">
<span class="sig-name descname"><span class="pre">init_weights_and_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_signature</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.EfficientAttentionBase.init_weights_and_state" title="Link to this definition"></a></dt>
<dd><p>Initializes weights and state, to handle input with the given signature.</p>
<p>A layer subclass must override this method if the layer uses weights or
state. To initialize weights, set <cite>self.weights</cite> to desired (typically
random) values. To initialize state (uncommon), set <cite>self.state</cite> to desired
starting values.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input_signature</strong> – A <cite>ShapeDtype</cite> instance (if this layer takes one input)
or a list/tuple of <cite>ShapeDtype</cite> instances.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.EfficientAttentionBase.create_weights_unbatched">
<span class="sig-name descname"><span class="pre">create_weights_unbatched</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_signature</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rng</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.EfficientAttentionBase.create_weights_unbatched" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.EfficientAttentionBase.create_state_unbatched">
<span class="sig-name descname"><span class="pre">create_state_unbatched</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_signature</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rng</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.EfficientAttentionBase.create_state_unbatched" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.EfficientAttentionBase.forward_unbatched">
<span class="sig-name descname"><span class="pre">forward_unbatched</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.EfficientAttentionBase.forward_unbatched" title="Link to this definition"></a></dt>
<dd><p>Perform attention for a single batch element and head.</p>
<p>Subclasses should override this method.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>*inputs</strong> – Inputs for a single example (subclasses may use different inputs)</p></li>
<li><p><strong>weights</strong> – Weights for a single attention head</p></li>
<li><p><strong>state</strong> – State for a single example &amp; attention head pair.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A tuple (output, new_state) – output and new state for a single example
and attention head.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.EfficientAttentionBase.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.EfficientAttentionBase.forward" title="Link to this definition"></a></dt>
<dd><p>Computes this layer’s output as part of a forward pass through the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>inputs</strong> – Layer inputs (subclasses may use different inputs)</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A tuple (output, new_state).</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.EfficientAttentionBase.has_backward">
<span class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></span><span class="sig-name descname"><span class="pre">has_backward</span></span><a class="headerlink" href="#trax.layers.research.efficient_attention.EfficientAttentionBase.has_backward" title="Link to this definition"></a></dt>
<dd><p>Returns <cite>True</cite> if this layer provides its own custom backward pass code.</p>
<p>A layer subclass that provides custom backward pass code (for custom
gradients) must override this method to return <cite>True</cite>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.EfficientAttentionBase.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">new_state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rng</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.EfficientAttentionBase.backward" title="Link to this definition"></a></dt>
<dd><p>Custom backward pass, for efficiency (see forward_and_or_backward).</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.EfficientAttentionBase.forward_and_or_backward">
<span class="sig-name descname"><span class="pre">forward_and_or_backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rng</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">compute_output</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">update_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.EfficientAttentionBase.forward_and_or_backward" title="Link to this definition"></a></dt>
<dd><p>Performs batched forward and/or backward passes.</p>
<p>See <cite>forward</cite> for a reference implementation of what this layer does. The
reference implementation is not very efficient, however, and this method
provides a more performant version.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> – inputs to the attention layer</p></li>
<li><p><strong>weights</strong> – weights for the attention layer</p></li>
<li><p><strong>state</strong> – state of the attention layer</p></li>
<li><p><strong>rng</strong> – PRNG key for the layer (shared across all examples and heads)</p></li>
<li><p><strong>output_grad</strong> – gradient of the loss wrt the output of the layer, or None.
This function performs the backward pass iff <cite>output_grad</cite> is not
None.</p></li>
<li><p><strong>compute_output</strong> – bool: whether to return the output of the forward pass
(for example, a pure backwards pass does not need to return the
output).</p></li>
<li><p><strong>update_state</strong> – bool: whether to return an updated layer state.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><p>A tuple (output, new_state, inputs_grad, weights_grad).</p>
<ul class="simple">
<li><p>output is not None iff compute_output is True</p></li>
<li><p>new_state is not None iff update_state is True</p></li>
<li><p>inputs_grad &amp; weights_grad are not None iff output_grad is not None</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.SelfAttention">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.research.efficient_attention.</span></span><span class="sig-name descname"><span class="pre">SelfAttention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_qk</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_v</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">share_qk</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">causal</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">masked</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chunk_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_chunks_before</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_chunks_after</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">predict_mem_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">predict_drop_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_parallel_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_python_loop</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_reference_code</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.SelfAttention" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#trax.layers.base.Layer" title="trax.layers.base.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Layer</span></code></a></p>
<p>Memory-efficient self-attention (second attempt).</p>
<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.SelfAttention.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_qk</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_v</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">share_qk</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">causal</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">masked</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chunk_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_chunks_before</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_chunks_after</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">predict_mem_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">predict_drop_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_parallel_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_python_loop</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_reference_code</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.SelfAttention.__init__" title="Link to this definition"></a></dt>
<dd><p>Construct a self-attention layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_heads</strong> – int: Number of attention heads</p></li>
<li><p><strong>d_qk</strong> – int: Depth of query ond key vectors</p></li>
<li><p><strong>d_v</strong> – int: Depth of value vectors</p></li>
<li><p><strong>share_qk</strong> – bool: Set to True to share query and key projection weights</p></li>
<li><p><strong>causal</strong> – bool: Set to True to mask out attention to future items</p></li>
<li><p><strong>masked</strong> – bool: Set to True to accept an additional mask argument, that
allows masking out attention to padding tokens.</p></li>
<li><p><strong>chunk_len</strong> (<em>optional</em>) – Number of tokens per chunk. Setting this option will
enable chunked attention.</p></li>
<li><p><strong>n_chunks_before</strong> – Number of previous chunks to attend to, when using
chunked attention.</p></li>
<li><p><strong>n_chunks_after</strong> – Number of subsequent chunks to attend to, when using
chunked attention. Don’t use this option for causal attention, because
attention to future tokens will be masked out anyway. However, note that
cross-chunk attention “wraps around” in both directions, so this option
is never a strict no-op.</p></li>
<li><p><strong>bias</strong> – bool: Set to True to add bias vectors when computing query/key/value</p></li>
<li><p><strong>mode</strong> – ‘train’, ‘eval’, or ‘predict’</p></li>
<li><p><strong>predict_mem_len</strong> – int: Number of input positions to remember in a cache
when doing fast inference. Whenever the cache fills up, some input
elements will be forgotten. When chunking is enabled, the default is to
store chunk_len * (1 + n_chunks_before) elements.</p></li>
<li><p><strong>predict_drop_len</strong> – int: Number of input elements to drop once the fast
inference input cache fills up. When chunking is enabled, the default is
to drop exactly chunk_len elements.</p></li>
<li><p><strong>attention_dropout</strong> – Dropout probability for attention mask.</p></li>
<li><p><strong>output_dropout</strong> – Dropout probability for the layer output.</p></li>
<li><p><strong>n_parallel_heads</strong> – <p>Number of attention heads to compute in parallel.</p>
<ul>
<li><p>If <cite>n_parallel_heads</cite> is None (default), the entire layer is
computed with maximum parallelism. This mode is the fastest, but
also uses the most memory. Start with this mode, but switch to one
of the others if memory runs out.</p></li>
<li><p>If <cite>n_parallel_heads</cite> is 1, attention is computed one head at a
time, and one example at a time. This mode uses the least memory
but is not as fast as batched attention. Use this mode when working
with very long sequences, such that any amount of parallelism won’t
fit in memory.</p></li>
<li><p>If <cite>n_parallel_heads</cite> is a multiple of <cite>n_heads</cite>, attention is
computed for sub-batches of (<cite>n_parallel_heads // n_heads</cite>)
examples at a time.</p></li>
<li><p>If <cite>1 &lt; n_parallel_heads &lt; n_heads</cite>, attention is computed for
several heads at a time, but only within a single example. It must
be the case that <cite>n_heads</cite> is a multiple of <cite>n_parallel_heads</cite>. Use
this mode for long sequences, to strike a balance between
parallelism and memory usage.</p></li>
</ul>
</p></li>
<li><p><strong>use_python_loop</strong> – Set to True to use a Python loop when iterating over
sub-batches of examples/heads (as opposed to a JAX/XLA loop).
This option will increase compilation time and jitted code size,
potentially drastically. Using it is not recommended except for
testing/debugging. In particular, note that enabling this option on
TPU can decrease the maximum model size that will fit in memory.</p></li>
<li><p><strong>use_reference_code</strong> – Set to True to fall back to the reference
implementation of batched attention. This option will increase
compilation time and jitted code size, potentially drastically. Using
it is not recommended except for testing/debugging.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.SelfAttention.init_weights_and_state">
<span class="sig-name descname"><span class="pre">init_weights_and_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_signature</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.SelfAttention.init_weights_and_state" title="Link to this definition"></a></dt>
<dd><p>Initializes weights and state, to handle input with the given signature.</p>
<p>A layer subclass must override this method if the layer uses weights or
state. To initialize weights, set <cite>self.weights</cite> to desired (typically
random) values. To initialize state (uncommon), set <cite>self.state</cite> to desired
starting values.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input_signature</strong> – A <cite>ShapeDtype</cite> instance (if this layer takes one input)
or a list/tuple of <cite>ShapeDtype</cite> instances.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.SelfAttention.create_weights_unbatched">
<span class="sig-name descname"><span class="pre">create_weights_unbatched</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_signature</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rng</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.SelfAttention.create_weights_unbatched" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.SelfAttention.create_state_unbatched">
<span class="sig-name descname"><span class="pre">create_state_unbatched</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_signature</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rng</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.SelfAttention.create_state_unbatched" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.SelfAttention.forward_unbatched">
<span class="sig-name descname"><span class="pre">forward_unbatched</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="keyword-only-separator o"><abbr title="Keyword-only parameters separator (PEP 3102)"><span class="pre">*</span></abbr></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rng</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">update_state</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.SelfAttention.forward_unbatched" title="Link to this definition"></a></dt>
<dd><p>Perform attention for a single batch element and head.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> – Inputs for a single example (subclasses may use different inputs)</p></li>
<li><p><strong>mask</strong> – Mask for the inputs.</p></li>
<li><p><strong>weights</strong> – Weights for a single attention head</p></li>
<li><p><strong>state</strong> – State for a single example &amp; attention head pair.</p></li>
<li><p><strong>rng</strong> – PRNG key for the layer (shared across all examples and heads)</p></li>
<li><p><strong>update_state</strong> – bool: whether to return an updated layer state.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A tuple (output, new_state) – output and new state for a single example
and attention head.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.SelfAttention.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.SelfAttention.forward" title="Link to this definition"></a></dt>
<dd><p>Computes this layer’s output as part of a forward pass through the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>inputs</strong> – Layer inputs (subclasses may use different inputs)</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A tuple (output, new_state).</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.SelfAttention.has_backward">
<span class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></span><span class="sig-name descname"><span class="pre">has_backward</span></span><a class="headerlink" href="#trax.layers.research.efficient_attention.SelfAttention.has_backward" title="Link to this definition"></a></dt>
<dd><p>Returns <cite>True</cite> if this layer provides its own custom backward pass code.</p>
<p>A layer subclass that provides custom backward pass code (for custom
gradients) must override this method to return <cite>True</cite>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.SelfAttention.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">new_state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rng</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.SelfAttention.backward" title="Link to this definition"></a></dt>
<dd><p>Custom backward pass, for efficiency (see forward_and_or_backward).</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.SelfAttention.forward_and_or_backward">
<span class="sig-name descname"><span class="pre">forward_and_or_backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rng</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">compute_output</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">update_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.SelfAttention.forward_and_or_backward" title="Link to this definition"></a></dt>
<dd><p>Performs batched forward and/or backward passes.</p>
<p>See <cite>forward</cite> for a reference implementation of what this layer does. The
reference implementation is not very efficient, however, and this method
provides a more performant version.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> – inputs to the attention layer</p></li>
<li><p><strong>weights</strong> – weights for the attention layer</p></li>
<li><p><strong>state</strong> – state of the attention layer</p></li>
<li><p><strong>rng</strong> – PRNG key for the layer (shared across all examples and heads)</p></li>
<li><p><strong>output_grad</strong> – gradient of the loss wrt the output of the layer, or None.
This function performs the backward pass iff <cite>output_grad</cite> is not
None.</p></li>
<li><p><strong>compute_output</strong> – bool: whether to return the output of the forward pass
(for example, a pure backwards pass does not need to return the
output).</p></li>
<li><p><strong>update_state</strong> – bool: whether to return an updated layer state.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><p>A tuple (output, new_state, inputs_grad, weights_grad).</p>
<ul class="simple">
<li><p>output is not None iff compute_output is True</p></li>
<li><p>new_state is not None iff update_state is True</p></li>
<li><p>inputs_grad &amp; weights_grad are not None iff output_grad is not None</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.LSHSelfAttention">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.research.efficient_attention.</span></span><span class="sig-name descname"><span class="pre">LSHSelfAttention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_qk</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_v</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">share_qk</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'unused'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">causal</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">masked</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chunk_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">128</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_chunks_before</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_chunks_after</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_hashes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_buckets</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">predict_mem_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2048</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">predict_drop_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">256</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_length_for_buckets</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_parallel_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_python_loop</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_reference_code</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.LSHSelfAttention" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#trax.layers.base.Layer" title="trax.layers.base.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Layer</span></code></a></p>
<p>LSH self-attention (second implementation).</p>
<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.LSHSelfAttention.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_qk</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_v</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">share_qk</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'unused'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">causal</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">masked</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chunk_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">128</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_chunks_before</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_chunks_after</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_hashes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_buckets</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">predict_mem_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2048</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">predict_drop_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">256</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_length_for_buckets</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_parallel_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_python_loop</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_reference_code</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.LSHSelfAttention.__init__" title="Link to this definition"></a></dt>
<dd><p>Construct an LSH self-attention layer.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.LSHSelfAttention.init_weights_and_state">
<span class="sig-name descname"><span class="pre">init_weights_and_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_signature</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.LSHSelfAttention.init_weights_and_state" title="Link to this definition"></a></dt>
<dd><p>Initializes weights and state, to handle input with the given signature.</p>
<p>A layer subclass must override this method if the layer uses weights or
state. To initialize weights, set <cite>self.weights</cite> to desired (typically
random) values. To initialize state (uncommon), set <cite>self.state</cite> to desired
starting values.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input_signature</strong> – A <cite>ShapeDtype</cite> instance (if this layer takes one input)
or a list/tuple of <cite>ShapeDtype</cite> instances.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.LSHSelfAttention.create_weights_unbatched">
<span class="sig-name descname"><span class="pre">create_weights_unbatched</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_signature</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rng</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.LSHSelfAttention.create_weights_unbatched" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.LSHSelfAttention.create_state_unbatched">
<span class="sig-name descname"><span class="pre">create_state_unbatched</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_signature</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rng</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.LSHSelfAttention.create_state_unbatched" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.LSHSelfAttention.hash_vectors">
<span class="sig-name descname"><span class="pre">hash_vectors</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">vecs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rng</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.LSHSelfAttention.hash_vectors" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.LSHSelfAttention.forward_unbatched">
<span class="sig-name descname"><span class="pre">forward_unbatched</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="keyword-only-separator o"><abbr title="Keyword-only parameters separator (PEP 3102)"><span class="pre">*</span></abbr></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rng</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">update_state</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.LSHSelfAttention.forward_unbatched" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.LSHSelfAttention.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.LSHSelfAttention.forward" title="Link to this definition"></a></dt>
<dd><p>Computes this layer’s output as part of a forward pass through the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>inputs</strong> – Layer inputs (subclasses may use different inputs)</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A tuple (output, new_state).</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.LSHSelfAttention.has_backward">
<span class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></span><span class="sig-name descname"><span class="pre">has_backward</span></span><a class="headerlink" href="#trax.layers.research.efficient_attention.LSHSelfAttention.has_backward" title="Link to this definition"></a></dt>
<dd><p>Returns <cite>True</cite> if this layer provides its own custom backward pass code.</p>
<p>A layer subclass that provides custom backward pass code (for custom
gradients) must override this method to return <cite>True</cite>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.LSHSelfAttention.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">new_state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rng</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.LSHSelfAttention.backward" title="Link to this definition"></a></dt>
<dd><p>Custom backward pass, for efficiency (see forward_and_or_backward).</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.LSHSelfAttention.forward_and_or_backward">
<span class="sig-name descname"><span class="pre">forward_and_or_backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rng</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">compute_output</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">update_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.LSHSelfAttention.forward_and_or_backward" title="Link to this definition"></a></dt>
<dd><p>Performs batched forward and/or backward passes.</p>
<p>See <cite>forward</cite> for a reference implementation of what this layer does. The
reference implementation is not very efficient, however, and this method
provides a more performant version.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> – inputs to the attention layer</p></li>
<li><p><strong>weights</strong> – weights for the attention layer</p></li>
<li><p><strong>state</strong> – state of the attention layer</p></li>
<li><p><strong>rng</strong> – PRNG key for the layer (shared across all examples and heads)</p></li>
<li><p><strong>output_grad</strong> – gradient of the loss wrt the output of the layer, or None.
This function performs the backward pass iff <cite>output_grad</cite> is not
None.</p></li>
<li><p><strong>compute_output</strong> – bool: whether to return the output of the forward pass
(for example, a pure backwards pass does not need to return the
output).</p></li>
<li><p><strong>update_state</strong> – bool: whether to return an updated layer state.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><p>A tuple (output, new_state, inputs_grad, weights_grad).</p>
<ul class="simple">
<li><p>output is not None iff compute_output is True</p></li>
<li><p>new_state is not None iff update_state is True</p></li>
<li><p>inputs_grad &amp; weights_grad are not None iff output_grad is not None</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.PureLSHSelfAttention">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.research.efficient_attention.</span></span><span class="sig-name descname"><span class="pre">PureLSHSelfAttention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_qk</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_v</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">share_qk</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'unused'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">causal</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">masked</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chunk_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">128</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_chunks_before</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_chunks_after</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_hashes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_buckets</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">predict_mem_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2048</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">predict_drop_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">256</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_length_for_buckets</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_parallel_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_python_loop</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_reference_code</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.PureLSHSelfAttention" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#trax.layers.base.Layer" title="trax.layers.base.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Layer</span></code></a></p>
<p>LSH self-attention without weights.</p>
<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.PureLSHSelfAttention.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_qk</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_v</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">share_qk</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'unused'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">causal</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">masked</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chunk_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">128</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_chunks_before</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_chunks_after</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_hashes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_buckets</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">predict_mem_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2048</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">predict_drop_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">256</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_length_for_buckets</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_parallel_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_python_loop</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_reference_code</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.PureLSHSelfAttention.__init__" title="Link to this definition"></a></dt>
<dd><p>Construct an LSH self-attention layer.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.PureLSHSelfAttention.init_weights_and_state">
<span class="sig-name descname"><span class="pre">init_weights_and_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_signature</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.PureLSHSelfAttention.init_weights_and_state" title="Link to this definition"></a></dt>
<dd><p>Initializes weights and state, to handle input with the given signature.</p>
<p>A layer subclass must override this method if the layer uses weights or
state. To initialize weights, set <cite>self.weights</cite> to desired (typically
random) values. To initialize state (uncommon), set <cite>self.state</cite> to desired
starting values.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input_signature</strong> – A <cite>ShapeDtype</cite> instance (if this layer takes one input)
or a list/tuple of <cite>ShapeDtype</cite> instances.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.PureLSHSelfAttention.create_state_unbatched">
<span class="sig-name descname"><span class="pre">create_state_unbatched</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_signature</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rng</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.PureLSHSelfAttention.create_state_unbatched" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.PureLSHSelfAttention.hash_vectors">
<span class="sig-name descname"><span class="pre">hash_vectors</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">vecs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rng</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.PureLSHSelfAttention.hash_vectors" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.PureLSHSelfAttention.forward_unbatched">
<span class="sig-name descname"><span class="pre">forward_unbatched</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">qk</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">v</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="keyword-only-separator o"><abbr title="Keyword-only parameters separator (PEP 3102)"><span class="pre">*</span></abbr></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rng</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">update_state</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.PureLSHSelfAttention.forward_unbatched" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.PureLSHSelfAttention.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.PureLSHSelfAttention.forward" title="Link to this definition"></a></dt>
<dd><p>Computes this layer’s output as part of a forward pass through the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>inputs</strong> – Layer inputs (subclasses may use different inputs)</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A tuple (output, new_state).</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.PureLSHSelfAttention.has_backward">
<span class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></span><span class="sig-name descname"><span class="pre">has_backward</span></span><a class="headerlink" href="#trax.layers.research.efficient_attention.PureLSHSelfAttention.has_backward" title="Link to this definition"></a></dt>
<dd><p>Returns <cite>True</cite> if this layer provides its own custom backward pass code.</p>
<p>A layer subclass that provides custom backward pass code (for custom
gradients) must override this method to return <cite>True</cite>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.PureLSHSelfAttention.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">new_state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rng</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.PureLSHSelfAttention.backward" title="Link to this definition"></a></dt>
<dd><p>Custom backward pass, for efficiency (see forward_and_or_backward).</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.PureLSHSelfAttention.forward_and_or_backward">
<span class="sig-name descname"><span class="pre">forward_and_or_backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rng</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">compute_output</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">update_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.PureLSHSelfAttention.forward_and_or_backward" title="Link to this definition"></a></dt>
<dd><p>Performs batched forward and/or backward passes.</p>
<p>See <cite>forward</cite> for a reference implementation of what this layer does. The
reference implementation is not very efficient, however, and this method
provides a more performant version.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> – inputs to the attention layer tuple (qk, v, mask)</p></li>
<li><p><strong>state</strong> – state of the attention layer</p></li>
<li><p><strong>rng</strong> – PRNG key for the layer (shared across all examples and heads)</p></li>
<li><p><strong>output_grad</strong> – gradient of the loss wrt the output of the layer, or None.
This function performs the backward pass iff <cite>output_grad</cite> is not
None.</p></li>
<li><p><strong>compute_output</strong> – bool: whether to return the output of the forward pass
(for example, a pure backwards pass does not need to return the
output).</p></li>
<li><p><strong>update_state</strong> – bool: whether to return an updated layer state.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><p>A tuple (output, new_state, inputs_grad, weights_grad).</p>
<ul class="simple">
<li><p>output is not None iff compute_output is True</p></li>
<li><p>new_state is not None iff update_state is True</p></li>
<li><p>inputs_grad &amp; weights_grad are not None iff output_grad is not None</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.MixedLSHSelfAttention">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.research.efficient_attention.</span></span><span class="sig-name descname"><span class="pre">MixedLSHSelfAttention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_qk</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_v</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">causal</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">masked</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">std_length</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">force_no_dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">pure_lsh_implementation_kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.MixedLSHSelfAttention" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#trax.layers.base.Layer" title="trax.layers.base.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Layer</span></code></a></p>
<p>LSH attention mixed with standard attention used until std_length.</p>
<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.MixedLSHSelfAttention.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_qk</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_v</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">causal</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">masked</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">std_length</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">force_no_dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">pure_lsh_implementation_kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.MixedLSHSelfAttention.__init__" title="Link to this definition"></a></dt>
<dd><p>Creates a partially initialized, unconnected layer instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_in</strong> – Number of inputs expected by this layer.</p></li>
<li><p><strong>n_out</strong> – Number of outputs promised by this layer.</p></li>
<li><p><strong>name</strong> – Class-like name for this layer; for use when printing this layer.</p></li>
<li><p><strong>sublayers_to_print</strong> – Sublayers to display when printing out this layer;
if None (the default), display all sublayers.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.MixedLSHSelfAttention.init_weights_and_state">
<span class="sig-name descname"><span class="pre">init_weights_and_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_signature</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.MixedLSHSelfAttention.init_weights_and_state" title="Link to this definition"></a></dt>
<dd><p>Initializes weights and state for inputs with the given signature.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.MixedLSHSelfAttention.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">xs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.MixedLSHSelfAttention.forward" title="Link to this definition"></a></dt>
<dd><p>Executes this layer as part of a forward pass through the model.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.MixedLSHSelfAttention.forward_and_or_backward">
<span class="sig-name descname"><span class="pre">forward_and_or_backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rng</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">compute_output</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">update_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.MixedLSHSelfAttention.forward_and_or_backward" title="Link to this definition"></a></dt>
<dd><p>Performs batched forward and/or backward passes.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.PureLSHSelfAttentionWrapper">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.research.efficient_attention.</span></span><span class="sig-name descname"><span class="pre">PureLSHSelfAttentionWrapper</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_qk</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_v</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">causal</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">masked</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pure_lsh_implementation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sparsity</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'model'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rotary_position_emb</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">pure_lsh_implementation_kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.PureLSHSelfAttentionWrapper" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#trax.layers.combinators.Serial" title="trax.layers.combinators.Serial"><code class="xref py py-class docutils literal notranslate"><span class="pre">Serial</span></code></a></p>
<p>Pure LSH serial.</p>
<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.PureLSHSelfAttentionWrapper.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_qk</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_v</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">causal</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">masked</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pure_lsh_implementation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sparsity</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'model'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rotary_position_emb</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">pure_lsh_implementation_kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.PureLSHSelfAttentionWrapper.__init__" title="Link to this definition"></a></dt>
<dd><p>Creates a partially initialized, unconnected layer instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_in</strong> – Number of inputs expected by this layer.</p></li>
<li><p><strong>n_out</strong> – Number of outputs promised by this layer.</p></li>
<li><p><strong>name</strong> – Class-like name for this layer; for use when printing this layer.</p></li>
<li><p><strong>sublayers_to_print</strong> – Sublayers to display when printing out this layer;
if None (the default), display all sublayers.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.PureLSHSelfAttentionWrapper.forward_and_or_backward">
<span class="sig-name descname"><span class="pre">forward_and_or_backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rng</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">compute_output</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">update_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.PureLSHSelfAttentionWrapper.forward_and_or_backward" title="Link to this definition"></a></dt>
<dd><p>Performs batched forward and/or backward passes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> – inputs to the attention layer</p></li>
<li><p><strong>weights</strong> – weights for the attention layer</p></li>
<li><p><strong>state</strong> – state of the attention layer</p></li>
<li><p><strong>rng</strong> – PRNG key for the layer (shared across all examples and heads)</p></li>
<li><p><strong>output_grad</strong> – gradient of the loss wrt the output of the layer, or None.
This function performs the backward pass iff <cite>output_grad</cite> is not
None.</p></li>
<li><p><strong>compute_output</strong> – bool: whether to return the output of the forward pass
(for example, a pure backwards pass does not need to return the
output).</p></li>
<li><p><strong>update_state</strong> – bool: whether to return an updated layer state.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A tuple (output, new_state, inputs_grad, weights_grad).
- output is not None iff compute_output is True
- new_state is not None iff update_state is True
- inputs_grad &amp; weights_grad are not None iff output_grad is not None</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.EncDecAttention">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.research.efficient_attention.</span></span><span class="sig-name descname"><span class="pre">EncDecAttention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_qk</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_v</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">masked</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_parallel_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_python_loop</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_reference_code</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.EncDecAttention" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#trax.layers.research.efficient_attention.EfficientAttentionBase" title="trax.layers.research.efficient_attention.EfficientAttentionBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">EfficientAttentionBase</span></code></a></p>
<p>Memory-efficient encoder-decoder attention.</p>
<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.EncDecAttention.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_qk</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_v</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">masked</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_parallel_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_python_loop</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_reference_code</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.EncDecAttention.__init__" title="Link to this definition"></a></dt>
<dd><p>Constructs an EfficientAttentionBase instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_heads</strong> – Number of attention heads.</p></li>
<li><p><strong>n_in</strong> – Number of inputs to the layer (default 1).</p></li>
<li><p><strong>n_parallel_heads</strong> – <p>Number of attention heads to compute in parallel.</p>
<ul>
<li><p>If <cite>n_parallel_heads</cite> is None (default), the entire layer is
computed with maximum parallelism. This mode is the fastest, but
also uses the most memory. Start with this mode, but switch to one
of the others if memory runs out.</p></li>
<li><p>If <cite>n_parallel_heads</cite> is 1, attention is computed one head at a
time, and one example at a time. This mode uses the least memory
but is not as fast as batched attention. Use this mode when working
with very long sequences, such that any amount of parallelism won’t
fit in memory.</p></li>
<li><p>If <cite>n_parallel_heads</cite> is a multiple of <cite>n_heads</cite>, attention is
computed for sub-batches of (<cite>n_parallel_heads // n_heads</cite>)
examples at a time.</p></li>
<li><p>If <cite>1 &lt; n_parallel_heads &lt; n_heads</cite>, attention is computed for
several heads at a time, but only within a single example. It must
be the case that <cite>n_heads</cite> is a multiple of <cite>n_parallel_heads</cite>. Use
this mode for long sequences, to strike a balance between
parallelism and memory usage.</p></li>
</ul>
</p></li>
<li><p><strong>incremental</strong> – If <cite>True</cite>, enable fast inference for self-attention types.
Note that this flag should <em>not</em> be set when doing encoder-decoder
attention, but only when doing self-attention.</p></li>
<li><p><strong>predict_mem_len</strong> – Number of input positions to remember in a cache
when doing fast inference. Whenever the cache fills up, some input
elements will be forgotten.</p></li>
<li><p><strong>predict_drop_len</strong> – Number of input elements to drop once the fast
inference input cache fills up.</p></li>
<li><p><strong>use_python_loop</strong> – Set to True to use a Python loop when iterating over
sub-batches of examples/heads (as opposed to a JAX/XLA loop).
This option will increase compilation time and jitted code size,
potentially drastically. Using it is not recommended except for
testing/debugging. In particular, note that enabling this option on
TPU can decrease the maximum model size that will fit in memory.</p></li>
<li><p><strong>use_reference_code</strong> – Set to True to fall back to the reference
implementation of batched attention. This option will increase
compilation time and jitted code size, potentially drastically. Using
it is not recommended except for testing/debugging.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.EncDecAttention.create_weights_unbatched">
<span class="sig-name descname"><span class="pre">create_weights_unbatched</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_signature</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rng</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.EncDecAttention.create_weights_unbatched" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.EncDecAttention.forward_unbatched">
<span class="sig-name descname"><span class="pre">forward_unbatched</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">q_antecedent</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kv_antecedent</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="keyword-only-separator o"><abbr title="Keyword-only parameters separator (PEP 3102)"><span class="pre">*</span></abbr></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rng</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">update_state</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.EncDecAttention.forward_unbatched" title="Link to this definition"></a></dt>
<dd><p>Perform attention for a single batch element and head.</p>
<p>Subclasses should override this method.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>*inputs</strong> – Inputs for a single example (subclasses may use different inputs)</p></li>
<li><p><strong>weights</strong> – Weights for a single attention head</p></li>
<li><p><strong>state</strong> – State for a single example &amp; attention head pair.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A tuple (output, new_state) – output and new state for a single example
and attention head.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.LSHFF">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.research.efficient_attention.</span></span><span class="sig-name descname"><span class="pre">LSHFF</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d_ff</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_buckets</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_hashes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">&lt;function</span> <span class="pre">ScaledInitializer.&lt;locals&gt;.Init&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias_initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">&lt;function</span> <span class="pre">RandomNormalInitializer.&lt;locals&gt;.&lt;lambda&gt;&gt;</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.LSHFF" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#trax.layers.base.Layer" title="trax.layers.base.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Layer</span></code></a></p>
<p>Feed-forward block with LSH.</p>
<p>The original (non-LSH) feed-forward block is a triple Dense(d_ff)-Relu-Dense
that takes an input, makes it of size d_ff (usually larger than it was) and
then brings it back to the original size after Relu. It is commonly used in
Transformer models where it often accounts for most of the trainable weights.</p>
<p>The original block can be slow in decoding due to the need to fetch a lot of
weights from memory. The LSH block aims to exploit this sparsity. So in the
first Dense(d_ff) layer, instead of making a full matrix multiplication,
this block only multiplies by the parts of the weights matrix that have
the highest chance to give non-0 after Relu. This is determined by taking
a number of locality-sensitive hashes and masking to only include weights
that have one hash identical to the multiplied element.</p>
<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.LSHFF.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d_ff</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_buckets</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_hashes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">&lt;function</span> <span class="pre">ScaledInitializer.&lt;locals&gt;.Init&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias_initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">&lt;function</span> <span class="pre">RandomNormalInitializer.&lt;locals&gt;.&lt;lambda&gt;&gt;</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.LSHFF.__init__" title="Link to this definition"></a></dt>
<dd><p>Returns a LSH feed-forward block.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.LSHFF.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.LSHFF.forward" title="Link to this definition"></a></dt>
<dd><p>Executes this layer as part of a forward pass through the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> – Tensor of same shape and dtype as the input signature used to
initialize this layer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Tensor of same shape and dtype as the input.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.efficient_attention.LSHFF.init_weights_and_state">
<span class="sig-name descname"><span class="pre">init_weights_and_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_signature</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.efficient_attention.LSHFF.init_weights_and_state" title="Link to this definition"></a></dt>
<dd><p>Randomly initializes this layer’s weights.</p>
</dd></dl>

</dd></dl>

</section>
<section id="module-trax.layers.research.position_encodings">
<span id="research-position-encodings"></span><h2>research.position_encodings<a class="headerlink" href="#module-trax.layers.research.position_encodings" title="Link to this heading"></a></h2>
<p>Experimenting with position encodings.</p>
<dl class="py class">
<dt class="sig sig-object py" id="trax.layers.research.position_encodings.AxialPositionalEncoding">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.research.position_encodings.</span></span><span class="sig-name descname"><span class="pre">AxialPositionalEncoding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">3)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_embs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(384</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">384</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">256)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">&lt;function</span> <span class="pre">RandomNormalInitializer.&lt;locals&gt;.&lt;lambda&gt;&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout_broadcast_dims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.position_encodings.AxialPositionalEncoding" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#trax.layers.base.Layer" title="trax.layers.base.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Layer</span></code></a></p>
<p>Axial positional encoding.</p>
<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.position_encodings.AxialPositionalEncoding.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">3)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_embs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(384</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">384</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">256)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">&lt;function</span> <span class="pre">RandomNormalInitializer.&lt;locals&gt;.&lt;lambda&gt;&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout_broadcast_dims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.position_encodings.AxialPositionalEncoding.__init__" title="Link to this definition"></a></dt>
<dd><p>Creates a partially initialized, unconnected layer instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_in</strong> – Number of inputs expected by this layer.</p></li>
<li><p><strong>n_out</strong> – Number of outputs promised by this layer.</p></li>
<li><p><strong>name</strong> – Class-like name for this layer; for use when printing this layer.</p></li>
<li><p><strong>sublayers_to_print</strong> – Sublayers to display when printing out this layer;
if None (the default), display all sublayers.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.position_encodings.AxialPositionalEncoding.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.position_encodings.AxialPositionalEncoding.forward" title="Link to this definition"></a></dt>
<dd><p>Computes this layer’s output as part of a forward pass through the model.</p>
<p>A layer subclass overrides this method to define how the layer computes
outputs from inputs. If the layer depends on weights, state, or randomness
as part of the computation, the needed information can be accessed as
properties of the layer object: <cite>self.weights</cite>, <cite>self.state</cite>, and
<cite>self.rng</cite>. (See numerous examples in <cite>trax.layers.core</cite>.)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>inputs</strong> – Zero or more input tensors, packaged as described in the <cite>Layer</cite>
class docstring.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Zero or more output tensors, packaged as described in the <cite>Layer</cite> class
docstring.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.position_encodings.AxialPositionalEncoding.init_weights_and_state">
<span class="sig-name descname"><span class="pre">init_weights_and_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_signature</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.position_encodings.AxialPositionalEncoding.init_weights_and_state" title="Link to this definition"></a></dt>
<dd><p>Initializes weights and state, to handle input with the given signature.</p>
<p>A layer subclass must override this method if the layer uses weights or
state. To initialize weights, set <cite>self.weights</cite> to desired (typically
random) values. To initialize state (uncommon), set <cite>self.state</cite> to desired
starting values.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input_signature</strong> – A <cite>ShapeDtype</cite> instance (if this layer takes one input)
or a list/tuple of <cite>ShapeDtype</cite> instances.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="trax.layers.research.position_encodings.SinCosPositionalEncoding">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.research.position_encodings.</span></span><span class="sig-name descname"><span class="pre">SinCosPositionalEncoding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">add_offset</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2048</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout_broadcast_dims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(-2,)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">start_from_zero_one_in</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.position_encodings.SinCosPositionalEncoding" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#trax.layers.base.Layer" title="trax.layers.base.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Layer</span></code></a></p>
<p>Implements the sin-cos positional encoding.</p>
<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.position_encodings.SinCosPositionalEncoding.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">add_offset</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2048</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout_broadcast_dims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(-2,)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">start_from_zero_one_in</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.position_encodings.SinCosPositionalEncoding.__init__" title="Link to this definition"></a></dt>
<dd><p>Creates a SinCosPositionalEncoding instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>add_offset</strong> – Maximumnumber to add to positions during training.</p></li>
<li><p><strong>dropout</strong> – Probability of <em>not</em> adding positional encoding to a sequence
position.</p></li>
<li><p><strong>dropout_broadcast_dims</strong> – Axes along which dropout mask values are
broadcast rather than individually set at random.</p></li>
<li><p><strong>start_from_zero_one_in</strong> – how often to start from 0 during training,
every one in that many times (e.g., if 4, then it’s 25% of the time).</p></li>
<li><p><strong>mode</strong> – One of <cite>‘train’</cite>, <cite>‘eval’</cite>, or <cite>‘predict’</cite>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.position_encodings.SinCosPositionalEncoding.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.position_encodings.SinCosPositionalEncoding.forward" title="Link to this definition"></a></dt>
<dd><p>Returns the input activations, with added positional information.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.position_encodings.SinCosPositionalEncoding.init_weights_and_state">
<span class="sig-name descname"><span class="pre">init_weights_and_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_signature</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.position_encodings.SinCosPositionalEncoding.init_weights_and_state" title="Link to this definition"></a></dt>
<dd><p>Randomly initializes the positional encoding vectors.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input_signature</strong> – <cite>ShapeDtype</cite> instance characterizing the input this
layer should compute on.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="trax.layers.research.position_encodings.FixedBasePositionalEncoding">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.research.position_encodings.</span></span><span class="sig-name descname"><span class="pre">FixedBasePositionalEncoding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">bases=[11,</span> <span class="pre">13,</span> <span class="pre">14,</span> <span class="pre">15],</span> <span class="pre">n_digits=8,</span> <span class="pre">start_from_zero_one_in=2,</span> <span class="pre">base_dropout_one_in=100,</span> <span class="pre">mode='train',</span> <span class="pre">initializer=&lt;function</span> <span class="pre">RandomUniformInitializer.&lt;locals&gt;.&lt;lambda&gt;&gt;</span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.position_encodings.FixedBasePositionalEncoding" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#trax.layers.base.Layer" title="trax.layers.base.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Layer</span></code></a></p>
<p>Implements fixed-training positional encoding.</p>
<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.position_encodings.FixedBasePositionalEncoding.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">bases=[11,</span> <span class="pre">13,</span> <span class="pre">14,</span> <span class="pre">15],</span> <span class="pre">n_digits=8,</span> <span class="pre">start_from_zero_one_in=2,</span> <span class="pre">base_dropout_one_in=100,</span> <span class="pre">mode='train',</span> <span class="pre">initializer=&lt;function</span> <span class="pre">RandomUniformInitializer.&lt;locals&gt;.&lt;lambda&gt;&gt;</span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.position_encodings.FixedBasePositionalEncoding.__init__" title="Link to this definition"></a></dt>
<dd><p>Creates a partially initialized, unconnected layer instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_in</strong> – Number of inputs expected by this layer.</p></li>
<li><p><strong>n_out</strong> – Number of outputs promised by this layer.</p></li>
<li><p><strong>name</strong> – Class-like name for this layer; for use when printing this layer.</p></li>
<li><p><strong>sublayers_to_print</strong> – Sublayers to display when printing out this layer;
if None (the default), display all sublayers.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.position_encodings.FixedBasePositionalEncoding.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.position_encodings.FixedBasePositionalEncoding.forward" title="Link to this definition"></a></dt>
<dd><p>Computes this layer’s output as part of a forward pass through the model.</p>
<p>A layer subclass overrides this method to define how the layer computes
outputs from inputs. If the layer depends on weights, state, or randomness
as part of the computation, the needed information can be accessed as
properties of the layer object: <cite>self.weights</cite>, <cite>self.state</cite>, and
<cite>self.rng</cite>. (See numerous examples in <cite>trax.layers.core</cite>.)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>inputs</strong> – Zero or more input tensors, packaged as described in the <cite>Layer</cite>
class docstring.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Zero or more output tensors, packaged as described in the <cite>Layer</cite> class
docstring.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.position_encodings.FixedBasePositionalEncoding.init_weights_and_state">
<span class="sig-name descname"><span class="pre">init_weights_and_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_signature</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.position_encodings.FixedBasePositionalEncoding.init_weights_and_state" title="Link to this definition"></a></dt>
<dd><p>Initializes weights and state, to handle input with the given signature.</p>
<p>A layer subclass must override this method if the layer uses weights or
state. To initialize weights, set <cite>self.weights</cite> to desired (typically
random) values. To initialize state (uncommon), set <cite>self.state</cite> to desired
starting values.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input_signature</strong> – A <cite>ShapeDtype</cite> instance (if this layer takes one input)
or a list/tuple of <cite>ShapeDtype</cite> instances.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.research.position_encodings.threefry_2x32_prf">
<span class="sig-prename descclassname"><span class="pre">trax.layers.research.position_encodings.</span></span><span class="sig-name descname"><span class="pre">threefry_2x32_prf</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">key</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">gin.configurable.np.ndarray</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">gin.configurable.np.ndarray</span></span></span><a class="headerlink" href="#trax.layers.research.position_encodings.threefry_2x32_prf" title="Link to this definition"></a></dt>
<dd><p>Apply the threefry PRF to an array of inputs.</p>
<p>This function is vectorized over x.
For threefry_2x32: K = X = uint32[2]</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>key</strong> – uint32[2] the key of the PRF</p></li>
<li><p><strong>x</strong> – uint32[…, 2] the inputs</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>uint32[…, 2] the outputs</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>y</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trax.layers.research.position_encodings.threefry_2x32_prange">
<span class="sig-prename descclassname"><span class="pre">trax.layers.research.position_encodings.</span></span><span class="sig-name descname"><span class="pre">threefry_2x32_prange</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">key</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lo</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hi</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">2</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.position_encodings.threefry_2x32_prange" title="Link to this definition"></a></dt>
<dd><p>Splits a key into a stream of random keys.</p>
<p>This uses the little-endian counter mode.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>key</strong> – uint32[2] the key to split</p></li>
<li><p><strong>lo</strong> – the range to start extracting from</p></li>
<li><p><strong>hi</strong> – the range to stop extracting from</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>uint32[hi - lo, 2] the split keys</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>keys</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="trax.layers.research.position_encodings.InfinitePositionalEncoding">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.research.position_encodings.</span></span><span class="sig-name descname"><span class="pre">InfinitePositionalEncoding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">drift</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.03</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">affine</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'any'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_bin_length</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.position_encodings.InfinitePositionalEncoding" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#trax.layers.base.Layer" title="trax.layers.base.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Layer</span></code></a></p>
<p>Infinite positional encoding.</p>
<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.position_encodings.InfinitePositionalEncoding.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">drift</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.03</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">affine</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'any'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_bin_length</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.position_encodings.InfinitePositionalEncoding.__init__" title="Link to this definition"></a></dt>
<dd><p>Initializes the encoding.</p>
<p>The encoding tries to roughly evenly traverse the latent space.
The recurrence time is dependent on how many bits per dimension you use.</p>
<p>There are two parameters to control randomization:
- randomizing the origin every 1/drift steps by letting it drift
- randomizing the origin per call</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>drift</strong> – variance in position difference per unit of difference</p></li>
<li><p><strong>affine</strong> – whether to randomize the origin every call</p></li>
<li><p><strong>transform</strong> – learnable transform after encoding (any/diag/none)</p></li>
<li><p><strong>time_bin_length</strong> – Add features AxialPositionalEncoding learns if
TimeBinCausalAttention is the first layer.
bin_length should match TBCA.bin_length
If you set transform=’diag’, this flag increases your model capacity to
close to transform=’any’, though it will still train slower.</p></li>
<li><p><strong>mode</strong> – if ‘predict’, allow evaluating one token at a time</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.position_encodings.InfinitePositionalEncoding.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.position_encodings.InfinitePositionalEncoding.forward" title="Link to this definition"></a></dt>
<dd><p>Computes this layer’s output as part of a forward pass through the model.</p>
<p>A layer subclass overrides this method to define how the layer computes
outputs from inputs. If the layer depends on weights, state, or randomness
as part of the computation, the needed information can be accessed as
properties of the layer object: <cite>self.weights</cite>, <cite>self.state</cite>, and
<cite>self.rng</cite>. (See numerous examples in <cite>trax.layers.core</cite>.)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>inputs</strong> – Zero or more input tensors, packaged as described in the <cite>Layer</cite>
class docstring.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Zero or more output tensors, packaged as described in the <cite>Layer</cite> class
docstring.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.position_encodings.InfinitePositionalEncoding.init_weights_and_state">
<span class="sig-name descname"><span class="pre">init_weights_and_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_signature</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.position_encodings.InfinitePositionalEncoding.init_weights_and_state" title="Link to this definition"></a></dt>
<dd><p>Initializes weights and state, to handle input with the given signature.</p>
<p>A layer subclass must override this method if the layer uses weights or
state. To initialize weights, set <cite>self.weights</cite> to desired (typically
random) values. To initialize state (uncommon), set <cite>self.state</cite> to desired
starting values.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input_signature</strong> – A <cite>ShapeDtype</cite> instance (if this layer takes one input)
or a list/tuple of <cite>ShapeDtype</cite> instances.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="trax.layers.research.position_encodings.TimeBinPositionalEncoding">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">trax.layers.research.position_encodings.</span></span><span class="sig-name descname"><span class="pre">TimeBinPositionalEncoding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">time_bin_length</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.position_encodings.TimeBinPositionalEncoding" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#trax.layers.base.Layer" title="trax.layers.base.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Layer</span></code></a></p>
<p>Just the engineered features from InfinitePositionalEncoding.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="trax.layers.research.position_encodings.TimeBinPositionalEncoding.num_features">
<span class="sig-name descname"><span class="pre">num_features</span></span><span class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">3</span></span><a class="headerlink" href="#trax.layers.research.position_encodings.TimeBinPositionalEncoding.num_features" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.position_encodings.TimeBinPositionalEncoding.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">time_bin_length</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.position_encodings.TimeBinPositionalEncoding.__init__" title="Link to this definition"></a></dt>
<dd><p>Initializes the encoding.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>time_bin_length</strong> – TimeBinCausalAttention.bin_length of the first layer.</p></li>
<li><p><strong>mode</strong> – if ‘predict’, allow evaluating one token at a time</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.position_encodings.TimeBinPositionalEncoding.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.position_encodings.TimeBinPositionalEncoding.forward" title="Link to this definition"></a></dt>
<dd><p>Computes this layer’s output as part of a forward pass through the model.</p>
<p>A layer subclass overrides this method to define how the layer computes
outputs from inputs. If the layer depends on weights, state, or randomness
as part of the computation, the needed information can be accessed as
properties of the layer object: <cite>self.weights</cite>, <cite>self.state</cite>, and
<cite>self.rng</cite>. (See numerous examples in <cite>trax.layers.core</cite>.)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>inputs</strong> – Zero or more input tensors, packaged as described in the <cite>Layer</cite>
class docstring.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Zero or more output tensors, packaged as described in the <cite>Layer</cite> class
docstring.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trax.layers.research.position_encodings.TimeBinPositionalEncoding.init_weights_and_state">
<span class="sig-name descname"><span class="pre">init_weights_and_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_signature</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trax.layers.research.position_encodings.TimeBinPositionalEncoding.init_weights_and_state" title="Link to this definition"></a></dt>
<dd><p>Initializes weights and state, to handle input with the given signature.</p>
<p>A layer subclass must override this method if the layer uses weights or
state. To initialize weights, set <cite>self.weights</cite> to desired (typically
random) values. To initialize state (uncommon), set <cite>self.state</cite> to desired
starting values.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input_signature</strong> – A <cite>ShapeDtype</cite> instance (if this layer takes one input)
or a list/tuple of <cite>ShapeDtype</cite> instances.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="trax.fastmath.html" class="btn btn-neutral float-left" title="trax.fastmath" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="trax.models.html" class="btn btn-neutral float-right" title="trax.models" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, Google LLC..</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>